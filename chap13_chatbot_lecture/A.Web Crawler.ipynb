{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WebCrawler Test\n",
    "WebCrawler는 기본적으로 http 통신을 통해서 얻을 수 있는 HTML 데이터에서 우리가 원하는 데이터를 뽑아내는 과정이라고 볼 수 있다. <br>\n",
    "python에서는 request 라는 통신 라이브러리와 BeauifulSoup라는 Common한 수준에서의 HTML Tag를 추출하는 기능으르 제공하는 라이브러리를 사용하여 간단하게 구현해 볼 수 있다. 기본적으로 추출하고자 하는 데이터가 Json 이나 XML 처럼 정규화된 형태가 없기 때문에 각 사이트 별로 로직을 다르게 구성할 필요가 있을 것이라고 생각된다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import common done\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re, json, os, random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from functools import reduce \n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "# url = \"{0}:{1}\".format(os.environ['HOSTNAME'] , \"8000\")\n",
    "# nn_id = \"nn123\"\n",
    "# nn_wf_ver_id =\"1\"\n",
    "\n",
    "print(\"import common done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 정규 표현식 테스트 (Preprocessing 적용)\n",
    "정규표현식은 Crawler 에서 필수적인 기능으로 사용법 몇 가지를 테스트해보도록 하겠다. 아래와 같이 몇가지 정규 표현식 예제들을 만들어 보았다. 정규 표현식은 매우 유용하지만 Cralwer 를 개발시에는 조금 불편한 부분들이 있을 수 있다. 이러한 경우 BeautifulSoup 라는 라이브러리를 사용하여 간단하게 사용할 수도 있지만, 정규표현식은 여전히 중요하다고 생각된다. \n",
    "- ^ : 시작 \n",
    "- $ : 끝\n",
    "- [] : 문자 , 예) [a-z]\n",
    "- {최소, 최대} : 예) [a-z]{2,3} \n",
    "- '+' : '{1,}'\n",
    "- '?' : '{0,1}'\n",
    "- '*' : '{0,}'  \n",
    "- '.' : 모든 문자 가능\n",
    "- \\d : 모든 숫자 \n",
    "- \\w : 모든 문자 \n",
    "- [^a] : a를 제외한 ! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.알파뱃 찾기\n",
      "IN : ['aaabbbcccddsgjs', '', 'adjkfeklsjdfk', '', 'jvsklfjsdklf', '', 'jsdfadffsdf', '']\n",
      "OUT : ['aaabbbcccddsgjs', '', 'adjkfeklsjdfk', '', 'jvsklfjsdklf', '', 'jsdfadffsdf', '']\n",
      "\n",
      "\n",
      "2.알파뱃 두단어 찾기\n",
      "IN : ['aaabb', 'bcccd', 'dsgjs', '', 'adjkf', 'eklsj', 'dfk', '', 'jvskl', 'fjsdk', 'lf', '', 'jsdfa', 'dffsd', 'f', '']\n",
      "OUT : ['aaabb', 'bcccd', 'dsgjs', '', 'adjkf', 'eklsj', 'dfk', '', 'jvskl', 'fjsdk', 'lf', '', 'jsdfa', 'dffsd', 'f', '']\n",
      "\n",
      "\n",
      "3.전화번호 패턴 추출\n",
      "IN : 010-9999-9999, 019-2222-4444, 082-1111-3333, 112, 02-111-1111, 3333-3333\n",
      "OUT : ['010-9999-9999', '019-2222-4444', '082-1111-3333', '02-111-1111']\n",
      "\n",
      "\n",
      "4.전화번호 패턴 추출 (or 문 사용)\n",
      "IN : 010-9999-9999, 019-2222-4444, 082-1111-3333, 112, 02-111-1111, 3333-3333\n",
      "OUT : ['010-9999-9999', '019-2222-4444', '082-1111-3333', '02-111-1111', '3333-3333']\n",
      "\n",
      "\n",
      "5.특정 패턴 제외하고 찾기\n",
      "IN : aaabbbcccddsgjs adjkfeklsjdfk jvsklfjsdklf jsdfadffsdf\n",
      "OUT : [' jsdfadffsdf']\n",
      "\n",
      "\n",
      "5.한글만 다 찾아 보기\n",
      "IN : <html><body><title>abcd</title><p>가나다라</p><p>마바사아</p></body></html>\n",
      "OUT : ['가나다라', '마바사아']\n",
      "\n",
      "\n",
      "6.Title Tag만 다 찾아보기\n",
      "IN : <html><body><title>abcd</title><p>가나다라</p><p>마바사아</p></body></html>\n",
      "OUT : ['abcd']\n",
      "\n",
      "\n",
      "7.P Tag만 다 찾아보기\n",
      "IN : <html><body><title>abcd</title><p>가나다라</p><p>마바사아</p></body></html>\n",
      "OUT : ['가나다라', '마바사아']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "\n",
    "data = \"aaabbbcccddsgjs adjkfeklsjdfk jvsklfjsdklf jsdfadffsdf\"\n",
    "tel_no_list = \"010-9999-9999, 019-2222-4444, 082-1111-3333, 112, 02-111-1111, 3333-3333\"\n",
    "virtual_html = \"<html><body><title>abcd</title><p>가나다라</p><p>마바사아</p></body></html>\"\n",
    "\n",
    "#알파뱃(a~z) 중에서 단어를 찾는다 \n",
    "reg = re.compile('[a-z]*')\n",
    "print(\"1.알파뱃 찾기\")\n",
    "print(\"IN : {0}\".format(reg.findall(data)))\n",
    "print(\"OUT : {0}\".format(reg.findall(data)))\n",
    "print('\\n')\n",
    "\n",
    "#알파뱃(a~z) 중에서 두단어를 찾는다 \n",
    "reg = re.compile('[a-z]{0,5}')\n",
    "print(\"2.알파뱃 두단어 찾기\")\n",
    "print(\"IN : {0}\".format(reg.findall(data)))\n",
    "print(\"OUT : {0}\".format(reg.findall(data)))\n",
    "print('\\n')\n",
    "\n",
    "# 특정 패턴에 해당하는 전화번호를 다 찾아보자 \n",
    "reg = re.compile('\\d{2,3}-\\d{3,4}-\\d{4,4}')\n",
    "print(\"3.전화번호 패턴 추출\")\n",
    "print(\"IN : {0}\".format(tel_no_list))\n",
    "print(\"OUT : {0}\".format(reg.findall(tel_no_list)))\n",
    "print('\\n')\n",
    "\n",
    "# 특정 패턴에 해당하는 전화번호를 다 찾아보자 \n",
    "# 3333-3333 도 찾아보자 \n",
    "reg = re.compile('(\\d{2,3}-\\d{3,4}-\\d{4,4}|\\d{3,4}-\\d{4,4})')\n",
    "out = reg.findall(tel_no_list)\n",
    "print(\"4.전화번호 패턴 추출 (or 문 사용)\")\n",
    "print(\"IN : {0}\".format(tel_no_list))\n",
    "print(\"OUT : {0}\".format(reg.findall(tel_no_list)))\n",
    "print('\\n')\n",
    "\n",
    "# 특정 문자를 제외하고 \n",
    "reg = re.compile('[^a]{1,1}[\\w]+$')\n",
    "print(\"5.특정 패턴 제외하고 찾기\")\n",
    "print(\"IN : {0}\".format(data))\n",
    "print(\"OUT : {0}\".format(reg.findall(data)))\n",
    "print('\\n')\n",
    "\n",
    "# 한글 전체 찾기\n",
    "reg = re.compile('[가-힣]{1,}')\n",
    "print(\"5.한글만 다 찾아 보기\")\n",
    "print(\"IN : {0}\".format(virtual_html))\n",
    "print(\"OUT : {0}\".format(reg.findall(virtual_html)))\n",
    "print('\\n')\n",
    "\n",
    "# title 태그 안에 있는 것만 다 가지고 와보자\n",
    "# (xxx) 은 추출이다 \n",
    "reg = re.compile('<title[^>]*>([^<]+)</title>')\n",
    "print(\"6.Title Tag만 다 찾아보기\")\n",
    "print(\"IN : {0}\".format(virtual_html))\n",
    "print(\"OUT : {0}\".format(reg.findall(virtual_html)))\n",
    "print('\\n')\n",
    "\n",
    "# p 태그 안에 있는 것만 다 가지고 와보자\n",
    "reg = re.compile('<p>([\\w]+)</p>')\n",
    "print(\"7.P Tag만 다 찾아보기\")\n",
    "print(\"IN : {0}\".format(virtual_html))\n",
    "print(\"OUT : {0}\".format(reg.findall(virtual_html)))\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BeautifulSoup 간단하게 연결해 보기\n",
    "아래는 간단하게 Naver.com 에 접근하여 Title Tag 값을 추출하는 예제이다. 보는 것처럼 requests 를 호출해서 HTML 데이터를 얻은 후 이 데이터를\n",
    "BeautifulSoup 를 사용해서 필요한 Tag 를 손쉽게 추출 할 수 있다. 정규식아라고 하면 아래와 같은 형태로 구현이 되어야 할 것이지만 라이브러리를 사용하면 더욱 손 쉽게 사용가능하다. <br>\n",
    "- reg = re.compile(\"<title[^>]*>([^<]+)</title>\") <br>\n",
    "- soup.find_all('title')<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<title>NAVER</title>\n"
     ]
    }
   ],
   "source": [
    "def crawler(iter) : \n",
    "    url = \"http://naver.com\"\n",
    "    source_code = requests.get(url)\n",
    "    plain_text = source_code.text\n",
    "    soup = BeautifulSoup(plain_text , 'lxml')\n",
    "    for raw in soup.find_all('title') : \n",
    "        print(raw)\n",
    "crawler(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BeautifulSoup 간단하게 Table Parsing 해보기\n",
    "HTML Table 을 Parsing 해서 간단하게 csv 로 바꿔보자. BeautifulSoup 이 얼마나 편한지 알 수 있다. 마치 Jquery 를 하는것 처럼 Elements 를 따라가면서 Find method 를 사용하고 마치 JQuery Iter 작업 처럼 복수의 요소를 쉽게 검색할 수 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv save done\n"
     ]
    }
   ],
   "source": [
    "# 엄청간단하게 table 을 파싱할 수 있다 \n",
    "def crawler_csv() : \n",
    "    url = \"https://race.kra.co.kr/raceScore/ObjtRaceRaceList.do?Act=04&Sub=3&meet=3\"\n",
    "    return_line = []\n",
    "    return_header = \"\"\n",
    "    return_td = \"\"\n",
    "    source_code = requests.get(url)\n",
    "    plain_text = source_code.text\n",
    "    soup = BeautifulSoup(plain_text, 'lxml')\n",
    "    \n",
    "    div = soup.find('div', class_=\"tableType2\")\n",
    "    for th in div.find_all('th') : \n",
    "        return_header = return_header + th.text + ','\n",
    "    for tr in div.find_all('tr') :\n",
    "        for td in tr.find_all('td') : \n",
    "            return_td = return_td + td.text + ','\n",
    "        return_td = return_td.rstrip(',') + '\\\\n'\n",
    "    return (return_header.rstrip(',') + '\\\\n' + return_td)\n",
    "    \n",
    "test_set = crawler_csv()\n",
    "# pandas 를 이용해서 컨버팅한 데이터가 정상인지 한번 확인해 보자\n",
    "test_data = StringIO(test_set)\n",
    "df = pd.read_csv(test_data, sep=\",\")\n",
    "df.to_csv(\"./table_1.csv\", sep=',', encoding='utf-8')\n",
    "print(\"csv save done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Site 에 존재하는 복수의 테이블을 저장해 보자\n",
    "물론 여기서는 Table , Tr, Td 의 전형적인 구조라고 가정한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def save_as_csv(data) :\n",
    "    \"\"\"\n",
    "    랜덤하게 csv 이름을 생성하여 주어진 데이터를 저장 \n",
    "    \"\"\"\n",
    "    rand_name = random.randrange(1,10000)\n",
    "    save_data = StringIO(data)\n",
    "    df = pd.read_csv(save_data, sep=\",\")\n",
    "    df.to_csv(\"./\" + str(rand_name) + \".csv\", sep=',', encoding='utf-8')\n",
    "    \n",
    "def table_to_csv(url) : \n",
    "    \"\"\"\n",
    "    전형적인 형태의 Table 을 Parsing 하여 csv 로 저장한다 \n",
    "    \"\"\"\n",
    "    return_line = []\n",
    "    return_td = \"\"\n",
    "    source_code = requests.get(url)\n",
    "    plain_text = source_code.text\n",
    "    soup = BeautifulSoup(plain_text, 'lxml')\n",
    "    for table in soup.find_all('table') :\n",
    "        for tr in table.find_all('tr') :\n",
    "            for td in table.find_all('td') :\n",
    "                return_td = return_td + td.text + ','\n",
    "            return_td = return_td.rstrip(',') + '\\\\n'\n",
    "        # save each table \n",
    "        save_as_csv(return_td)\n",
    "        return_td = \"\"\n",
    "    \n",
    "table_to_csv(\"https://ko.wikipedia.org/wiki/\")\n",
    "print(\"table csv save done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikipedia 한글 페이지 Link를 따라가면서 데이터 수집\n",
    "아주 간단한 WebCralwer 를 테스트 해보았다. 간단하게 WikiPedia 한글 사이트를 처음으로 시작해서 해당 사이트에 존재하는 \"P\" 태그를 수집하고 해당 페이지에서 존재하는 Link를 찾아서 이동하고, 해당 페이지에서 \"P\" 태그를 찾아서 저장하는 행위를 반복하는 코드이다.  <br>\n",
    "별도의 정규 표현식을 이용하고 싶을 경우에는 정규 표현식을 파라메터로 받아서 해당 정규 표현식으로 Crawler 작업을 실행한다. <br> \n",
    "메서드는 spider(2, 'https://ko.wikipedia.org/wiki/') 형태로 되어 있으며, 첫 번째 파레메터는 Inception Level로 몇번이나, Link를 따라 들어가서 Crawler 작업을 수행할 것인지를 지정하는 작업이고, 두 번째 파라메터는 시작할 사이트의 주소가 되겠다. 실제로 실행시 지정한 페이지뿐만 아니라 연결된 Link들을 계속 찾아서 필요한 데이터를 추출하는 것을 볼 수 있다. 세번째 파라메터로 정규 표현식을 입력 받으며, 해당 값이 있는 경우 정규 표현식을 활용하여 파싱 작업을 수행한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Job Start!!\n",
      "href : https://ko.wikipedia.org/wiki/\n",
      "file saved as : 3351\n",
      "file saved as : 8723\n",
      "file saved as : 1269\n",
      "file saved as : 5878\n",
      "href : https://ko.wiktionary.org/wiki/\n",
      "href : https://ko.wiktionary.org/wiki/\n",
      "href : https://ko.wikinews.org/wiki/\n",
      "href : https://ko.wikinews.org/wiki/\n",
      "href : https://ko.wikisource.org/wiki/\n",
      "href : https://ko.wikisource.org/wiki/\n",
      "href : https://ko.wikiversity.org/wiki/\n",
      "file saved as : 9810\n",
      "file saved as : 9706\n",
      "file saved as : 3516\n",
      "href : https://ko.wikiversity.org/wiki/\n",
      "file saved as : 4979\n",
      "file saved as : 4298\n",
      "file saved as : 4276\n",
      "href : https://ko.wikivoyage.org/wiki/\n",
      "file saved as : 2667\n",
      "href : https://ko.wikivoyage.org/wiki/%EB%8C%80%EB%AC%B8\n",
      "file saved as : 7796\n",
      "href : https://ko.wikiquote.org/wiki/\n",
      "file saved as : 8306\n",
      "file saved as : 7449\n",
      "file saved as : 2191\n",
      "file saved as : 9446\n",
      "file saved as : 6062\n",
      "href : https://ko.wikiquote.org/wiki/\n",
      "file saved as : 1220\n",
      "file saved as : 9043\n",
      "file saved as : 5517\n",
      "file saved as : 4447\n",
      "file saved as : 6692\n",
      "href : https://ko.wikibooks.org/wiki/\n",
      "href : https://ko.wikibooks.org/wiki/\n",
      "href : https://ko.wikipedia.org/w/index.php?title=위키백과:대문&oldid=15252069\n",
      "file saved as : 791\n",
      "file saved as : 6752\n",
      "file saved as : 6663\n",
      "file saved as : 3567\n",
      "href : https://ko.wikibooks.org/wiki/%EC%9C%84%ED%82%A4%EC%B1%85:%EB%8C%80%EB%AC%B8\n",
      "href : https://ko.wikinews.org/wiki/%EC%9C%84%ED%82%A4%EB%89%B4%EC%8A%A4:%EB%8C%80%EB%AC%B8\n",
      "href : https://ko.wikiquote.org/wiki/%EC%9C%84%ED%82%A4%EC%9D%B8%EC%9A%A9%EC%A7%91:%EB%93%A4%EB%A8%B8%EB%A6%AC\n",
      "file saved as : 501\n",
      "file saved as : 9595\n",
      "file saved as : 4611\n",
      "file saved as : 3811\n",
      "file saved as : 3810\n",
      "href : https://ko.wikisource.org/wiki/%EC%9C%84%ED%82%A4%EB%AC%B8%ED%97%8C:%EB%8C%80%EB%AC%B8\n",
      "href : https://ko.wikiversity.org/wiki/%EC%9C%84%ED%82%A4%EB%B0%B0%EC%9B%80%ED%84%B0:%EB%8C%80%EB%AC%B8\n",
      "file saved as : 2574\n",
      "file saved as : 7101\n",
      "file saved as : 331\n",
      "href : https://ko.wiktionary.org/wiki/%EC%9C%84%ED%82%A4%EB%82%B1%EB%A7%90%EC%82%AC%EC%A0%84:%EB%8C%80%EB%AC%B8\n",
      "# Job Done!!\n"
     ]
    }
   ],
   "source": [
    "def task(page, max_pages, url_path, file_w, reg = None):\n",
    "    \"\"\"\n",
    "    지정된 수만큼 제귀 형태로 모든 링크를 따라가서 전부 수집한다. \n",
    "    \"\"\"\n",
    "    if page == max_pages :\n",
    "        get_single_article(url_path, file_w, reg_exp=str(reg))\n",
    "        table_to_csv(url_path)\n",
    "    else : \n",
    "        get_single_article(url_path, file_w, reg_exp=str(reg))\n",
    "        table_to_csv(url_path)\n",
    "        source_code = requests.get(url_path)\n",
    "        plain_text = source_code.text\n",
    "        soup = BeautifulSoup(plain_text, 'lxml')\n",
    "        page += 1\n",
    "        for link in soup.find_all('a'):\n",
    "            href = link.get('href')\n",
    "            if (href != None and re.search(\"https://ko\", href)) : \n",
    "                task(page, max_pages, href, file_w, reg=str(reg))\n",
    "\n",
    "def get_single_article(item_url, file_w, reg_exp = None):\n",
    "    \"\"\"\n",
    "    p 태그를 가지고와서 파싱하거나 \n",
    "    지정된 reg_exp 를 사용하여 파싱한다 \n",
    "    \"\"\"\n",
    "    print(\"href : {0}\".format(item_url))\n",
    "    source_code = requests.get(item_url)\n",
    "    plain_text = source_code.text\n",
    "    soup = BeautifulSoup(plain_text, 'lxml')\n",
    "    \n",
    "    if(reg_exp) : \n",
    "        #정규 표현식이 있는 경우 해당 정규 표현식에 맞는 데이터를 추출 \n",
    "        reg = re.compile(reg_exp)\n",
    "        for contents in reg.findall(plain_text):\n",
    "            file_w.write(contents)\n",
    "    else : \n",
    "        #별도의 Regex가 없는 경우 p tag 에 있는 모든 데이터 추출 \n",
    "        for contents in soup.find_all('p'):\n",
    "            file_w.write(contents.text)\n",
    "\n",
    "def spider(max_pages, url_path, path = \"/home/dev/wiki/\", file_name='test.txt', reg_exp = None) :\n",
    "    \"\"\"\n",
    "    본 Function 을 실행하면 WikiPedia 첫 페이지에서 실행해서 \n",
    "    지정된 횟수만큼 페이지를 따라 들어가서 정해진 패턴을 수집한다. \n",
    "    max_pages : 몇번 Page를 따라 들어갈 것인가를 정의하는 변수 \n",
    "    \"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    with open(''.join([path, file_name]), \"w\") as file_w :   \n",
    "        print(\"# Job Start!!\")\n",
    "        task(1, max_pages, url_path, file_w, reg = reg_exp)\n",
    "        print(\"# Job Done!!\")\n",
    "\n",
    "def save_as_csv(data) :\n",
    "    \"\"\"\n",
    "    랜덤하게 csv 이름을 생성하여 주어진 데이터를 저장 \n",
    "    \"\"\"\n",
    "    rand_name = random.randrange(1,10000)\n",
    "    save_data = StringIO(data)\n",
    "    df = pd.read_csv(save_data, sep=\",\")\n",
    "    df.to_csv(\"./\" + str(rand_name) + \".csv\", sep=',', encoding='utf-8')\n",
    "    print(\"file saved as : {0}\".format(str(rand_name)))\n",
    "    \n",
    "def table_to_csv(url) : \n",
    "    \"\"\"\n",
    "    전형적인 형태의 Table 을 Parsing 하여 csv 로 저장한다 \n",
    "    \"\"\"\n",
    "    try : \n",
    "        return_line = []\n",
    "        return_td = \"\"\n",
    "        source_code = requests.get(url)\n",
    "        plain_text = source_code.text\n",
    "        soup = BeautifulSoup(plain_text, 'lxml')\n",
    "        for table in soup.find_all('table') :\n",
    "            for tr in table.find_all('tr') :\n",
    "                for td in table.find_all('td') :\n",
    "                    return_td = return_td + td.text + ','\n",
    "                return_td = return_td.rstrip(',') + '\\\\n'\n",
    "            # save each table \n",
    "            save_as_csv(return_td)\n",
    "            return_td = \"\"\n",
    "    except Exception as e : \n",
    "        return True\n",
    "        \n",
    "# 주어진 횟수만큼 해당 사이트를 시작으로 크롤링 시작 \n",
    "# first parm : Inception 횟수 \n",
    "# second parm : initial site \n",
    "# reg_exp : 정규 표현식 사용 가능 \n",
    "\n",
    "# (1) 정규 표현식 사용 CASE (한글 전체 추출)\n",
    "spider(2, 'https://ko.wikipedia.org/wiki/', reg_exp ='[가-힣\\s]{1,}')\n",
    "#spider(2, 'https://ko.wikipedia.org/wiki/', reg_exp ='<title[^>]*>([^<]+)</title>')\n",
    "\n",
    "# (2) 정규 표현식 사용하지 않고 P 태크 추출\n",
    "#spider(2, 'https://ko.wikipedia.org/wiki/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
