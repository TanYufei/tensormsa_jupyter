{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Vector를 통한 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['안녕', '만나', '서', '반가워', '넌', '누구', '니', '나', '는', 'AI', '봇', '이', '란다', '.']\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Mecab\n",
    "\n",
    "qna_data = [\n",
    "                ['안녕', '만나서 반가워']\n",
    "                ,['넌누구니', '나는 AI 봇이란다.']\n",
    "#                 ,['피자 주문 할께', '페파로니 주문해줘']\n",
    "#                ,['음료는 멀로', '콜라로 해줘']\n",
    "            ]\n",
    "mecab = Mecab('/usr/local/lib/mecab/dic/mecab-ko-dic')\n",
    "\n",
    "train_data = list(map(lambda x : mecab.morphs(' '.join(x)) , qna_data))\n",
    "\n",
    "import itertools\n",
    "train_data = list(itertools.chain.from_iterable(train_data))\n",
    "#train_data = list(set(train_data))\n",
    "\n",
    "print(list(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "[ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "[ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "[ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "[ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "[ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "[ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "bucket = np.zeros(len(train_data), dtype=np.float)\n",
    "\n",
    "for word in train_data :\n",
    "    bucket_temp = bucket.copy()\n",
    "    np.put(bucket_temp, train_data.index(word), 1)\n",
    "    print(bucket_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word to Vector (By Gensim)\n",
    "### W2V를 통해 출력해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['안녕', '만나', '서', '반가워', '넌', '누구', '니', '나', '는', 'AI', '봇', '이', '란다', '.']]\n",
      "model check : Word2Vec(vocab=14, size=50, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "train_data = [train_data]\n",
    "print(train_data)\n",
    "\n",
    "model = word2vec.Word2Vec(size=50, window=2, min_count=1)\n",
    "model.build_vocab(train_data)\n",
    "model.train(train_data)\n",
    "print(\"model check : {0}\".format(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model load check : Word2Vec(vocab=14, size=50, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "import os  \n",
    "file_path = './model'\n",
    "if not os.path.exists(file_path):\n",
    "    os.makedirs(file_path)\n",
    "model.save(file_path + \"/w2v.bin\")\n",
    "model = word2vec.Word2Vec.load(\"./model/w2v.bin\")\n",
    "print(\"model load check : {0}\".format(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['나', '는', '니', '.', '서', '란다', '반가워', '넌', '누구', '봇', '이', 'AI', '안녕', '만나']\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'vocab' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-23d881892cd7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex2word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'vocab' is not defined"
     ]
    }
   ],
   "source": [
    "print(model.wv.index2word)\n",
    "X = model[vocab]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 안녕, AI등 값의 Vector값 출력 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00759181 -0.0003396   0.00920007 -0.00732685 -0.0073232  -0.00331754\n",
      "  0.00489599  0.00528371  0.00610048  0.0074512  -0.00556528  0.00851572\n",
      " -0.00567806 -0.00255054 -0.00983929  0.00884408 -0.00646101 -0.00799495\n",
      " -0.00815    -0.00513075 -0.00993239 -0.00930947 -0.00724646 -0.00952602\n",
      "  0.00053064 -0.00427199  0.00522723 -0.00521637 -0.00087569  0.00935656\n",
      " -0.00459941 -0.00517498  0.00479802  0.00868245  0.00539141 -0.00147727\n",
      "  0.00581742  0.00830147  0.00336319  0.00915079 -0.00435504 -0.00662863\n",
      " -0.00674443 -0.00810368  0.00080349  0.00745528  0.00570942 -0.00193492\n",
      " -0.00854987  0.00456542]\n"
     ]
    }
   ],
   "source": [
    "print(model['안녕'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(model['AI'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result1 = model.most_similar(positive='나', negative='', topn=10)\n",
    "print(result1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "font_name = matplotlib.font_manager.FontProperties(\n",
    "                fname=\"/usr/share/fonts/truetype/nanum/NanumGothic.ttf\"  # 한글 폰트 위치를 넣어주세요\n",
    "            ).get_name()\n",
    "vocab = model.wv.index2word\n",
    "matplotlib.rc('font', family=font_name)\n",
    "tsne = TSNE(n_components=2)\n",
    "X_tsne = tsne.fit_transform(X) #t-분포 확률적 임베딩(t-SNE)은 데이터의 차원 축소에 사용\n",
    "df = pd.concat([pd.DataFrame(X_tsne),\n",
    "                pd.Series(vocab)],\n",
    "               axis=1)\n",
    "\n",
    "df.columns = ['x', 'y', 'word']\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "print(df)\n",
    "ax.scatter(df['x'], df['y'])\n",
    "ax.set_xlim(df['x'].max(), df['x'].min())\n",
    "ax.set_ylim(df['y'].max(), df['y'].min())\n",
    "for i, txt in enumerate(df['word']):\n",
    "    ax.annotate(txt, (df['x'].iloc[i], df['y'].iloc[i]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
