{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 워드 임베딩 실습\n",
    "\n",
    "- 딥러닝을 사용한 워드 임베딩\n",
    "- N-Gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 딥러닝을 사용한 워드 임베딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f7b346d3258>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#-*- coding: utf-8 -*-\n",
    "\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "-2.9718  1.7070 -0.4305 -2.2820  0.5237\n",
      "[torch.FloatTensor of size 1x5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "word_to_ix = {\"hello\": 0, \"world\": 1}\n",
    "embeds = nn.Embedding(2, 5)  # the vocabulary size = 2 , dimensionality of the embeddings = 5 \n",
    "lookup_tensor = torch.LongTensor([word_to_ix[\"hello\"]])\n",
    "hello_embed = embeds(autograd.Variable(lookup_tensor))\n",
    "print(hello_embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. N-Gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터(Shakespeare Sonnet 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_sentence = \"\"\"When forty winters shall besiege thy brow,\n",
    "And dig deep trenches in thy beauty's field,\n",
    "Thy youth's proud livery so gazed on now,\n",
    "Will be a totter'd weed of small worth held:\n",
    "Then being asked, where all thy beauty lies,\n",
    "Where all the treasure of thy lusty days;\n",
    "To say, within thine own deep sunken eyes,\n",
    "Were an all-eating shame, and thriftless praise.\n",
    "How much more praise deserv'd thy beauty's use,\n",
    "If thou couldst answer 'This fair child of mine\n",
    "Shall sum my count, and make my old excuse,'\n",
    "Proving his beauty by succession thine!\n",
    "This were to be new made when thou art old,\n",
    "And see thy blood warm when thou feel'st it cold.\"\"\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CONTEXT_SIZE = 2\n",
    "EMBEDDING_DIM = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['When', 'forty'], 'winters'), (['forty', 'winters'], 'shall'), (['winters', 'shall'], 'besiege')]\n"
     ]
    }
   ],
   "source": [
    "# build a list of tuples.  Each tuple is ([ word_i-2, word_i-1 ], target word)\n",
    "trigrams = [([test_sentence[i], test_sentence[i + 1]], test_sentence[i + 2])\n",
    "            for i in range(len(test_sentence) - 2)]\n",
    "\n",
    "print(trigrams[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NGramLanguageModeler(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(NGramLanguageModeler, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs).view((1, -1))\n",
    "        out = F.relu(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "        log_probs = F.log_softmax(out)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = set(test_sentence)\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
    "\n",
    "losses = []\n",
    "loss_function = nn.NLLLoss()\n",
    "model = NGramLanguageModeler(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Shall': 0, 'warm': 1, 'all': 2, 'held:': 3, 'trenches': 4, 'much': 5, 'it': 6, 'And': 7, 'own': 8, 'old,': 9, 'thine!': 10, 'make': 11, 'treasure': 12, 'Then': 13, 'Where': 14, 'couldst': 15, 'within': 16, 'in': 17, 'were': 18, 'cold.': 19, 'praise.': 20, 'proud': 21, 'blood': 22, 'use,': 23, 'so': 24, \"'This\": 25, 'besiege': 26, 'thriftless': 27, 'shall': 28, 'lies,': 29, 'winters': 30, 'Proving': 31, \"totter'd\": 32, 'where': 33, 'say,': 34, 'my': 35, 'an': 36, 'on': 37, 'child': 38, 'worth': 39, \"beauty's\": 40, 'How': 41, 'praise': 42, 'his': 43, 'art': 44, \"feel'st\": 45, 'mine': 46, 'Will': 47, 'succession': 48, 'field,': 49, 'all-eating': 50, 'more': 51, 'the': 52, 'thy': 53, 'thine': 54, 'shame,': 55, 'be': 56, 'sum': 57, 'eyes,': 58, 'When': 59, 'Thy': 60, 'deep': 61, 'lusty': 62, 'by': 63, 'and': 64, 'fair': 65, 'answer': 66, 'sunken': 67, 'when': 68, 'Were': 69, 'forty': 70, 'count,': 71, 'brow,': 72, 'of': 73, 'made': 74, \"deserv'd\": 75, 'thou': 76, 'If': 77, 'This': 78, 'old': 79, 'beauty': 80, \"youth's\": 81, 'being': 82, 'days;': 83, 'small': 84, 'see': 85, 'now,': 86, 'dig': 87, 'asked,': 88, 'to': 89, 'gazed': 90, \"excuse,'\": 91, 'To': 92, 'livery': 93, 'a': 94, 'weed': 95, 'new': 96}\n"
     ]
    }
   ],
   "source": [
    "print(word_to_ix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습\n",
    "- 실습을 위해 iter를 조정합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      " 519.3522\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 517.0147\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 514.6943\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 512.3885\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 510.0958\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 507.8158\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 505.5493\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 503.2964\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 501.0559\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 498.8267\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 496.6071\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 494.3968\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 492.1956\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 490.0011\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 487.8141\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 485.6337\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 483.4597\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 481.2915\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 479.1304\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 476.9745\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 474.8228\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 472.6739\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 470.5281\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 468.3846\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 466.2434\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 464.1051\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 461.9667\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 459.8299\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 457.6941\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 455.5580\n",
      "[torch.FloatTensor of size 1]\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(20):\n",
    "    total_loss = torch.Tensor([0])\n",
    "    for context, target in trigrams:\n",
    "\n",
    "        # Step 1. Prepare the inputs to be passed to the model (i.e, turn the words\n",
    "        # into integer indices and wrap them in variables)\n",
    "        context_idxs = [word_to_ix[w] for w in context]\n",
    "        context_var = autograd.Variable(torch.LongTensor(context_idxs))\n",
    "\n",
    "        # Step 2. Recall that torch *accumulates* gradients. Before passing in a\n",
    "        # new instance, you need to zero out the gradients from the old\n",
    "        # instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 3. Run the forward pass, getting log probabilities over next\n",
    "        # words\n",
    "        log_probs = model(context_var)\n",
    "\n",
    "        # Step 4. Compute your loss function. (Again, Torch wants the target\n",
    "        # word wrapped in a variable)\n",
    "        loss = loss_function(log_probs, autograd.Variable(\n",
    "            torch.LongTensor([word_to_ix[target]])))\n",
    "\n",
    "        # Step 5. Do the backward pass and update the gradient\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.data\n",
    "    losses.append(total_loss)\n",
    "\n",
    "print(losses)  # The loss decreased every iteration over the training data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 결과물"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      " 0.8419  0.5512  0.3871  0.9134 -0.8409  1.2282 -1.8657  1.4142 -1.8785 -0.4687\n",
      "-0.7577  0.4223 -0.4822 -1.1195  0.3057  1.0374  0.5202 -0.5010  1.2187  0.2106\n",
      "-1.0640 -1.9448 -0.9589  0.5472 -0.9890 -0.3833  1.5045  1.8272  0.5541  1.6437\n",
      " 0.4978 -1.5070  1.7673 -0.3574 -0.1696  0.4062 -0.4276 -1.1283  1.4272 -1.4026\n",
      " 1.4831 -1.1568  1.6193  0.9586  0.7752  0.1948  0.1682  0.3063  1.0745 -1.0313\n",
      " 1.0930  0.7783 -1.3134  0.7099  0.9952 -0.2707 -0.6485 -0.1365 -0.2953 -0.7731\n",
      "-0.2218  0.5075 -0.6797 -1.6115  0.5227 -0.8885  0.2617  0.0303  0.0020 -1.3986\n",
      " 1.4673 -0.1030 -0.0089 -0.8413 -0.2057  1.0665  0.1739 -0.6861  0.3119  0.2359\n",
      "-1.0648  0.3628  0.3775 -0.2448 -0.5841  2.0831 -0.1183  0.4896  0.8347  0.8889\n",
      " 0.4152  0.0515 -0.9640 -2.0121  0.5246  2.1330 -0.0822  0.8390 -1.3226  0.0709\n",
      " 1.2214  0.4264 -1.2325 -0.6191  1.5139  1.9958 -0.6577 -0.4147 -0.2250 -0.6903\n",
      " 0.9886  0.7419 -2.0984  1.2585 -0.3983 -1.0958 -1.0704  0.6409  1.6194  0.5258\n",
      "-0.2960 -0.0689 -0.2834 -0.4702 -1.7654 -0.1657  0.2333 -0.0836 -1.7728 -1.0717\n",
      " 1.0253 -0.7111  0.7080  0.8286  1.3516  1.6195  0.3428 -0.9127 -0.9970  0.7446\n",
      " 0.7373  1.2530  0.8521 -0.4153 -0.7494  1.0635  0.0067 -1.4262 -0.0792 -0.5153\n",
      " 1.1384 -1.0246 -1.0303 -1.0124  0.0050 -0.9351 -0.9869  1.3794 -0.1176  0.9321\n",
      " 1.3276 -1.0170 -1.8586  0.9017  0.1495 -0.0331 -0.6070 -1.0049 -0.2830 -0.2711\n",
      " 1.3200  1.1595  0.3464 -0.1142 -0.8910  0.2904 -2.1006 -1.1267 -0.8205  0.5334\n",
      " 0.1395  1.6893  1.4115 -0.9810 -0.7574 -0.3785  1.7211  0.0314 -0.4262 -0.3865\n",
      "-0.6089  1.1652 -0.1326 -0.0228  1.1848 -1.0322 -0.7039  0.8813  1.4276 -0.9245\n",
      "-0.8022 -0.7857  0.7889  0.0779  1.7055 -0.8091 -0.4602 -1.1796  0.3810 -0.0064\n",
      " 0.5297  1.0000  1.0741  1.2505  0.5958 -0.2692 -0.3677 -1.0544  0.6615  0.2926\n",
      " 0.0404 -1.1907  0.0159 -0.6842  0.2815 -0.1486 -0.2897 -1.1742  0.4371  0.4335\n",
      " 0.9966  0.1272  0.4702  0.0782  1.1762  0.2104  0.0112  0.4638  0.5253 -0.6375\n",
      "-0.7037 -0.1749 -2.4775 -0.8545  0.4415 -0.6764  0.5180  0.5116 -0.0799  0.4308\n",
      "-1.1150 -0.6668  1.0212 -0.1980 -0.8871 -0.3573  1.8174  0.2123  0.2581 -0.7853\n",
      " 0.1686  1.5806  0.4844 -1.2906 -0.8034  0.6841  1.0929 -1.2616 -0.0204 -1.0574\n",
      "-0.9927 -0.3662  0.6216  0.7164  0.5360  0.2130  1.7015  1.6244  2.1412  0.6156\n",
      " 1.3624  1.0907  0.2115  1.3689 -0.0625 -0.2634 -1.3554 -0.5342  0.5242  0.0413\n",
      " 0.7501  0.3636  0.9266 -0.1907 -0.7291 -0.3330  0.1135  0.3760 -0.0080  0.5749\n",
      "-0.0232 -0.5942  0.7936  2.2267 -0.8402 -0.4719  0.3145 -0.0600  0.9406 -1.3481\n",
      " 0.0549  0.8413 -0.6757  0.0261 -2.0638 -1.9366  1.0068 -1.8596  0.9321  1.4070\n",
      " 1.4418  0.1695  0.2571  0.1210 -1.8285  0.1572 -1.3312 -1.0497 -1.0000 -0.4616\n",
      "-0.5059  1.1230  0.4811 -0.0334 -0.4917 -0.2713 -0.8704  0.8152 -0.6620 -0.2194\n",
      "-0.0649 -2.2278 -0.5411 -0.9733 -0.0503  0.5302  1.5530  0.6884 -0.4736  0.5036\n",
      "-0.2688 -0.2849  0.1122  0.4352  0.0082 -0.5959  0.8591  0.3824 -0.0273  0.4463\n",
      " 0.1881 -0.5228  0.8039  2.1669 -0.8393 -1.0083 -1.8908 -0.2953  1.2427 -0.6366\n",
      "-0.0744  0.8817  1.4779  0.6493 -0.1139 -1.2110  0.2268 -1.6175  0.5023 -0.3299\n",
      " 1.3859 -0.6321  1.0502  0.0922  0.1582  1.6082 -1.2482 -1.1269  0.9566 -0.3174\n",
      " 2.1682 -0.7003 -0.6324 -0.2337  0.1107 -0.0632 -1.4384 -1.7744 -0.6433  0.5318\n",
      "-0.6026  0.0538 -0.1756 -0.1357 -1.0424 -0.3361  1.6250 -0.8132  0.8350 -0.4689\n",
      "-1.0827  0.8417  0.2602 -0.0598  0.4310  0.8315  0.2839  0.2886  1.0167  1.1540\n",
      "-0.1600 -0.0068  1.0992  0.1498  2.4151 -0.8188  0.4203 -0.7979  1.1250  0.1077\n",
      " 0.2103  0.4222 -0.5832  1.0282  1.4156  1.5278  0.4514 -1.1519 -0.4562  0.6719\n",
      " 0.8702 -0.3428 -1.8527 -1.0493  0.5368 -0.5130  0.1242 -0.1820  2.1539 -1.4562\n",
      "-1.0259  0.2645 -0.8839 -0.2659  0.3939 -1.2512 -0.1165  0.5074 -2.4449  0.1493\n",
      "-0.7048 -0.5058  2.4595  0.2425 -0.5838 -0.3888 -0.6918  0.3076  0.7025 -1.8685\n",
      "-0.2921  0.6719  1.4365 -1.1454 -1.1067 -0.5894  0.2624  0.5941  0.3158 -1.2951\n",
      "-0.2393 -0.2045  0.1694  0.0594 -0.2801  0.1548 -0.4350 -0.9131  1.1431 -0.6418\n",
      " 2.0117  0.0048  1.7369 -0.2558 -0.5469  0.5372  0.4083 -0.0681 -0.4626 -0.3570\n",
      " 0.3252 -0.6034 -0.9263 -0.4697 -0.6414 -0.6657 -0.3443  1.5127 -1.3217 -0.8004\n",
      " 0.0005 -0.3938  0.1719 -0.2114  0.0560 -1.7639 -0.6886 -2.0346  0.0638 -0.0724\n",
      "-0.3465  0.7033 -0.5362 -0.9871 -0.9008  0.2871 -0.5477  0.5016 -0.1833  0.1161\n",
      "-1.0038  0.7193 -0.2157  0.4826 -0.9245 -0.8911 -0.5524  0.2562  1.0314 -0.1705\n",
      "-0.6643 -1.3652  0.2547  0.7596 -0.1275  0.0626 -0.3911  0.5878  0.0741 -0.2441\n",
      " 1.7045 -1.6001  1.4356 -0.9924 -0.6298 -1.0296 -0.0225  0.1802 -0.0121  0.8424\n",
      " 0.7629 -0.9871 -1.9528 -0.3464  0.2069 -0.6052 -0.3747 -0.1854 -0.0312 -0.2985\n",
      "-1.0759 -0.1290  0.0257 -0.1957 -0.3387 -0.1554 -1.7851  0.3923 -0.7824  1.1888\n",
      " 1.5654  0.7147 -1.1257  1.0845  0.6466  0.3528  0.3006 -0.0332  0.3378  0.7398\n",
      " 0.7582 -1.8523  0.2757 -0.2750 -0.5543 -1.3611 -0.5837 -0.2720  0.6453  1.0307\n",
      "-2.1171  0.4466 -0.6391  0.9373  0.3032  1.9143 -1.5969 -1.0497 -0.2092  0.5072\n",
      " 1.0765 -0.6242  0.9471 -0.6024 -0.0318  0.7030  0.8257  0.6921  0.4011  1.0715\n",
      "-0.8990  0.0030 -0.0170 -0.1328  0.3078  1.1467  0.0178  1.4959  0.2016 -0.2712\n",
      "-0.5071  0.2956 -1.0124 -0.9121  0.0907  0.8714  1.6868  1.2556 -1.7581 -0.1753\n",
      "-0.6920  0.1006  0.3846  0.5473 -1.0903 -1.0460 -0.5610 -0.1578 -0.6770 -0.9849\n",
      " 1.8310  2.3476  0.0041 -0.5593  0.0533  0.3100 -2.1251 -0.2587  0.3488 -1.5471\n",
      " 0.8527  0.1202 -0.4494  0.8900  0.2874 -0.2938  0.5229 -0.7714 -0.0439 -0.0104\n",
      " 0.9356 -1.0419  0.6350 -0.2071  0.6474 -1.2422  0.5360 -0.6981  0.2526  0.1821\n",
      "-0.9325 -1.3054 -0.4879 -1.6151 -0.7887 -0.6529  0.4792 -1.4866  1.2966  0.2869\n",
      " 0.3344 -2.6221 -0.0977 -0.8448 -0.0518  0.8658 -0.0317  0.7146 -1.0059 -1.1028\n",
      "-0.9791  1.4348  0.3459 -1.1477 -1.3034  0.4591  0.0628 -0.2998  1.0860 -0.0644\n",
      "-0.8138  2.4927  0.5874  0.7389  1.0346 -0.7374 -0.9933 -0.2658  2.5696 -0.4109\n",
      "-0.4399 -0.5106  3.1403 -0.1199 -1.4082 -0.4195 -1.7946 -0.3008  0.7302  0.8310\n",
      "-1.2888  1.6469  1.2170  0.2036  0.7008  1.1098 -0.0611 -2.2332  0.8529  0.1643\n",
      "-1.0759  1.1975  1.1731 -1.3764 -0.4026 -1.0712 -0.4187 -1.0779  0.7224  1.7773\n",
      "-1.1901  0.4534 -0.6914 -0.3892  1.1515 -0.0734  0.2300  0.7706  0.0092  0.7143\n",
      "-0.1114  1.4385  0.0035 -2.2527 -0.6464  0.1776  1.9284  0.7215 -1.1174 -0.0588\n",
      " 0.3807  1.5618  0.2543 -0.8108 -0.3409  1.1071  0.6636  0.1183 -0.3584 -0.2387\n",
      " 0.3354 -0.5001 -0.8668  0.8043 -0.0023 -1.7603 -2.1581 -0.1513 -0.6815 -0.1798\n",
      " 1.5828 -0.4140  0.2569 -0.7290  1.2624  0.2600 -0.1349 -0.5165 -0.9409  0.2090\n",
      " 0.3930 -0.1340  0.2115  1.1702 -0.1647  1.5295  0.9630  0.5423 -0.8546  0.4110\n",
      " 1.4247  1.1643 -0.2031 -0.2273  0.3237 -0.9359 -0.3638 -1.0472  0.1650 -1.6224\n",
      "-1.2037  1.8327  0.4551 -0.6646 -1.6631  0.7980  0.6561 -1.3065 -0.1054 -0.1067\n",
      " 1.0123  1.2796  1.1979  0.6441  1.4983  0.1739  1.2091  0.5385 -1.2606  0.3354\n",
      " 0.7163  0.6189  2.4869  0.4385  0.0894 -2.9447 -0.7811  0.6073 -0.0375 -1.9012\n",
      "-2.7121  0.8363 -1.2625  0.4090 -1.4764  0.2065 -0.4192  0.0692  1.0101 -2.9165\n",
      "-1.4221  1.0021  2.6586 -1.8148  1.4165  0.3350  0.0409 -0.2740  1.1227  1.1320\n",
      "-1.1224 -1.0333  1.3682  0.0897 -0.4600 -0.1131  0.8126  0.0160  1.6493 -0.5230\n",
      " 1.7390 -1.1784  0.3397 -1.1465  1.3662 -0.8056  0.2448  0.3184  0.7583  1.1571\n",
      " 0.7285  1.7622 -1.1077 -1.0735  1.0552 -0.7094  1.9833 -0.1246 -0.3996 -1.6058\n",
      "-0.0891 -0.7789  3.4451 -2.2090 -1.0602  0.7625  0.0006  1.4285  0.5371  1.0284\n",
      " 1.5354  1.4325  0.7806 -1.8804  1.1709  1.5458 -0.1281  1.4386  0.5583 -1.2050\n",
      "-0.3034  0.7546  3.0760 -0.3415 -0.1866  1.7839 -1.1541 -0.2472 -0.6778  1.5809\n",
      " 0.4368 -0.2743 -1.5070  1.6756 -0.2764  0.1223 -0.7173 -0.0558  1.7813 -0.7203\n",
      " 0.8389  0.1653 -0.2363 -1.1260  1.1225 -0.9162  0.5489  0.0948 -0.8466 -0.1183\n",
      "-0.9605  1.8551  1.3255 -1.6128 -1.1226 -0.4331 -0.2892 -0.7324 -1.9703  0.6026\n",
      "-0.8122 -0.8472 -0.2246  0.8721  2.1184  0.1235 -0.5990 -0.3140 -0.6964  1.7148\n",
      "[torch.FloatTensor of size 97x10]\n",
      "\n",
      "Parameter containing:\n",
      " 0.0116  0.1524  0.1897  ...   0.0629 -0.1215 -0.1625\n",
      "-0.1846 -0.0395 -0.1239  ...  -0.1485 -0.1195  0.1698\n",
      " 0.0337 -0.0060 -0.0712  ...   0.1899  0.2255 -0.1728\n",
      "          ...             ⋱             ...          \n",
      " 0.2181  0.1003 -0.0228  ...  -0.0344  0.2059  0.0237\n",
      "-0.1038  0.1563  0.0513  ...   0.0054 -0.1563 -0.1950\n",
      "-0.0474 -0.0592  0.2068  ...  -0.0063  0.0950 -0.1398\n",
      "[torch.FloatTensor of size 128x20]\n",
      "\n",
      "Parameter containing:\n",
      " 0.0687\n",
      "-0.2182\n",
      "-0.0172\n",
      "-0.0730\n",
      " 0.0963\n",
      " 0.0699\n",
      "-0.1762\n",
      "-0.1487\n",
      " 0.1969\n",
      " 0.0806\n",
      " 0.1557\n",
      "-0.1705\n",
      " 0.0735\n",
      " 0.1874\n",
      "-0.0545\n",
      " 0.1099\n",
      "-0.0342\n",
      " 0.1963\n",
      "-0.0065\n",
      " 0.2148\n",
      " 0.2004\n",
      " 0.0626\n",
      " 0.1659\n",
      "-0.0775\n",
      "-0.0098\n",
      "-0.1370\n",
      " 0.0216\n",
      " 0.2098\n",
      " 0.1672\n",
      "-0.0076\n",
      " 0.2045\n",
      " 0.0316\n",
      " 0.0626\n",
      " 0.0918\n",
      " 0.1095\n",
      " 0.1027\n",
      "-0.0644\n",
      "-0.2107\n",
      " 0.1899\n",
      "-0.1244\n",
      " 0.1535\n",
      "-0.2033\n",
      " 0.0294\n",
      "-0.1785\n",
      " 0.1642\n",
      " 0.1457\n",
      " 0.1544\n",
      "-0.2157\n",
      " 0.1395\n",
      " 0.0347\n",
      "-0.1638\n",
      " 0.2052\n",
      " 0.0186\n",
      " 0.1355\n",
      " 0.1808\n",
      "-0.1874\n",
      " 0.1250\n",
      " 0.2067\n",
      "-0.0408\n",
      " 0.0905\n",
      " 0.2158\n",
      " 0.0677\n",
      "-0.1075\n",
      "-0.0752\n",
      "-0.0786\n",
      " 0.2088\n",
      " 0.2047\n",
      "-0.1465\n",
      " 0.1424\n",
      " 0.0635\n",
      " 0.1805\n",
      " 0.1493\n",
      "-0.2210\n",
      "-0.0657\n",
      "-0.0495\n",
      "-0.0574\n",
      "-0.1195\n",
      " 0.1845\n",
      "-0.0940\n",
      "-0.0938\n",
      " 0.1109\n",
      " 0.1326\n",
      " 0.0949\n",
      " 0.1197\n",
      "-0.0149\n",
      " 0.0556\n",
      "-0.0645\n",
      "-0.0945\n",
      " 0.0393\n",
      " 0.1697\n",
      " 0.0119\n",
      "-0.1147\n",
      " 0.0213\n",
      "-0.0021\n",
      " 0.0535\n",
      "-0.1687\n",
      "-0.0467\n",
      "-0.1373\n",
      " 0.1211\n",
      "-0.0662\n",
      "-0.1099\n",
      " 0.0141\n",
      " 0.0626\n",
      "-0.0535\n",
      " 0.2063\n",
      "-0.1272\n",
      "-0.2267\n",
      " 0.1709\n",
      " 0.0933\n",
      " 0.1829\n",
      "-0.1795\n",
      " 0.1447\n",
      " 0.0211\n",
      " 0.0695\n",
      " 0.0546\n",
      "-0.0414\n",
      "-0.1169\n",
      "-0.1160\n",
      " 0.1867\n",
      " 0.0835\n",
      " 0.0558\n",
      " 0.0929\n",
      "-0.1176\n",
      "-0.1954\n",
      " 0.1204\n",
      "-0.0107\n",
      "-0.1746\n",
      " 0.0285\n",
      "[torch.FloatTensor of size 128]\n",
      "\n",
      "Parameter containing:\n",
      " 7.8310e-03  4.8064e-02 -6.9007e-02  ...   3.0556e-02  8.8457e-02 -1.0573e-02\n",
      "-2.3096e-02  1.4549e-02  6.8595e-02  ...  -9.0104e-02  6.3390e-02  2.0846e-02\n",
      " 5.8850e-02 -1.1741e-02  6.4407e-02  ...  -3.4034e-02  6.3043e-03 -8.4447e-02\n",
      "                ...                   ⋱                   ...                \n",
      " 1.7804e-02  1.1515e-03  6.7596e-02  ...   2.9925e-02  5.8871e-02 -4.3367e-02\n",
      "-1.1714e-02 -6.0167e-02 -9.2021e-02  ...   4.2528e-02  1.3429e-02 -1.3334e-02\n",
      "-3.0186e-02  4.0729e-02  6.7101e-02  ...   3.1356e-02 -6.6728e-02  8.0316e-02\n",
      "[torch.FloatTensor of size 97x128]\n",
      "\n",
      "Parameter containing:\n",
      "1.00000e-02 *\n",
      "  1.0131\n",
      " -6.2255\n",
      " -0.5315\n",
      "  5.0941\n",
      "  4.9542\n",
      "  1.4517\n",
      " -4.0691\n",
      "  1.9501\n",
      " -4.0346\n",
      " -6.4810\n",
      " -1.6600\n",
      " -5.0591\n",
      "  2.6874\n",
      " -3.2647\n",
      "  8.2730\n",
      "  5.6609\n",
      " -1.9702\n",
      " -4.5633\n",
      "  3.6691\n",
      "  1.8853\n",
      "  6.0186\n",
      "  3.3953\n",
      "  2.6581\n",
      "  5.9928\n",
      "  4.7803\n",
      "  3.2091\n",
      "  5.3538\n",
      "  4.7624\n",
      "  1.7088\n",
      " -7.6769\n",
      " -1.2272\n",
      "  0.0135\n",
      " -6.2074\n",
      " -5.9947\n",
      "  4.9105\n",
      "  6.8590\n",
      " -5.3761\n",
      " -8.3919\n",
      "  8.6678\n",
      " -4.8201\n",
      "  1.7168\n",
      " -0.8721\n",
      " -8.6614\n",
      " -2.4571\n",
      " -1.7781\n",
      " -0.9456\n",
      " -4.7182\n",
      " -7.3712\n",
      " -8.2295\n",
      " -7.2644\n",
      "  3.6614\n",
      "  5.6889\n",
      " -3.3336\n",
      "  5.9991\n",
      "  5.5353\n",
      "  6.5956\n",
      "  5.3060\n",
      " -5.1851\n",
      " -7.7661\n",
      "  7.4711\n",
      "  3.3202\n",
      "  5.8466\n",
      " -5.5732\n",
      " -4.0049\n",
      "  5.6256\n",
      " -3.1954\n",
      " -4.6427\n",
      "  8.5290\n",
      " -0.9087\n",
      "  7.6821\n",
      "  5.8157\n",
      " -6.1097\n",
      "  2.5951\n",
      "  6.4357\n",
      " -7.2196\n",
      " -0.6673\n",
      " -4.5025\n",
      " -5.9518\n",
      " -8.7632\n",
      " -7.4228\n",
      "  9.2804\n",
      "  6.2751\n",
      " -8.0292\n",
      " -2.8560\n",
      " -7.3018\n",
      " -7.4749\n",
      " -0.9702\n",
      " -5.7783\n",
      " -7.9667\n",
      "  4.3543\n",
      " -3.6522\n",
      " -1.6039\n",
      "  2.8762\n",
      " -6.9009\n",
      " -8.0109\n",
      "  1.3435\n",
      " -0.9289\n",
      "[torch.FloatTensor of size 97]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for param in model.parameters():\n",
    "    print(param)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
