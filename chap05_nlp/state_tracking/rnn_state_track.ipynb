{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lib load done\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import fasttext\n",
    "from gensim.models.wrappers.fasttext import FastText\n",
    "import urllib, os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from konlpy.tag import Mecab\n",
    "print(\"lib load done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set configuration parms\n"
     ]
    }
   ],
   "source": [
    "vector_size = 300\n",
    "encode_length = 6\n",
    "label_size = 10\n",
    "vector_model = None\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_iters = 100\n",
    "display_step = 5\n",
    "batch_size = 5\n",
    "\n",
    "# Network Parameters\n",
    "n_input = vector_size\n",
    "n_hidden = 128 \n",
    "n_steps = encode_length\n",
    "n_classes = label_size \n",
    "state_parm = None\n",
    "print(\"set configuration parms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download pretrained Facebook fasttext vec for Korean\n",
    "- https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def download_fasttext_vec() :\n",
    "    url = \"https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.ko.vec\"\n",
    "    file_name = url.split('/')[-1]\n",
    "    u = urllib.request.urlopen(url)\n",
    "    if (os.path.exists('./model/' + file_name) == False) : \n",
    "        f = open('./model/' + file_name, 'wb')\n",
    "    else: \n",
    "        print(\"Vector file already exists\")\n",
    "        return False\n",
    "    meta = u.info()\n",
    "    file_size = 2000000000\n",
    "    print (\"Downloading: %s Bytes: %s\" % (file_name, file_size) )\n",
    "\n",
    "    file_size_dl = 0\n",
    "    block_sz = 8192\n",
    "    while True:\n",
    "        buffer = u.read(block_sz)\n",
    "        if not buffer:\n",
    "            break\n",
    "\n",
    "        file_size_dl += len(buffer)\n",
    "        f.write(buffer)\n",
    "        status = r\"%10d  [%3.2f%%]\" % (file_size_dl, file_size_dl * 100. / file_size)\n",
    "        status = status + chr(8)*(len(status)+1)\n",
    "        if (file_size_dl % 10000000 == 0) :\n",
    "            print (status)\n",
    "    f.close()\n",
    "    print(\"Download file down\")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load FastText Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_embed_model(vector_model) : \n",
    "    if(vector_model == None) : \n",
    "        vector_model = FastText.load_word2vec_format('./model/wiki.ko.vec')\n",
    "    print(\"Load Vector Model done\")\n",
    "    return vector_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_train_csv(model, batch_size):\n",
    "    df_csv_read = pd.read_csv('./data/train.csv',\n",
    "                              skipinitialspace=True,\n",
    "                              engine=\"python\",\n",
    "                              encoding='utf-8-sig')\n",
    "\n",
    "    mecab = Mecab('/usr/local/lib/mecab/dic/mecab-ko-dic')\n",
    "    \n",
    "    inputs = []\n",
    "    labels = []\n",
    "    \n",
    "    for encode_raw in df_csv_read['encode'] :   \n",
    "        encode_raw = mecab.pos(encode_raw)\n",
    "        encode_raw = list(filter(lambda x :  x[1] in ['MM', 'NNG', 'NNP', 'SF','XSN','NNBC', 'VV', 'NP','VCN+EF'], encode_raw ))\n",
    "        encode_raw = ''.join(list(map(lambda x : '@' if x[1] in ['SF'] else x[0], encode_raw))).split('@')\n",
    "        encode_raw = list(map(lambda x : mecab.morphs(x) , encode_raw))\n",
    "        \n",
    "        input = np.zeros((1,vector_size),dtype=float)\n",
    "        for sent in encode_raw : \n",
    "            if(len(sent) > 0 ) :\n",
    "                input = np.array(list(map(lambda x : model[x] if x in model.index2word else np.zeros(vector_size,dtype=float) , sent)))\n",
    "            if (encode_length - np.shape(input)[0] > 0 ) : \n",
    "                input = np.concatenate((input, np.zeros((encode_length - np.shape(input)[0],vector_size),dtype=float)))\n",
    "            else :\n",
    "                input = input[0:encode_length]\n",
    "        inputs.append(input)\n",
    "        \n",
    "    for decode_raw in df_csv_read['decode'] : \n",
    "        label = np.zeros(label_size, dtype=float)\n",
    "        np.put(label, decode_raw, 1)\n",
    "        labels.append(label)\n",
    "\n",
    "    print(\"Load data for batch size : {0}\".format(batch_size))\n",
    "    return np.array(inputs), np.array(labels)\n",
    "    #return np.take(np.array(inputs),np.random.choice(len(inputs),batch_size)) , np.take(np.array(labels),np.random.choice(len(labels),batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Build Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_graph() :\n",
    "    tf.reset_default_graph()\n",
    "    # tf Graph input\n",
    "    x_input = tf.placeholder(\"float\", [None, n_steps, n_input])\n",
    "    y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "    # Define weights\n",
    "    weights = {\n",
    "        'out': tf.Variable(tf.random_normal([n_hidden, n_classes]))\n",
    "    }\n",
    "    biases = {\n",
    "        'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "\n",
    "    # Unstack to get a list of 'n_steps' tensors of shape (batch_size, n_input)\n",
    "    x = tf.unstack(x_input,n_steps, 1)\n",
    "    \n",
    "    # Define a lstm cell with tensorflow\n",
    "    lstm_cell = rnn.BasicLSTMCell(n_hidden, forget_bias=1.0)\n",
    "    istate = lstm_cell.zero_state(batch_size, tf.float32)\n",
    "    outputs, states = rnn.static_rnn(lstm_cell, x, initial_state=istate, dtype=tf.float32)\n",
    "\n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    pred = tf.matmul(outputs[-1], weights['out']) + biases['out']\n",
    "\n",
    "    # Define loss and optimizer\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "    \n",
    "    pred_result = tf.argmax(pred,1)\n",
    "    # Evaluate model\n",
    "    correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "    # Initializing the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "    print(\"Build graph done\")\n",
    "    \n",
    "    return x_input,y,pred, cost,init, optimizer, accuracy, pred_result, lstm_cell, states, istate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    # Launch the graph\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        saver = tf.train.Saver(tf.all_variables())\n",
    "        step = 1\n",
    "\n",
    "        # Keep training until reach max iterations\n",
    "        while step * batch_size < training_iters:\n",
    "            batch_x, batch_y = load_train_csv(vector_model, batch_size)\n",
    "            # Reshape data to get 28 seq of 28 elements\n",
    "            batch_x = batch_x.reshape((batch_size, n_steps, n_input))\n",
    "            # Run optimization op (backprop)\n",
    "            sess.run(optimizer, feed_dict={x_input: batch_x, y: batch_y})\n",
    "\n",
    "            if step % display_step == 0:\n",
    "                # Calculate batch accuracy\n",
    "                acc = sess.run(accuracy, feed_dict={x_input: batch_x, y: batch_y})\n",
    "                # Calculate batch loss\n",
    "                loss = sess.run(cost, feed_dict={x_input: batch_x, y: batch_y})\n",
    "                print(\"Iter \" + str(step*batch_size) + \", Minibatch Loss= \" + \\\n",
    "                      \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                      \"{:.5f}\".format(acc))\n",
    "            step += 1\n",
    "        print(\"Optimization Finished!\")\n",
    "\n",
    "        # Calculate accuracy for 128 mnist test images\n",
    "        test_len = 128\n",
    "        test_data = batch_x\n",
    "        test_label = batch_y\n",
    "        print(\"Testing Accuracy:\", \\\n",
    "            sess.run(accuracy, feed_dict={x_input: test_data, y: test_label}))\n",
    "        saver.save(sess, './model/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Train Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: wiki.ko.vec Bytes: 2000000000\n",
      " 640000000  [32.00%]\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "1280000000  [64.00%]\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "1920000000  [96.00%]\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Download file down\n",
      "Load Vector Model done\n",
      "Build graph done\n",
      "WARNING:tensorflow:From <ipython-input-7-47848222e136>:5: all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Please use tf.global_variables instead.\n",
      "Load data for batch size : 5\n",
      "Load data for batch size : 5\n",
      "Load data for batch size : 5\n",
      "Load data for batch size : 5\n",
      "Load data for batch size : 5\n",
      "Iter 25, Minibatch Loss= 0.794948, Training Accuracy= 1.00000\n",
      "Load data for batch size : 5\n",
      "Load data for batch size : 5\n",
      "Load data for batch size : 5\n",
      "Load data for batch size : 5\n",
      "Load data for batch size : 5\n",
      "Iter 50, Minibatch Loss= 0.071836, Training Accuracy= 1.00000\n",
      "Load data for batch size : 5\n",
      "Load data for batch size : 5\n",
      "Load data for batch size : 5\n",
      "Load data for batch size : 5\n",
      "Load data for batch size : 5\n",
      "Iter 75, Minibatch Loss= 0.002244, Training Accuracy= 1.00000\n",
      "Load data for batch size : 5\n",
      "Load data for batch size : 5\n",
      "Load data for batch size : 5\n",
      "Load data for batch size : 5\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "#fast text word embedding 을 위한 pretrained 된 vector 파일을 다운로드 \n",
    "download_fasttext_vec()\n",
    "#Vector 모델을 fasttext 라이브러리에 로드\n",
    "vector_model = load_embed_model(vector_model)\n",
    "#그래프를 빌드한다. \n",
    "batch_size = 5\n",
    "x_input,y,pred, cost,init, optimizer, accuracy, lstm_cell, pred_result, state, istate = build_graph()\n",
    "#실제 트레인을 실행한다 \n",
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Data Prepare "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done prepare get vector function for prediction\n"
     ]
    }
   ],
   "source": [
    "def get_vector(query, model) :\n",
    "    mecab = Mecab('/usr/local/lib/mecab/dic/mecab-ko-dic')\n",
    "    inputs = []\n",
    "    encode_raw = mecab.pos(query)\n",
    "    encode_raw = list(filter(lambda x :  x[1] in ['MM', 'NNG', 'NNP', 'SF','XSN','NNBC', 'VV', 'NP','VCN+EF'], encode_raw ))\n",
    "    encode_raw = ''.join(list(map(lambda x : '@' if x[1] in ['SF'] else x[0], encode_raw))).split('@')\n",
    "    encode_raw = list(map(lambda x : mecab.morphs(x) , encode_raw))\n",
    "    print(\"====morph data : {0}\".format(encode_raw))\n",
    "    input = np.zeros((1,vector_size),dtype=float)\n",
    "    for sent in encode_raw : \n",
    "        if(len(sent) > 0 ) :\n",
    "            input = np.array(list(map(lambda x : model[x] if x in model.index2word else np.zeros(vector_size,dtype=float) , sent)))\n",
    "        if (encode_length - np.shape(input)[0] > 0 ) : \n",
    "            input = np.concatenate((input, np.zeros((encode_length - np.shape(input)[0],vector_size),dtype=float)))\n",
    "        else :\n",
    "            input = input[0:encode_length]\n",
    "    inputs.append(input)\n",
    "    return np.array(inputs)\n",
    "print(\"done prepare get vector function for prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done prepare predict session run function\n"
     ]
    }
   ],
   "source": [
    "def predict(vector, pred_result, state, state_parm, x_input, lstm_cell, batch_size ) : \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        saver = tf.train.Saver(tf.all_variables())\n",
    "        saver.restore(sess, './model/')\n",
    "        if(state_parm == None) : \n",
    "            print(\"=======initialze======\")\n",
    "            result,state_parm = sess.run([pred, state], feed_dict={x_input: vector})    \n",
    "        else :\n",
    "            print(\"=======load state======\")\n",
    "            result,state_parm = sess.run([pred, state], feed_dict={x_input: vector, istate:state_parm})    \n",
    "    return result, state_parm\n",
    "print(\"done prepare predict session run function\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Service Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector file already exists\n",
      "Load Vector Model done\n",
      "Build graph done\n"
     ]
    }
   ],
   "source": [
    "#fast text word embedding 을 위한 pretrained 된 vector 파일을 다운로드 \n",
    "download_fasttext_vec()\n",
    "#Vector 모델을 fasttext 라이브러리에 로드\n",
    "vector_model = load_embed_model(vector_model)\n",
    "#그래프를 빌드한다. \n",
    "batch_size = 1\n",
    "state_parm = None\n",
    "x_input,y,pred, cost,init, optimizer, accuracy, lstm_cell, pred_result, state, istate   = build_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Service Test\n",
    "# [시나리오]\n",
    "- (의도 : 1) 안녕\n",
    "- (의도 : 2) 넌 뭐할줄 아냐?\n",
    "- (의도 : 3) 사람을 찾고싶어\n",
    "- (의도 : 3) 김승우 찾아줘\n",
    "- (의도 : 4) 문자보내줘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====morph data : [['뭐', '아냐'], []]\n",
      "WARNING:tensorflow:From <ipython-input-10-538761a2e00d>:4: all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Please use tf.global_variables instead.\n",
      "INFO:tensorflow:Restoring parameters from ./model/\n",
      "=======initialze======\n",
      "[[ 0.95854872  0.00311426  0.47420675  0.81543148 -1.10605061 -0.70528424\n",
      "  -0.04886642 -0.47452116 -0.55120313 -0.25828254  0.56813639  0.77732325\n",
      "  -0.47374451  0.36261284 -0.0922368   0.3239626  -0.14634919 -0.48350111\n",
      "   0.51146436  0.23472466  0.27762547  0.0541451  -0.15378116  0.10287026\n",
      "  -0.46569183  0.6232999   0.2875618  -0.16551121  0.06420634  0.00370907\n",
      "  -0.06195594  0.07124117 -0.09534165 -0.05522212  0.3868804   0.05430844\n",
      "   0.66743159 -0.1548849   0.81005263  0.13730553 -0.25688592  0.1057874\n",
      "  -0.16586143 -0.15877753 -0.44768041  0.61756134  0.25059211  0.13127139\n",
      "  -0.64165598  0.13816799 -0.3383362  -0.39154503  0.34258851 -0.87412286\n",
      "   0.79840589 -0.6162501  -0.92601758  1.41798806 -0.127694    0.55242479\n",
      "   0.2979663  -0.43040991 -0.12030718  0.27243057 -0.71002322 -0.72835827\n",
      "   0.29100931  0.2447163   0.90264463  0.13459183  0.06383131  1.16533279\n",
      "  -0.66573429 -0.07217535  0.11191772 -0.07321803  0.02254346  0.6532144\n",
      "   0.62833387 -0.1419501   0.54625243  0.11693568 -0.70228356 -0.06053567\n",
      "   0.41316128 -0.16609648  0.80302596  0.25653803 -0.08776379 -0.03968558\n",
      "  -0.8691836  -0.66771722 -0.68037039  0.90321881 -1.23764789 -0.67457145\n",
      "  -0.33436477  0.74998897  0.0581357  -0.62551969  0.70085478  0.47581244\n",
      "   0.24623212 -1.11504006 -0.06356002  0.35470766  0.57891071  0.48939675\n",
      "   0.14716755  0.21852416  0.07524428 -0.69394475 -0.42572373 -0.34158039\n",
      "  -0.28389865  0.72788817  0.07216655  0.10148734  0.67020702  0.32158908\n",
      "  -0.37114865 -0.89415938 -0.41125625  0.82364976 -0.43345365  0.42703152\n",
      "   0.51461077 -0.45439544]]\n",
      "=============================================\n",
      "Intent Result : 2\n",
      "=============================================\n"
     ]
    }
   ],
   "source": [
    "#실제 트레인을 실행한다 \n",
    "result, state_parm = predict(get_vector(\"넌뭐할줄아냐?\", vector_model), pred_result, state, state_parm, x_input, lstm_cell, batch_size)\n",
    "print(state_parm[0])\n",
    "print(\"=============================================\")\n",
    "print(\"Intent Result : {0}\".format(np.argmax(result)))\n",
    "print(\"=============================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reset State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "state_parm = None"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
