{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeding FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.wrappers import FastText\n",
    "\n",
    "model = fasttext.FastText.train('../../data/novel_1.txt', corpus_file='text8')\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1.0\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow.contrib.seq2seq' has no attribute 'prepare_attention'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-6e2ac60bab0f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mhelp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq2seq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_attention\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow.contrib.seq2seq' has no attribute 'prepare_attention'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from lib import data_utils, model_utils\n",
    "from configs import model_config\n",
    "print(tf.__version__)\n",
    "\n",
    "help(tf.contrib.seq2seq.prepare_attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "\tconfig = model_config.Config()\n",
    "\twith tf.Session() as sess:\n",
    "\t\tforward_only = False\n",
    "\n",
    "\t\tvocab_path = os.path.join(config.data_dir, 'vocab%d.in' % config.input_vocab_size)\n",
    "\n",
    "\t\ttrain_data_path = os.path.join(config.data_dir, 'chat_ids%d.in' % config.input_vocab_size)\n",
    "\n",
    "\t\t# Load data\n",
    "\t\tvocab, vocab_rev = data_utils.load_vocabulary(vocab_path)\n",
    "\t\ttrain_set = data_utils.read_data_chat(train_data_path, config)\n",
    "\t\t# print(train_set[0])\n",
    "\n",
    "\t\tif forward_only:\n",
    "\t\t\tconfig.batch_size = 1\n",
    "\t\t\tmodel = model_utils.create_model(sess, config, forward_only)\n",
    "\t\telse:\n",
    "\t\t\tmodel = model_utils.create_model(sess, config, forward_only)\n",
    "\n",
    "\t\t# This is the training loop.\n",
    "\t\tsteps_per_checkpoint = 100\n",
    "\t\tstep_time, loss = 0.0, 0.0\n",
    "\t\tcurrent_step = 0\n",
    "\t\tperplexity = 10000.0\n",
    "\t\tprevious_losses = []\n",
    "\n",
    "\t\twhile current_step < config.max_epoch and not forward_only:\n",
    "\t\t\tstart_time = time.time()\n",
    "\t\t\tbucket_id = 0\n",
    "\t\t\tencoder_inputs, encoder_inputs_length, decoder_inputs, decoder_inputs_length, target_weights = (\n",
    "\t\t\t\tdata_utils.get_batch(train_set[bucket_id], config))\n",
    "\n",
    "\t\t\t_, step_loss, _, _, enc_embedding, dec_embedding = model.step(sess, encoder_inputs, encoder_inputs_length,\n",
    "\t\t\t                                                              decoder_inputs, decoder_inputs_length, target_weights,forward_only)\n",
    "\n",
    "\t\t\tstep_time += (time.time() - start_time) / 100\n",
    "\t\t\tloss += step_loss / 100\n",
    "\t\t\tcurrent_step += 1\n",
    "\n",
    "\t\t\tif current_step % 100 == 0:\n",
    "\t\t\t\t# Print statistics for the previous epoch.\n",
    "\t\t\t\t# loss *= config.max_state_length \t\t# Temporary purpose only\n",
    "\t\t\t\tperplexity = math.exp(loss) if loss < 300 else float('inf')\n",
    "\t\t\t\tprint(\"global step %d learning rate %.4f step-time %.2f perplexity %.2f loss %.2f\" %\n",
    "\t\t\t\t\t\t\t(model.global_step.eval(), model.learning_rate.eval(), step_time, perplexity, loss))\n",
    "\n",
    "\t\t\t\tif len(previous_losses) > 2 and loss > max(previous_losses[-2:]):\n",
    "\t\t\t\t\t# if len(previous_losses) > 0 and loss > previous_losses[-1:]:\n",
    "\t\t\t\t\tsess.run(model.learning_rate_decay_op)\n",
    "\n",
    "\t\t\t\tprevious_losses.append(loss)\n",
    "\n",
    "\t\t\t\t# Save checkpoint and zero timer and loss.\n",
    "\t\t\t\tcheckpoint_path = os.path.join(config.model_dir, \"model.ckpt\")\n",
    "\t\t\t\tmodel.saver.save(sess, checkpoint_path, global_step=model.global_step)\n",
    "\t\t\t\tstep_time, loss = 0.0, 0.0\n",
    "\n",
    "\t\t\t\tsys.stdout.flush()\n",
    "\n",
    "\t\tif forward_only:\n",
    "\t\t\tvalid_data_path = os.path.join(config.data_dir, 'chat_valid_ids%d.in'% config.input_vocab_size)\n",
    "\t\t\tdev_set = data_utils.read_data_chat(valid_data_path, config)\n",
    "\t\t\tprint (dev_set)\n",
    "\t\t\tbucket_id = 0\n",
    "\t\t\t# for i in range(len(dev_set[0])):\n",
    "\t\t\tfor i in range(1):\n",
    "\t\t\t\tdev_inputs, dev_inputs_length, dev_outputs, dev_outputs_length, target_weights = (\n",
    "\t\t\t\t\tdata_utils.get_test_line(train_set[bucket_id], i))\n",
    "\n",
    "\t\t\t\t_, _, logits, predicted, enc_embedding, dec_embedding = model.step(sess, dev_inputs, dev_inputs_length,\n",
    "\t\t\t\t                                                        dev_outputs, dev_outputs_length, target_weights,forward_only)\n",
    "\n",
    "\t\t\t\tprint(\"Prediction Results in Iteration %d : \" % i)\n",
    "\t\t\t\tprint(dev_inputs.transpose())\n",
    "\t\t\t\tprint(dev_outputs.transpose())\n",
    "\t\t\t\tprint(predicted.transpose())\n",
    "\t\t\t\tprint(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 24 119 27 974 9 3 3 17 4016 7885 47 420 88 3853 3 953 3 61 42 10 3 4 196 4\n",
      " 70 6 8 152 24 5 79 324 45 3 6 56 16 5 13 123 45 7 4\n",
      "\n",
      "  reading data line 1000\n",
      "  reading data line 2000\n",
      "  reading data line 3000\n",
      "  reading data line 4000\n",
      "  reading data line 5000\n",
      "  reading data line 6000\n",
      "  reading data line 7000\n",
      "  reading data line 8000\n",
      "  reading data line 9000\n",
      "  reading data line 10000\n",
      "  reading data line 11000\n",
      "  reading data line 12000\n",
      "  reading data line 13000\n",
      "  reading data line 14000\n",
      "  reading data line 15000\n",
      "  reading data line 16000\n",
      "  reading data line 17000\n",
      "  reading data line 18000\n",
      "  reading data line 19000\n",
      "  reading data line 20000\n",
      "  reading data line 21000\n",
      "  reading data line 22000\n",
      "  reading data line 23000\n",
      "  reading data line 24000\n",
      "  reading data line 25000\n",
      "  reading data line 26000\n",
      "  reading data line 27000\n",
      "  reading data line 28000\n",
      "  reading data line 29000\n",
      "  reading data line 30000\n",
      "  reading data line 31000\n",
      "  reading data line 32000\n",
      "  reading data line 33000\n",
      "  reading data line 34000\n",
      "  reading data line 35000\n",
      "  reading data line 36000\n",
      "  reading data line 37000\n",
      "  reading data line 38000\n",
      "  reading data line 39000\n",
      "  reading data line 40000\n",
      "  reading data line 41000\n",
      "  reading data line 42000\n",
      "  reading data line 43000\n",
      "  reading data line 44000\n",
      "  reading data line 45000\n",
      "  reading data line 46000\n",
      "  reading data line 47000\n",
      "  reading data line 48000\n",
      "  reading data line 49000\n",
      "  reading data line 50000\n",
      "  reading data line 51000\n",
      "  reading data line 52000\n",
      "  reading data line 53000\n",
      "  reading data line 54000\n",
      "  reading data line 55000\n",
      "  reading data line 56000\n",
      "  reading data line 57000\n",
      "  reading data line 58000\n",
      "  reading data line 59000\n",
      "  reading data line 60000\n",
      "  reading data line 61000\n",
      "  reading data line 62000\n",
      "  reading data line 63000\n",
      "  reading data line 64000\n",
      "  reading data line 65000\n",
      "  reading data line 66000\n",
      "  reading data line 67000\n",
      "  reading data line 68000\n",
      "  reading data line 69000\n",
      "  reading data line 70000\n",
      "  reading data line 71000\n",
      "  reading data line 72000\n",
      "  reading data line 73000\n",
      "  reading data line 74000\n",
      "  reading data line 75000\n",
      "  reading data line 76000\n",
      "  reading data line 77000\n",
      "  reading data line 78000\n",
      "  reading data line 79000\n",
      "  reading data line 80000\n",
      "  reading data line 81000\n",
      "  reading data line 82000\n",
      "  reading data line 83000\n",
      "  reading data line 84000\n",
      "  reading data line 85000\n",
      "  reading data line 86000\n",
      "  reading data line 87000\n",
      "  reading data line 88000\n",
      "  reading data line 89000\n",
      "  reading data line 90000\n",
      "  reading data line 91000\n",
      "  reading data line 92000\n",
      "  reading data line 93000\n",
      "  reading data line 94000\n",
      "  reading data line 95000\n",
      "  reading data line 96000\n",
      "  reading data line 97000\n",
      "  reading data line 98000\n",
      "  reading data line 99000\n",
      "  reading data line 100000\n",
      "  reading data line 101000\n",
      "  reading data line 102000\n",
      "  reading data line 103000\n",
      "  reading data line 104000\n",
      "  reading data line 105000\n",
      "  reading data line 106000\n",
      "  reading data line 107000\n",
      "  reading data line 108000\n",
      "  reading data line 109000\n",
      "  reading data line 110000\n",
      "  reading data line 111000\n",
      "  reading data line 112000\n",
      "  reading data line 113000\n",
      "  reading data line 114000\n",
      "  reading data line 115000\n",
      "  reading data line 116000\n",
      "  reading data line 117000\n",
      "  reading data line 118000\n",
      "  reading data line 119000\n",
      "  reading data line 120000\n",
      "  reading data line 121000\n",
      "  reading data line 122000\n",
      "  reading data line 123000\n",
      "  reading data line 124000\n",
      "  reading data line 125000\n",
      "  reading data line 126000\n",
      "  reading data line 127000\n",
      "  reading data line 128000\n",
      "  reading data line 129000\n",
      "  reading data line 130000\n",
      "  reading data line 131000\n",
      "  reading data line 132000\n",
      "  reading data line 133000\n",
      "  reading data line 134000\n",
      "  reading data line 135000\n",
      "  reading data line 136000\n",
      "  reading data line 137000\n",
      "  reading data line 138000\n",
      "  reading data line 139000\n",
      "  reading data line 140000\n",
      "  reading data line 141000\n",
      "  reading data line 142000\n",
      "  reading data line 143000\n",
      "  reading data line 144000\n",
      "  reading data line 145000\n",
      "  reading data line 146000\n",
      "  reading data line 147000\n",
      "  reading data line 148000\n",
      "  reading data line 149000\n",
      "  reading data line 150000\n",
      "  reading data line 151000\n",
      "  reading data line 152000\n",
      "  reading data line 153000\n",
      "  reading data line 154000\n",
      "  reading data line 155000\n",
      "  reading data line 156000\n",
      "  reading data line 157000\n",
      "  reading data line 158000\n",
      "  reading data line 159000\n",
      "  reading data line 160000\n",
      "  reading data line 161000\n",
      "  reading data line 162000\n",
      "  reading data line 163000\n",
      "  reading data line 164000\n",
      "  reading data line 165000\n",
      "  reading data line 166000\n",
      "  reading data line 167000\n",
      "  reading data line 168000\n",
      "  reading data line 169000\n",
      "  reading data line 170000\n",
      "  reading data line 171000\n",
      "  reading data line 172000\n",
      "  reading data line 173000\n",
      "  reading data line 174000\n",
      "  reading data line 175000\n",
      "  reading data line 176000\n",
      "  reading data line 177000\n",
      "  reading data line 178000\n",
      "  reading data line 179000\n",
      "  reading data line 180000\n",
      "  reading data line 181000\n",
      "  reading data line 182000\n",
      "  reading data line 183000\n",
      "  reading data line 184000\n",
      "  reading data line 185000\n",
      "  reading data line 186000\n",
      "  reading data line 187000\n",
      "  reading data line 188000\n",
      "  reading data line 189000\n",
      "  reading data line 190000\n",
      "  reading data line 191000\n",
      "  reading data line 192000\n",
      "  reading data line 193000\n",
      "  reading data line 194000\n",
      "  reading data line 195000\n",
      "  reading data line 196000\n",
      "  reading data line 197000\n",
      "  reading data line 198000\n",
      "  reading data line 199000\n",
      "  reading data line 200000\n",
      "  reading data line 201000\n",
      "  reading data line 202000\n",
      "  reading data line 203000\n",
      "  reading data line 204000\n",
      "  reading data line 205000\n",
      "  reading data line 206000\n",
      "  reading data line 207000\n",
      "  reading data line 208000\n",
      "  reading data line 209000\n",
      "  reading data line 210000\n",
      "  reading data line 211000\n",
      "  reading data line 212000\n",
      "  reading data line 213000\n",
      "  reading data line 214000\n",
      "  reading data line 215000\n",
      "  reading data line 216000\n",
      "  reading data line 217000\n",
      "  reading data line 218000\n",
      "  reading data line 219000\n",
      "  reading data line 220000\n",
      "  reading data line 221000\n",
      "Created model with fresh parameters.\n",
      "global step 100 learning rate 0.0500 step-time 0.07 perplexity 9445574041731.57 loss 29.88\n",
      "global step 200 learning rate 0.0500 step-time 0.07 perplexity 563376648676463147041492566016.00 loss 68.50\n",
      "global step 300 learning rate 0.0500 step-time 0.08 perplexity 10403239445366528855853280902352781557357871405989888.00 loss 119.77\n",
      "global step 400 learning rate 0.0500 step-time 0.08 perplexity 21702440294061805947094593878144836167128760158708605011375347990528.00 loss 155.05\n",
      "global step 500 learning rate 0.0495 step-time 0.08 perplexity 18318520804316096182289951064775656910342223506344617452124124940045417817868261100879872.00 loss 203.23\n",
      "global step 600 learning rate 0.0490 step-time 0.08 perplexity 37215193012369749481417399808141119697718309767020256340084320947131121540999224995832132318101640477843944112128.00 loss 259.20\n",
      "global step 700 learning rate 0.0485 step-time 0.08 perplexity 267864547722252915795746766573135183333548205796683966171654544324233349288172332245518987644336483598336.00 loss 240.45\n",
      "global step 800 learning rate 0.0485 step-time 0.07 perplexity inf loss 322.24\n",
      "global step 900 learning rate 0.0480 step-time 0.07 perplexity 564134062967392341882922075280398050521739556486382578590046792397309207782204703994311566596315120811960215174649025545633792.00 loss 289.55\n",
      "global step 1000 learning rate 0.0480 step-time 0.07 perplexity inf loss 342.01\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from lib import data_utils, model_utils\n",
    "from configs import model_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "        \n",
    "    \n",
    "def main():\n",
    "\tconfig = model_config.Config()\n",
    "\twith tf.Session() as sess:\n",
    "\t\tforward_only = True\n",
    "\n",
    "\t\tvocab_path = os.path.join(config.data_dir, 'vocab%d.in' % config.input_vocab_size)\n",
    "\n",
    "\t\t# Load data\n",
    "\t\tvocab, vocab_rev = data_utils.load_vocabulary(vocab_path)\n",
    "\n",
    "\t\tconfig.batch_size = 1\n",
    "\t\tmodel = model_utils.create_model(sess, config, forward_only)\n",
    "\n",
    "\n",
    "\t\tvalid_data_path = os.path.join(config.data_dir, 'chat_valid_ids%d.in'% config.input_vocab_size)\n",
    "\t\tdev_set = data_utils.read_test_data_chat(valid_data_path, config)[:2]\n",
    "\t\tbucket_id = 0\n",
    "\n",
    "\t\tfor i in range(len(dev_set[0])):\n",
    "\t\t\tdev_inputs, dev_inputs_length, dev_outputs, dev_outputs_length, target_weights = (\n",
    "\t\t\tdata_utils.get_test_line(dev_set[bucket_id], i))\n",
    "\n",
    "\t\t\t_, _, logits, predicted, enc_embedding, dec_embedding = model.step(sess, dev_inputs, dev_inputs_length,\n",
    "\t\t\t\t                                                        dev_outputs, dev_outputs_length, target_weights,forward_only)\n",
    "\n",
    "\t\t\tprint(\"\\nPrediction Results in Iteration %d : \" % i)\n",
    "\n",
    "\t\t\ts = \"\"\n",
    "\t\t\tfor input in dev_inputs:\n",
    "\t\t\t\ts += (vocab_rev[input[0]]) + \" \"\n",
    "\t\t\tprint (\"input :\" + s)\n",
    "\n",
    "\t\t\ts = \"\"\n",
    "\t\t\tfor output in dev_outputs:\n",
    "\t\t\t\ts += (vocab_rev[output[0]]) + \" \"\n",
    "\t\t\tprint (\"output : \" +s)\n",
    "\n",
    "\t\t\ts = \"\"\n",
    "\t\t\tfor i in predicted:\n",
    "\t\t\t\ts += (vocab_rev[i[0]]) + \" \"\n",
    "\t\t\tprint (\"prediction : \" + s)\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
