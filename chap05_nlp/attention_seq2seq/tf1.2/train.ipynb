{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from lib import data_utils, model_utils\n",
    "from configs import model_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "\tconfig = model_config.Config()\n",
    "\twith tf.Session() as sess:\n",
    "\t\tforward_only = False\n",
    "\n",
    "\t\tvocab_path = os.path.join(config.data_dir, 'vocab%d.in' % config.input_vocab_size)\n",
    "\n",
    "\t\ttrain_data_path = os.path.join(config.data_dir, 'chat_ids%d.in' % config.input_vocab_size)\n",
    "\n",
    "\t\t# Load data\n",
    "\t\tvocab, vocab_rev = data_utils.load_vocabulary(vocab_path)\n",
    "\t\ttrain_set = data_utils.read_data_chat(train_data_path, config)\n",
    "\t\t# print(train_set[0])\n",
    "\n",
    "\t\tif forward_only:\n",
    "\t\t\tconfig.batch_size = 1\n",
    "\t\t\tmodel = model_utils.create_model(sess, config, forward_only)\n",
    "\t\telse:\n",
    "\t\t\tmodel = model_utils.create_model(sess, config, forward_only)\n",
    "\n",
    "\t\t# This is the training loop.\n",
    "\t\tsteps_per_checkpoint = 100\n",
    "\t\tstep_time, loss = 0.0, 0.0\n",
    "\t\tcurrent_step = 0\n",
    "\t\tperplexity = 10000.0\n",
    "\t\tprevious_losses = []\n",
    "\n",
    "\t\twhile current_step < config.max_epoch and not forward_only:\n",
    "\t\t\tstart_time = time.time()\n",
    "\t\t\tbucket_id = 0\n",
    "\t\t\tencoder_inputs, encoder_inputs_length, decoder_inputs, decoder_inputs_length, target_weights = (\n",
    "\t\t\t\tdata_utils.get_batch(train_set[bucket_id], config))\n",
    "\n",
    "\t\t\t_, step_loss, _, _, enc_embedding, dec_embedding = model.step(sess, encoder_inputs, encoder_inputs_length,\n",
    "\t\t\t                                                              decoder_inputs, decoder_inputs_length, target_weights,forward_only)\n",
    "\n",
    "\t\t\tstep_time += (time.time() - start_time) / 100\n",
    "\t\t\tloss += step_loss / 100\n",
    "\t\t\tcurrent_step += 1\n",
    "\n",
    "\t\t\tif current_step % 100 == 0:\n",
    "\t\t\t\t# Print statistics for the previous epoch.\n",
    "\t\t\t\t# loss *= config.max_state_length \t\t# Temporary purpose only\n",
    "\t\t\t\tperplexity = math.exp(loss) if loss < 300 else float('inf')\n",
    "\t\t\t\tprint(\"global step %d learning rate %.4f step-time %.2f perplexity %.2f loss %.2f\" %\n",
    "\t\t\t\t\t\t\t(model.global_step.eval(), model.learning_rate.eval(), step_time, perplexity, loss))\n",
    "\n",
    "\t\t\t\tif len(previous_losses) > 2 and loss > max(previous_losses[-2:]):\n",
    "\t\t\t\t\t# if len(previous_losses) > 0 and loss > previous_losses[-1:]:\n",
    "\t\t\t\t\tsess.run(model.learning_rate_decay_op)\n",
    "\n",
    "\t\t\t\tprevious_losses.append(loss)\n",
    "\n",
    "\t\t\t\t# Save checkpoint and zero timer and loss.\n",
    "\t\t\t\tcheckpoint_path = os.path.join(config.model_dir, \"model.ckpt\")\n",
    "\t\t\t\tmodel.saver.save(sess, checkpoint_path, global_step=model.global_step)\n",
    "\t\t\t\tstep_time, loss = 0.0, 0.0\n",
    "\n",
    "\t\t\t\tsys.stdout.flush()\n",
    "\n",
    "\t\tif forward_only:\n",
    "\t\t\tvalid_data_path = os.path.join(config.data_dir, 'chat_valid_ids%d.in'% config.input_vocab_size)\n",
    "\t\t\tdev_set = data_utils.read_data_chat(valid_data_path, config)\n",
    "\t\t\tprint (dev_set)\n",
    "\t\t\tbucket_id = 0\n",
    "\t\t\t# for i in range(len(dev_set[0])):\n",
    "\t\t\tfor i in range(1):\n",
    "\t\t\t\tdev_inputs, dev_inputs_length, dev_outputs, dev_outputs_length, target_weights = (\n",
    "\t\t\t\t\tdata_utils.get_test_line(train_set[bucket_id], i))\n",
    "\n",
    "\t\t\t\t_, _, logits, predicted, enc_embedding, dec_embedding = model.step(sess, dev_inputs, dev_inputs_length,\n",
    "\t\t\t\t                                                        dev_outputs, dev_outputs_length, target_weights,forward_only)\n",
    "\n",
    "\t\t\t\tprint(\"Prediction Results in Iteration %d : \" % i)\n",
    "\t\t\t\tprint(dev_inputs.transpose())\n",
    "\t\t\t\tprint(dev_outputs.transpose())\n",
    "\t\t\t\tprint(predicted.transpose())\n",
    "\t\t\t\tprint(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 24 119 27 974 9 3 3 17 4016 7885 47 420 88 3853 3 953 3 61 42 10 3 4 196 4\n",
      " 70 6 8 152 24 5 79 324 45 3 6 56 16 5 13 123 45 7 4\n",
      "\n",
      "  reading data line 1000\n",
      "  reading data line 2000\n",
      "  reading data line 3000\n",
      "  reading data line 4000\n",
      "  reading data line 5000\n",
      "  reading data line 6000\n",
      "  reading data line 7000\n",
      "  reading data line 8000\n",
      "  reading data line 9000\n",
      "  reading data line 10000\n",
      "  reading data line 11000\n",
      "  reading data line 12000\n",
      "  reading data line 13000\n",
      "  reading data line 14000\n",
      "  reading data line 15000\n",
      "  reading data line 16000\n",
      "  reading data line 17000\n",
      "  reading data line 18000\n",
      "  reading data line 19000\n",
      "  reading data line 20000\n",
      "  reading data line 21000\n",
      "  reading data line 22000\n",
      "  reading data line 23000\n",
      "  reading data line 24000\n",
      "  reading data line 25000\n",
      "  reading data line 26000\n",
      "  reading data line 27000\n",
      "  reading data line 28000\n",
      "  reading data line 29000\n",
      "  reading data line 30000\n",
      "  reading data line 31000\n",
      "  reading data line 32000\n",
      "  reading data line 33000\n",
      "  reading data line 34000\n",
      "  reading data line 35000\n",
      "  reading data line 36000\n",
      "  reading data line 37000\n",
      "  reading data line 38000\n",
      "  reading data line 39000\n",
      "  reading data line 40000\n",
      "  reading data line 41000\n",
      "  reading data line 42000\n",
      "  reading data line 43000\n",
      "  reading data line 44000\n",
      "  reading data line 45000\n",
      "  reading data line 46000\n",
      "  reading data line 47000\n",
      "  reading data line 48000\n",
      "  reading data line 49000\n",
      "  reading data line 50000\n",
      "  reading data line 51000\n",
      "  reading data line 52000\n",
      "  reading data line 53000\n",
      "  reading data line 54000\n",
      "  reading data line 55000\n",
      "  reading data line 56000\n",
      "  reading data line 57000\n",
      "  reading data line 58000\n",
      "  reading data line 59000\n",
      "  reading data line 60000\n",
      "  reading data line 61000\n",
      "  reading data line 62000\n",
      "  reading data line 63000\n",
      "  reading data line 64000\n",
      "  reading data line 65000\n",
      "  reading data line 66000\n",
      "  reading data line 67000\n",
      "  reading data line 68000\n",
      "  reading data line 69000\n",
      "  reading data line 70000\n",
      "  reading data line 71000\n",
      "  reading data line 72000\n",
      "  reading data line 73000\n",
      "  reading data line 74000\n",
      "  reading data line 75000\n",
      "  reading data line 76000\n",
      "  reading data line 77000\n",
      "  reading data line 78000\n",
      "  reading data line 79000\n",
      "  reading data line 80000\n",
      "  reading data line 81000\n",
      "  reading data line 82000\n",
      "  reading data line 83000\n",
      "  reading data line 84000\n",
      "  reading data line 85000\n",
      "  reading data line 86000\n",
      "  reading data line 87000\n",
      "  reading data line 88000\n",
      "  reading data line 89000\n",
      "  reading data line 90000\n",
      "  reading data line 91000\n",
      "  reading data line 92000\n",
      "  reading data line 93000\n",
      "  reading data line 94000\n",
      "  reading data line 95000\n",
      "  reading data line 96000\n",
      "  reading data line 97000\n",
      "  reading data line 98000\n",
      "  reading data line 99000\n",
      "  reading data line 100000\n",
      "  reading data line 101000\n",
      "  reading data line 102000\n",
      "  reading data line 103000\n",
      "  reading data line 104000\n",
      "  reading data line 105000\n",
      "  reading data line 106000\n",
      "  reading data line 107000\n",
      "  reading data line 108000\n",
      "  reading data line 109000\n",
      "  reading data line 110000\n",
      "  reading data line 111000\n",
      "  reading data line 112000\n",
      "  reading data line 113000\n",
      "  reading data line 114000\n",
      "  reading data line 115000\n",
      "  reading data line 116000\n",
      "  reading data line 117000\n",
      "  reading data line 118000\n",
      "  reading data line 119000\n",
      "  reading data line 120000\n",
      "  reading data line 121000\n",
      "  reading data line 122000\n",
      "  reading data line 123000\n",
      "  reading data line 124000\n",
      "  reading data line 125000\n",
      "  reading data line 126000\n",
      "  reading data line 127000\n",
      "  reading data line 128000\n",
      "  reading data line 129000\n",
      "  reading data line 130000\n",
      "  reading data line 131000\n",
      "  reading data line 132000\n",
      "  reading data line 133000\n",
      "  reading data line 134000\n",
      "  reading data line 135000\n",
      "  reading data line 136000\n",
      "  reading data line 137000\n",
      "  reading data line 138000\n",
      "  reading data line 139000\n",
      "  reading data line 140000\n",
      "  reading data line 141000\n",
      "  reading data line 142000\n",
      "  reading data line 143000\n",
      "  reading data line 144000\n",
      "  reading data line 145000\n",
      "  reading data line 146000\n",
      "  reading data line 147000\n",
      "  reading data line 148000\n",
      "  reading data line 149000\n",
      "  reading data line 150000\n",
      "  reading data line 151000\n",
      "  reading data line 152000\n",
      "  reading data line 153000\n",
      "  reading data line 154000\n",
      "  reading data line 155000\n",
      "  reading data line 156000\n",
      "  reading data line 157000\n",
      "  reading data line 158000\n",
      "  reading data line 159000\n",
      "  reading data line 160000\n",
      "  reading data line 161000\n",
      "  reading data line 162000\n",
      "  reading data line 163000\n",
      "  reading data line 164000\n",
      "  reading data line 165000\n",
      "  reading data line 166000\n",
      "  reading data line 167000\n",
      "  reading data line 168000\n",
      "  reading data line 169000\n",
      "  reading data line 170000\n",
      "  reading data line 171000\n",
      "  reading data line 172000\n",
      "  reading data line 173000\n",
      "  reading data line 174000\n",
      "  reading data line 175000\n",
      "  reading data line 176000\n",
      "  reading data line 177000\n",
      "  reading data line 178000\n",
      "  reading data line 179000\n",
      "  reading data line 180000\n",
      "  reading data line 181000\n",
      "  reading data line 182000\n",
      "  reading data line 183000\n",
      "  reading data line 184000\n",
      "  reading data line 185000\n",
      "  reading data line 186000\n",
      "  reading data line 187000\n",
      "  reading data line 188000\n",
      "  reading data line 189000\n",
      "  reading data line 190000\n",
      "  reading data line 191000\n",
      "  reading data line 192000\n",
      "  reading data line 193000\n",
      "  reading data line 194000\n",
      "  reading data line 195000\n",
      "  reading data line 196000\n",
      "  reading data line 197000\n",
      "  reading data line 198000\n",
      "  reading data line 199000\n",
      "  reading data line 200000\n",
      "  reading data line 201000\n",
      "  reading data line 202000\n",
      "  reading data line 203000\n",
      "  reading data line 204000\n",
      "  reading data line 205000\n",
      "  reading data line 206000\n",
      "  reading data line 207000\n",
      "  reading data line 208000\n",
      "  reading data line 209000\n",
      "  reading data line 210000\n",
      "  reading data line 211000\n",
      "  reading data line 212000\n",
      "  reading data line 213000\n",
      "  reading data line 214000\n",
      "  reading data line 215000\n",
      "  reading data line 216000\n",
      "  reading data line 217000\n",
      "  reading data line 218000\n",
      "  reading data line 219000\n",
      "  reading data line 220000\n",
      "  reading data line 221000\n",
      "Reading model parameters from ./nn_models/model.ckpt-9500\n",
      "INFO:tensorflow:Restoring parameters from ./nn_models/model.ckpt-9500\n",
      "global step 9600 learning rate 0.0366 step-time 0.03 perplexity inf loss 312.01\n",
      "global step 9700 learning rate 0.0366 step-time 0.03 perplexity inf loss 301.90\n",
      "global step 9800 learning rate 0.0366 step-time 0.03 perplexity inf loss 305.90\n",
      "global step 9900 learning rate 0.0366 step-time 0.03 perplexity 10495812784899198347009756146740624842301655582410543162424522981411754849268449000895003881785021766636781950674084865972895744.00 loss 292.48\n",
      "global step 10000 learning rate 0.0366 step-time 0.03 perplexity inf loss 323.21\n",
      "global step 10100 learning rate 0.0362 step-time 0.03 perplexity inf loss 311.24\n",
      "global step 10200 learning rate 0.0362 step-time 0.03 perplexity 6397019867090225609306911880234786347336156238928458699724019867460498366691767022281347542356233504393570672485335040.00 loss 271.26\n",
      "global step 10300 learning rate 0.0362 step-time 0.03 perplexity inf loss 306.20\n",
      "global step 10400 learning rate 0.0362 step-time 0.03 perplexity inf loss 301.15\n",
      "global step 10500 learning rate 0.0362 step-time 0.03 perplexity 11058176526922610065366788599717263523839264378317830318013927441474876829332822854460306762030583270220308507191672832.00 loss 271.81\n",
      "global step 10600 learning rate 0.0362 step-time 0.03 perplexity 1186095605928277043789481388265580529913577960683229116603821747240310776994264028297790448785001575361247523001829122637186465792.00 loss 297.20\n",
      "global step 10700 learning rate 0.0362 step-time 0.03 perplexity 41085066390433145450090862715198621868158384093462060691373428433197266753381875415259107306087529564358933473423368433319280640.00 loss 293.84\n",
      "global step 10800 learning rate 0.0362 step-time 0.03 perplexity inf loss 309.40\n",
      "global step 10900 learning rate 0.0359 step-time 0.03 perplexity 27549161057791588137538888014591586740458637042544732804195704797333571058688272997820000089329029701129546269556276723712.00 loss 279.63\n",
      "global step 11000 learning rate 0.0359 step-time 0.03 perplexity 46652518905501492467177137955009447821302552996474516804458475056606282716145390213173009871397041457019636085567107692138332160.00 loss 293.97\n",
      "global step 11100 learning rate 0.0359 step-time 0.03 perplexity inf loss 300.03\n",
      "global step 11200 learning rate 0.0355 step-time 0.03 perplexity 572065756919916232052863347171963677529235039187052336805389306430189714057962477561031142546409451075555475581686523327676416.00 loss 289.57\n",
      "global step 11300 learning rate 0.0355 step-time 0.03 perplexity 5992850146263242623019874199384381639285219975710667061140049822573138265285906628054152253338141532881930857578703066413137920.00 loss 291.92\n",
      "global step 11400 learning rate 0.0355 step-time 0.03 perplexity inf loss 307.52\n",
      "global step 11500 learning rate 0.0352 step-time 0.03 perplexity inf loss 300.58\n",
      "global step 11600 learning rate 0.0352 step-time 0.03 perplexity inf loss 314.39\n",
      "global step 11700 learning rate 0.0348 step-time 0.03 perplexity inf loss 315.67\n",
      "global step 11800 learning rate 0.0345 step-time 0.04 perplexity inf loss 303.48\n",
      "global step 11900 learning rate 0.0345 step-time 0.03 perplexity 245952329217519184364435593180007869353670390075535577164725382946482030278459812460305129446477319871231724300940204834816000.00 loss 288.72\n",
      "global step 12000 learning rate 0.0345 step-time 0.04 perplexity 85133200436730716823339917930045713677630737873370159231531484253338429938846131348000686192221538236107043252636391309312.00 loss 280.75\n",
      "global step 12100 learning rate 0.0345 step-time 0.03 perplexity 860200837173630220032747893948594992823068654446722631409663019101980438882578968530643376938424984742391607432053457822941184.00 loss 289.98\n",
      "global step 12200 learning rate 0.0341 step-time 0.03 perplexity 1077450550575437818677342536986939230829981130043660036340501911142112935467160810841235443862387431871409000539054109162995712.00 loss 290.20\n",
      "global step 12300 learning rate 0.0338 step-time 0.03 perplexity 97935127016016947264194232294951212177043882459965090749595463140427324754987248501649255870734043292433419843380331249926144.00 loss 287.80\n",
      "global step 12400 learning rate 0.0338 step-time 0.04 perplexity 1956991026073697774430303371948005612463000637693701806927666327802324947667307667756106136830664517449041046027632640.00 loss 270.07\n",
      "global step 12500 learning rate 0.0338 step-time 0.03 perplexity 1042133297006138123507327900752255102605895229561549355932789903547293353500021532655763888419766318907524521167990539943936.00 loss 283.26\n",
      "global step 12600 learning rate 0.0338 step-time 0.04 perplexity 2147000831006939724508512613906938088875513804941388980204684307912804649640730180329333457737372486946359290810597376.00 loss 270.17\n",
      "global step 12700 learning rate 0.0338 step-time 0.03 perplexity 14402385031653535556216062469618985241502154359502093379019873787520717886474059480312722150008028591982807430987776.00 loss 265.16\n",
      "global step 12800 learning rate 0.0338 step-time 0.04 perplexity 10518323616265374103500355609408085784337366066083195522764423221962967107600960206426853097695774341260117220539165048832.00 loss 278.66\n",
      "global step 12900 learning rate 0.0334 step-time 0.03 perplexity 697870492556079911405437750691793507454809729997177751119380854028452941774549886254748387152992278810853267753402368.00 loss 269.04\n",
      "global step 13000 learning rate 0.0334 step-time 0.04 perplexity 679168435800749046157082263417041179649540696827077994225708491704889121011977524646535112591002410639447883251712.00 loss 262.11\n",
      "global step 13100 learning rate 0.0334 step-time 0.04 perplexity 34905607732534041367264845020838482941933779904199533651939653776197545765615160246381119969870916646458097664.00 loss 252.23\n",
      "global step 13200 learning rate 0.0334 step-time 0.04 perplexity 843983057744857455450055019251295573073906768725852486179080092302199545862866528863690807148403787108613881856.00 loss 255.42\n",
      "global step 13300 learning rate 0.0334 step-time 0.03 perplexity 242471373647318652471324829914334520190557272191291806696468018087496607862850665925854049527091908645897633792.00 loss 254.17\n",
      "global step 13400 learning rate 0.0334 step-time 0.03 perplexity 154699715708338507235421016510310810865957928149453491729306107930680186636131630291535125876425577463808.00 loss 239.91\n",
      "global step 13500 learning rate 0.0334 step-time 0.03 perplexity 32763916243265828927340036085287938851744273258043758158122030356913883731211958164347280738034988527346022197755904.00 loss 265.98\n",
      "global step 13600 learning rate 0.0331 step-time 0.04 perplexity 18367681342756269877917196163905034537460257914141555392100464930771147657690761408504080591814771823188377600.00 loss 251.59\n",
      "global step 13700 learning rate 0.0331 step-time 0.03 perplexity 66758561745793465385119460407441423747546261116627734097054621177450562090624605792740626397577092553311780864.00 loss 252.88\n",
      "global step 13800 learning rate 0.0331 step-time 0.04 perplexity 33649219366626125930484990015253041096921429780543471127367619488923417295555314973449146533865323235377152.00 loss 245.29\n",
      "global step 13900 learning rate 0.0331 step-time 0.03 perplexity 75881320918260048652223479005584276463144385521029611274069547166925349611946910404651157767645151993251344941056.00 loss 259.92\n",
      "global step 14000 learning rate 0.0328 step-time 0.04 perplexity 2613003275962014960112369564523434232126471102797342396336970283659277473726337427904851872684491931648.00 loss 235.82\n",
      "global step 14100 learning rate 0.0328 step-time 0.03 perplexity 31003640523083149930135228658623890487047551116481954794081848339291342753978029521435220733435473693769728.00 loss 245.21\n",
      "global step 14200 learning rate 0.0328 step-time 0.03 perplexity 7985658671552816395505156403514645042307945273412283421030439746620431288942020608852306503730742446522368.00 loss 243.85\n",
      "global step 14300 learning rate 0.0328 step-time 0.03 perplexity 602762805665409181109347870377565190902515900302601870796133355843653526091424561480584894510523363379642368.00 loss 248.17\n",
      "global step 14400 learning rate 0.0325 step-time 0.04 perplexity 1607274565918995115056046194018175052411914068568795068020640431251076546592731151196908703023265480704.00 loss 235.34\n",
      "global step 14500 learning rate 0.0325 step-time 0.04 perplexity 20269002151520057814989502856147058598906650595994256632688869113741832329263185971478709803346783371264.00 loss 237.87\n",
      "global step 14600 learning rate 0.0325 step-time 0.03 perplexity 26515007583171182100716876253547158288876355253646923994166279203851375606183750439620797019306831790246723584.00 loss 251.96\n",
      "global step 14700 learning rate 0.0321 step-time 0.03 perplexity 9257476543032081671412645537965951770120728356554874138686479001748572394305291994190697661788218697187328.00 loss 244.00\n",
      "global step 14800 learning rate 0.0321 step-time 0.04 perplexity 14157972886105191630079608386235312717996912875710138398763309952108377424269110348267170684615157836939264.00 loss 244.42\n",
      "global step 14900 learning rate 0.0321 step-time 0.03 perplexity 14016349593586589117531935341276095933500790744186982732898672726564522263660444480389351202770830399373312.00 loss 244.41\n",
      "global step 15000 learning rate 0.0321 step-time 0.04 perplexity 3314439207630302968629004963588832939615942892543838259995412546467369428500975097413340571173608606400512.00 loss 242.97\n",
      "global step 15100 learning rate 0.0321 step-time 0.03 perplexity 83462081609632194444696894445291659526415636895960530175845447615555731093474186085852909063515809068061864665022464.00 loss 266.92\n",
      "global step 15200 learning rate 0.0318 step-time 0.03 perplexity 12861658303335983652177655783601350400077691097203660840251214231010494227712067327461692920072378078644404224.00 loss 251.23\n",
      "global step 15300 learning rate 0.0318 step-time 0.03 perplexity 2370145208103508364727148528059588118640478496321419842182349756884928901576648378642862928620102427252621312.00 loss 249.54\n",
      "global step 15400 learning rate 0.0318 step-time 0.03 perplexity 43291768519346773321480208171471877373766621247714923545983520339878106543892428719994130422774532520542208.00 loss 245.54\n",
      "global step 15500 learning rate 0.0318 step-time 0.03 perplexity 6269218126477158506616133919622252905860084909628888361685693902413105368833042006372298420435781230264320.00 loss 243.61\n",
      "global step 15600 learning rate 0.0318 step-time 0.03 perplexity 1149636911171292199449328676897559315152194207431800863096281448025601839570111188061687315140272193536.00 loss 235.00\n",
      "global step 15700 learning rate 0.0318 step-time 0.04 perplexity 1512605235504912705311163588765572851992003189994124835623376408708166683134263827862782440390056495512813568.00 loss 249.09\n",
      "global step 15800 learning rate 0.0315 step-time 0.03 perplexity 41209636381674619989353017422863505340663367867262141285553549061337460599202948970970547857915904.00 loss 224.77\n",
      "global step 15900 learning rate 0.0315 step-time 0.03 perplexity 8958314988594514722057663140714609845367059135108443833090009898738635714548092042502779614195348381106176.00 loss 243.96\n",
      "global step 16000 learning rate 0.0315 step-time 0.03 perplexity 856141988895240976367448574128834508641107149650025958119729336288707094009186423772093201126012718415872.00 loss 241.62\n",
      "global step 16100 learning rate 0.0315 step-time 0.04 perplexity 1078013373729175410883823618762941353227220112534486240133203099067815189998249414497078117410811373353787457536.00 loss 255.66\n",
      "global step 16200 learning rate 0.0312 step-time 0.03 perplexity 78074346567600760461631078621666606610875071075235005741328917902337163509466939033120397280269507533733888.00 loss 246.13\n",
      "global step 16300 learning rate 0.0312 step-time 0.03 perplexity 1983744935371468576871077714428016076405944651370296464823935180516367097593998321770344448205156411116617728.00 loss 249.36\n",
      "global step 16400 learning rate 0.0312 step-time 0.03 perplexity 37962025244611539337140554360216167105261520145933896572600492628169037848371165398692005563503427285339986722816.00 loss 259.22\n",
      "global step 16500 learning rate 0.0309 step-time 0.03 perplexity 442027632338124312644207507954621707701792965799678519502928188979088688891808581513107772007444693050462154772185088.00 loss 268.59\n",
      "global step 16600 learning rate 0.0306 step-time 0.04 perplexity 5230361429232992975119365674632157763172727813695067682744832630558810946119650704955903273363652239999350341632.00 loss 257.24\n",
      "global step 16700 learning rate 0.0306 step-time 0.03 perplexity 29740528194225437623336356140043899954952291164279225803193863092458118956896404319203968317210547230345265152.00 loss 252.07\n",
      "global step 16800 learning rate 0.0306 step-time 0.04 perplexity 5035606741979291519145006793306469826344440898252649412284650782825565217638671778137055849549332987510784.00 loss 243.39\n",
      "global step 16900 learning rate 0.0306 step-time 0.04 perplexity 191706331985040112608949943777195983799784285728360901724004787641548236632635254878628212186865205768093696.00 loss 247.03\n",
      "global step 17000 learning rate 0.0306 step-time 0.03 perplexity 14449319724192732159345299315530181327481030245051887348121108421580570927714955703063553015655166675779584.00 loss 244.44\n",
      "global step 17100 learning rate 0.0306 step-time 0.04 perplexity 29274094473531630744250953181609934419061196468325145660617554295787206943393745477951728951302009114245201920.00 loss 252.06\n",
      "global step 17200 learning rate 0.0303 step-time 0.03 perplexity 824130411486097462309858253199711921775869541372987184524887124421278627284423951600164900100374528.00 loss 227.76\n",
      "global step 17300 learning rate 0.0303 step-time 0.04 perplexity 163970336227159186995299189239081878671311581300667020688758094822687042808319038922007921550558232576.00 loss 233.06\n",
      "global step 17400 learning rate 0.0303 step-time 0.04 perplexity 3140535267333193924263801043777346305076982308352450842647647235873609981421428248659920739714990080.00 loss 229.10\n",
      "global step 17500 learning rate 0.0303 step-time 0.04 perplexity 55406598630051264412462692144576229009487685916835558258324394454313985795494569028515757756753379328.00 loss 231.97\n",
      "global step 17600 learning rate 0.0303 step-time 0.04 perplexity 2676105140373073772966929606119752414036058790312605517045708363731439998327176405127591175527255904949895168.00 loss 249.66\n",
      "global step 17700 learning rate 0.0299 step-time 0.04 perplexity 966380899965305987542025122287578626819822985876495302124809070001698918966489626357905021815004069888.00 loss 234.83\n",
      "global step 17800 learning rate 0.0299 step-time 0.03 perplexity 19499470004629545903551480383688464630507816414436880558883214669116677188864342504348681980870656.00 loss 224.02\n",
      "global step 17900 learning rate 0.0299 step-time 0.04 perplexity 160348107669280422768655005293285106351902521128224341121222491101753608603358840932957348452892672.00 loss 226.13\n",
      "global step 18000 learning rate 0.0299 step-time 0.04 perplexity 7497796975963863119961323069095781628505847607890319236675507362556993386286145447872227669465628672.00 loss 229.97\n",
      "global step 18100 learning rate 0.0296 step-time 0.04 perplexity 198884263243277462133777456215026178681925635892616963168574933675057241740838417248090725421598900224.00 loss 233.25\n",
      "global step 18200 learning rate 0.0294 step-time 0.04 perplexity 9278541889544727958463532188915082053795463202884461489363499682266464076418513010045381791958494609408.00 loss 237.09\n",
      "global step 18300 learning rate 0.0291 step-time 0.03 perplexity 1972802813699123480248822607430996239026785334335434380403683635745202800683363007214961196340871168.00 loss 228.64\n",
      "global step 18400 learning rate 0.0291 step-time 0.04 perplexity 66884379467253840880253246356286812208027882763494571144620358016169975193145139007504848871489536.00 loss 225.25\n",
      "global step 18500 learning rate 0.0291 step-time 0.03 perplexity 1309998455003173139742005139181870955028739897358091360952906773844600418249086858234051415769088.00 loss 221.32\n",
      "global step 18600 learning rate 0.0291 step-time 0.03 perplexity 167161746502082085806705580625009035388949886848323144099350338551541282663833271735207990394880.00 loss 219.26\n",
      "global step 18700 learning rate 0.0291 step-time 0.03 perplexity 21442511277636295052029703587945816878903210379470585442139986995354634065342657278553029083136.00 loss 217.21\n",
      "global step 18800 learning rate 0.0291 step-time 0.04 perplexity 9677329398713088949223063489810859893270280190791883599650731934397279019853192834488087871488.00 loss 216.41\n",
      "global step 18900 learning rate 0.0291 step-time 0.03 perplexity 323602420184755122908672082287185488109389450139205598437554692991899322096509378807988224.00 loss 206.10\n",
      "global step 19000 learning rate 0.0291 step-time 0.04 perplexity 4859756853440240016995370056044631415928397357028729846873195567440432120994348982117861313080197120.00 loss 229.54\n",
      "global step 19100 learning rate 0.0288 step-time 0.04 perplexity 9110900270404175785801261000077435624343112097225151750898845453824852335626943802323763200.00 loss 209.44\n",
      "global step 19200 learning rate 0.0288 step-time 0.04 perplexity 174061974133049390449164905427616615313042755346583370769407754915930708813911581309806837760.00 loss 212.39\n",
      "global step 19300 learning rate 0.0288 step-time 0.04 perplexity 22015998359572040244189549230147165727460273802387146433997996341730982365582443973443584.00 loss 203.42\n",
      "global step 19400 learning rate 0.0288 step-time 0.04 perplexity 5380726423932263704625076953689680652064091750829184864282039998981505089125215100731392.00 loss 202.01\n",
      "global step 19500 learning rate 0.0288 step-time 0.04 perplexity 474754483038567642534731808458650665342281391524740544471824618490412523802400925151517156048896.00 loss 220.30\n",
      "global step 19600 learning rate 0.0285 step-time 0.04 perplexity 4771776716500764800344304029500341044775062596525759777266522204025824656707700549222400.00 loss 201.89\n",
      "global step 19700 learning rate 0.0285 step-time 0.04 perplexity 899098855906230564877910929398886346387364420546523367053616454295139077060309874114560.00 loss 200.22\n",
      "global step 19800 learning rate 0.0285 step-time 0.03 perplexity 5630465473984718811479036558681529331722482004664244367070741640459291736590331674624.00 loss 195.15\n",
      "global step 19900 learning rate 0.0285 step-time 0.04 perplexity 82777272956569723163297555348047592289284800580623175033335838812553281583616380412887040.00 loss 204.74\n",
      "global step 20000 learning rate 0.0282 step-time 0.03 perplexity 250038372501611611672118665411157353147163779502592618238568592148737072928534199940415488.00 loss 205.85\n",
      "global step 20100 learning rate 0.0279 step-time 0.03 perplexity 1515485764039120598359708357588226888129567114769404196055423021838891835697162962862080.00 loss 200.74\n",
      "global step 20200 learning rate 0.0279 step-time 0.04 perplexity 1911842019434774691317706362695037225888991514319977201322572340581040124874958173110272.00 loss 200.97\n",
      "global step 20300 learning rate 0.0279 step-time 0.04 perplexity 2685590487130799171782641441933826126231248995476284303305457799718014300454912.00 loss 180.59\n",
      "global step 20400 learning rate 0.0279 step-time 0.05 perplexity 52529355318461272198469921830465708609861456058215333542072588989312485169720832557056.00 loss 197.38\n",
      "global step 20500 learning rate 0.0279 step-time 0.04 perplexity 60014064843249059743433110617647826977485819228727352616839664629908751606144303104.00 loss 190.60\n",
      "global step 20600 learning rate 0.0279 step-time 0.05 perplexity 75711086836715500945950197338113989966718737605553499542221768237201274716942671823241216.00 loss 204.65\n",
      "global step 20700 learning rate 0.0276 step-time 0.04 perplexity 357751461070995307510846724829936431270074151893816252014418171814900116293612673133774372864.00 loss 213.11\n",
      "global step 20800 learning rate 0.0274 step-time 0.04 perplexity 12351405852182204289227182658321355083470199201260019495021091080656889058272394858201088.00 loss 202.84\n",
      "global step 20900 learning rate 0.0274 step-time 0.04 perplexity 20966177682971237277732360506539748450724546820302699828458094686340625560672141312.00 loss 189.55\n",
      "global step 21000 learning rate 0.0274 step-time 0.04 perplexity 22167892514837944558217110276799330045218438600577206954520274120542795013733037899776.00 loss 196.52\n",
      "global step 21100 learning rate 0.0274 step-time 0.04 perplexity 911517500767087258416187198863195132345270771451212477694172954075759994291118669824.00 loss 193.32\n",
      "global step 21200 learning rate 0.0274 step-time 0.04 perplexity 109269366461332528171593265608061124646506060054917800289315344629180099255381000192.00 loss 191.20\n",
      "global step 21300 learning rate 0.0274 step-time 0.04 perplexity 5426800486497656995818937040025393761256827666137513992252815021204573491870826496.00 loss 188.20\n",
      "global step 21400 learning rate 0.0274 step-time 0.04 perplexity 9187143991622738667777122977087079857873299302758545454873308255173897593802457088.00 loss 188.73\n",
      "global step 21500 learning rate 0.0274 step-time 0.04 perplexity 949766258444643908724113036127944017783567274717060901396097073427044068491264.00 loss 179.55\n",
      "global step 21600 learning rate 0.0274 step-time 0.04 perplexity 27133321154394824124979331038693575561937503473298017579770672698972555664375702093824.00 loss 196.72\n",
      "global step 21700 learning rate 0.0271 step-time 0.04 perplexity 28799654863137679431860692880471070948870685429893795724029048149467932923296481280.00 loss 189.87\n",
      "global step 21800 learning rate 0.0271 step-time 0.04 perplexity 199996167084461458154735844171421457377268832392136869357106446521844922283196416.00 loss 184.90\n",
      "global step 21900 learning rate 0.0271 step-time 0.04 perplexity 16341865552918371535574345225299710459220360344420857459315488492848230170624.00 loss 175.49\n",
      "global step 22000 learning rate 0.0271 step-time 0.04 perplexity 8591058434236201421712728275247086499546008226996314178287628916081001758720.00 loss 174.84\n",
      "global step 22100 learning rate 0.0271 step-time 0.04 perplexity 354457141339743283351944223317998483868173567465065408994894635188989621474164736.00 loss 185.47\n",
      "global step 22200 learning rate 0.0268 step-time 0.04 perplexity 5176238782921110790949774855380590282990153321577863342096937216220479486951424.00 loss 181.25\n",
      "global step 22300 learning rate 0.0268 step-time 0.04 perplexity 618830517945284518378943443532933522373808312236958432097312010903943346246385664.00 loss 186.03\n",
      "global step 22400 learning rate 0.0265 step-time 0.04 perplexity 7464515866761511854069812643465548974097406933164535238590582972049586307203072.00 loss 181.61\n",
      "global step 22500 learning rate 0.0265 step-time 0.04 perplexity 1393334461973879395558031746261053392167606133443246003536819734236257455882371072.00 loss 186.84\n",
      "global step 22600 learning rate 0.0263 step-time 0.04 perplexity 1674351497040202130980696775362554518404557449829086711765757513672163328.00 loss 166.30\n",
      "global step 22700 learning rate 0.0263 step-time 0.04 perplexity 48366092268588991428879623835416429741005422107244421908635284314054122606166016.00 loss 183.48\n",
      "global step 22800 learning rate 0.0263 step-time 0.04 perplexity 48158173073903825953712046548253732232060825002236804519409702281790095360.00 loss 169.66\n",
      "global step 22900 learning rate 0.0263 step-time 0.04 perplexity 117892371803239544206263657189570458749027678952997524090148102885906711576576.00 loss 177.46\n",
      "global step 23000 learning rate 0.0263 step-time 0.04 perplexity 43692929526057339948549462760345674223797548003079546854558128716805832704000.00 loss 176.47\n",
      "global step 23100 learning rate 0.0263 step-time 0.04 perplexity 179496222893277095146647818514296095128233719879771936512623231687798554624.00 loss 170.98\n",
      "global step 23200 learning rate 0.0263 step-time 0.04 perplexity 13234413558987472901034785476339155388966163670292873272856768279796770668544.00 loss 175.28\n",
      "global step 23300 learning rate 0.0263 step-time 0.04 perplexity 413277104423913450851204706807264404965829739914996886502223724438070738550784.00 loss 178.72\n",
      "global step 23400 learning rate 0.0260 step-time 0.04 perplexity 16311551933698372362463657905889600315421752189782491540145254762250913185792.00 loss 175.49\n",
      "global step 23500 learning rate 0.0260 step-time 0.03 perplexity 426992010448788605906064674099453208123159896279677334869385371604484096.00 loss 164.94\n",
      "global step 23600 learning rate 0.0260 step-time 0.03 perplexity 38796753735698643960020705134130848566216272658301929397751021081980829696.00 loss 169.44\n",
      "global step 23700 learning rate 0.0260 step-time 0.03 perplexity 790404466094696387074054759625339261671098544317245429091995255570432.00 loss 158.64\n",
      "global step 23800 learning rate 0.0260 step-time 0.03 perplexity 23776314419017524116179357010612693961629401582825889755208806825984.00 loss 155.14\n",
      "global step 23900 learning rate 0.0260 step-time 0.03 perplexity 341330934926015106622214654770412439768826497072945961471981685571584.00 loss 157.80\n",
      "global step 24000 learning rate 0.0260 step-time 0.03 perplexity 50157520859111519368004068604755358348368890106425588739811784474048331776.00 loss 169.70\n",
      "global step 24100 learning rate 0.0258 step-time 0.03 perplexity 709497175479820648137692729744593841555553022556446491787924997144576.00 loss 158.54\n",
      "global step 24200 learning rate 0.0258 step-time 0.03 perplexity 1979453112153198487380559610582298471098525351384725576599787763564281856.00 loss 166.47\n",
      "global step 24300 learning rate 0.0258 step-time 0.03 perplexity 937524057773470669523048750150650054288355914424051330992970605264896.00 loss 158.81\n",
      "global step 24400 learning rate 0.0258 step-time 0.03 perplexity 124275193707677642227205374893199197934855373715509308389843222925410304.00 loss 163.70\n",
      "global step 24500 learning rate 0.0258 step-time 0.03 perplexity 33290521137139554230872133886894736928319684593050335509347588636672.00 loss 155.48\n",
      "global step 24600 learning rate 0.0258 step-time 0.03 perplexity 690980118732682580228210108118462342235503866431218343521658494880441171968.00 loss 172.32\n",
      "global step 24700 learning rate 0.0255 step-time 0.03 perplexity 158581803963905797049159564020998935738545443800999906196060614164480.00 loss 157.04\n",
      "global step 24800 learning rate 0.0255 step-time 0.03 perplexity 671701449392942692271510421120022506981868732177905151977893416377778176.00 loss 165.39\n",
      "global step 24900 learning rate 0.0255 step-time 0.03 perplexity 10933581095851118867189043107236915724031992418912600103998599857176576.00 loss 161.27\n",
      "global step 25000 learning rate 0.0255 step-time 0.03 perplexity 11024113011264998623017634903852015923313718301703307232293343461376.00 loss 154.37\n",
      "global step 25100 learning rate 0.0255 step-time 0.03 perplexity 22992632293258666840618100069318405184725831429677091205634008750751744.00 loss 162.01\n",
      "global step 25200 learning rate 0.0252 step-time 0.03 perplexity 15945422907545491568561877535335360950008586274488227333294215881818112.00 loss 161.65\n",
      "global step 25300 learning rate 0.0252 step-time 0.04 perplexity 8674005424721553125131346844616951137933005526728472780707250751668224.00 loss 161.04\n",
      "global step 25400 learning rate 0.0252 step-time 0.04 perplexity 8648315086206418447735434935773452665720311836463081226378160373760.00 loss 154.13\n",
      "global step 25500 learning rate 0.0252 step-time 0.03 perplexity 560874278230555958483501103578176471851630026373802707105557937913856.00 loss 158.30\n",
      "global step 25600 learning rate 0.0252 step-time 0.03 perplexity 690535113522709568784318874943922468658389643010279817094994329600.00 loss 151.60\n",
      "global step 25700 learning rate 0.0252 step-time 0.03 perplexity 62530971384441495764666849459730579960725063298736587353630965760.00 loss 149.20\n",
      "global step 25800 learning rate 0.0252 step-time 0.03 perplexity 704206623206334805040525494063004015051688524958582458705275518976.00 loss 151.62\n",
      "global step 25900 learning rate 0.0250 step-time 0.03 perplexity 1911605330163735437609792769924487285960894002614534460057781721366528.00 loss 159.53\n",
      "global step 26000 learning rate 0.0247 step-time 0.03 perplexity 101967402559934916862066249034528477664630193554801871140228694016.00 loss 149.69\n",
      "global step 26100 learning rate 0.0247 step-time 0.03 perplexity 10056648141883839881495215463211917231578917803301265781737491968032768.00 loss 161.19\n",
      "global step 26200 learning rate 0.0245 step-time 0.03 perplexity 4404113637543418556289728960691218892953079017835936378074955776.00 loss 146.55\n",
      "global step 26300 learning rate 0.0245 step-time 0.03 perplexity 22137171423931421624033199263180868810634688715285360260534421684224.00 loss 155.07\n",
      "global step 26400 learning rate 0.0245 step-time 0.03 perplexity 250159335559907328475601978038129002178703277184497590175627476992.00 loss 150.58\n",
      "global step 26500 learning rate 0.0245 step-time 0.03 perplexity 41445559752174811186051626232032841132469331765976291842616983552.00 loss 148.79\n",
      "global step 26600 learning rate 0.0245 step-time 0.03 perplexity 21147173280303663802630990146714234424958257038501883644310389260288.00 loss 155.02\n",
      "global step 26700 learning rate 0.0242 step-time 0.03 perplexity 4129481001957946817894201078817098083735012288706931715267887104.00 loss 146.48\n",
      "global step 26800 learning rate 0.0242 step-time 0.03 perplexity 6058014096507322305989302431784985338073513483845156340552227136471040.00 loss 160.68\n",
      "global step 26900 learning rate 0.0240 step-time 0.04 perplexity 1929530334041251974595923786776418846278995369683863502325809152.00 loss 145.72\n",
      "global step 27000 learning rate 0.0240 step-time 0.03 perplexity 24522699579563608450648793232859010567158257569745416107239843823616.00 loss 155.17\n",
      "global step 27100 learning rate 0.0240 step-time 0.03 perplexity 316828088745498413005699085249564017137626581724972410859195203584.00 loss 150.82\n",
      "global step 27200 learning rate 0.0240 step-time 0.03 perplexity 3651082230930826722344407581992065962100078891461152089179462565888.00 loss 153.27\n",
      "global step 27300 learning rate 0.0240 step-time 0.03 perplexity 725569410992704193222818021240940899325035965481973505310326784.00 loss 144.74\n",
      "global step 27400 learning rate 0.0240 step-time 0.03 perplexity 27566097278902894041185625384836905486415482470150081691516928.00 loss 141.47\n",
      "global step 27500 learning rate 0.0240 step-time 0.03 perplexity 6259932721567220624028975275136772979302184320730001566521622528.00 loss 146.90\n",
      "global step 27600 learning rate 0.0238 step-time 0.03 perplexity 11100438899342200342801373225849562023522283494754286521286656.00 loss 140.56\n",
      "global step 27700 learning rate 0.0238 step-time 0.03 perplexity 1257645455858104606305792548959780605298116179270066145067008.00 loss 138.38\n",
      "global step 27800 learning rate 0.0238 step-time 0.03 perplexity 5100692852374012380329541181502595755540759839972855382016.00 loss 132.88\n",
      "global step 27900 learning rate 0.0238 step-time 0.03 perplexity 20317328123571619867917839563262258948402918267668446314496.00 loss 134.26\n",
      "global step 28000 learning rate 0.0238 step-time 0.03 perplexity 1754955524433022749295224258023398760870531724212174848.00 loss 124.90\n",
      "global step 28100 learning rate 0.0238 step-time 0.03 perplexity 163450951931016304129004837940798558890764512037666029568.00 loss 129.44\n",
      "global step 28200 learning rate 0.0238 step-time 0.03 perplexity 38026109689393559928691059586841169794648168892553182052352.00 loss 134.89\n",
      "global step 28300 learning rate 0.0235 step-time 0.03 perplexity 72695158485098788342024582748389212952071494799284240384.00 loss 128.63\n",
      "global step 28400 learning rate 0.0235 step-time 0.03 perplexity 423479338334353007315428972800638483960037373611461312512.00 loss 130.39\n",
      "global step 28500 learning rate 0.0235 step-time 0.04 perplexity 4043027683409494602471371117423123831199235655296090112.00 loss 125.74\n",
      "global step 28600 learning rate 0.0235 step-time 0.04 perplexity 30124064446818951801967936437240615147867092384930070528.00 loss 127.74\n",
      "global step 28700 learning rate 0.0235 step-time 0.03 perplexity 4690225861153133022922882565805320718682239594503441047617536.00 loss 139.70\n",
      "global step 28800 learning rate 0.0233 step-time 0.03 perplexity 95749211936869933170211172224458720998325232177680220160.00 loss 128.90\n",
      "global step 28900 learning rate 0.0233 step-time 0.03 perplexity 226308557529372268099096375941826472009522401264428646400.00 loss 129.76\n",
      "global step 29000 learning rate 0.0233 step-time 0.03 perplexity 69625341037084820255871171839382404120396562519076703830016.00 loss 135.49\n",
      "global step 29100 learning rate 0.0231 step-time 0.03 perplexity 3496896594951371028293821091096519846428854807065608060928.00 loss 132.50\n",
      "global step 29200 learning rate 0.0231 step-time 0.03 perplexity 147077737643570604766529668611961837404304745431040.00 loss 115.52\n",
      "global step 29300 learning rate 0.0231 step-time 0.03 perplexity 62996918597548321688413591667998834877947363881992060928.00 loss 128.48\n",
      "global step 29400 learning rate 0.0231 step-time 0.04 perplexity 41636201753150652513062449547440730439158249426486172844032.00 loss 134.98\n",
      "global step 29500 learning rate 0.0228 step-time 0.03 perplexity 29712655248557955179052182542292121816600283406409924608.00 loss 127.73\n",
      "global step 29600 learning rate 0.0228 step-time 0.03 perplexity 11245207380054151760460292102800617160774903497967139815424.00 loss 133.67\n",
      "global step 29700 learning rate 0.0228 step-time 0.03 perplexity 55453546171956430169343372367647756771665549076784480256.00 loss 128.36\n",
      "global step 29800 learning rate 0.0228 step-time 0.03 perplexity 20021347876977439048741550936846931022559152942029438189568.00 loss 134.24\n",
      "global step 29900 learning rate 0.0226 step-time 0.03 perplexity 44926497319222830571335486239865575782176853707456512.00 loss 121.24\n",
      "global step 30000 learning rate 0.0226 step-time 0.03 perplexity 4742116818129514768934148489982361948159495059888144384.00 loss 125.90\n",
      "global step 30100 learning rate 0.0226 step-time 0.03 perplexity 46661837047221156526356121984535378166511431370907058176.00 loss 128.18\n",
      "global step 30200 learning rate 0.0224 step-time 0.03 perplexity 241455946362499549274819849170059071624295938523136.00 loss 116.01\n",
      "global step 30300 learning rate 0.0224 step-time 0.04 perplexity 13159743666773778346736610482056815357652883765230305280.00 loss 126.92\n",
      "global step 30400 learning rate 0.0224 step-time 0.04 perplexity 2787262625926569621557470145440074530213169798840320.00 loss 118.46\n",
      "global step 30500 learning rate 0.0224 step-time 0.03 perplexity 10244555148232519430579646139021002155023263476620460032.00 loss 126.67\n",
      "global step 30600 learning rate 0.0224 step-time 0.03 perplexity 1815798709414951593993056173514156614726763625987440640.00 loss 124.94\n",
      "global step 30700 learning rate 0.0224 step-time 0.03 perplexity 512755668400387989303565033549977438051047986490769408.00 loss 123.67\n",
      "global step 30800 learning rate 0.0224 step-time 0.03 perplexity 230128642493267995760236849491317156173606757643321344.00 loss 122.87\n",
      "global step 30900 learning rate 0.0224 step-time 0.03 perplexity 414391045679095859121081609062799696021736820572160.00 loss 116.55\n",
      "global step 31000 learning rate 0.0224 step-time 0.03 perplexity 735258753769647268870801357677064271251036574795501142016.00 loss 130.94\n",
      "global step 31100 learning rate 0.0222 step-time 0.03 perplexity 39924629663879981530420315971469356739108643403332583424.00 loss 128.03\n",
      "global step 31200 learning rate 0.0222 step-time 0.03 perplexity 167907073456254244010048289232249164458270705869389824.00 loss 122.56\n",
      "global step 31300 learning rate 0.0222 step-time 0.03 perplexity 242219049890149536731736626334111714107385066882596864.00 loss 122.92\n",
      "global step 31400 learning rate 0.0222 step-time 0.03 perplexity 16221123705530292520527128388125305165448952491802624.00 loss 120.22\n",
      "global step 31500 learning rate 0.0222 step-time 0.03 perplexity 2421281167663633356723695335952679858686492984475648.00 loss 118.32\n",
      "global step 31600 learning rate 0.0222 step-time 0.03 perplexity 594297930992160060611658565685135045626840481792.00 loss 110.00\n",
      "global step 31700 learning rate 0.0222 step-time 0.03 perplexity 19486123112801939224462430791070290349926343770112.00 loss 113.49\n",
      "global step 31800 learning rate 0.0222 step-time 0.03 perplexity 732664534531133217567842966249911253041723918516224.00 loss 117.12\n",
      "global step 31900 learning rate 0.0219 step-time 0.03 perplexity 313471331695145865317329930188140346180671985680384.00 loss 116.27\n",
      "global step 32000 learning rate 0.0219 step-time 0.03 perplexity 1943638846120455759625391996833054647004010905600.00 loss 111.19\n",
      "global step 32100 learning rate 0.0219 step-time 0.03 perplexity 102707818689236779406112542082211872454349946880.00 loss 108.25\n",
      "global step 32200 learning rate 0.0219 step-time 0.03 perplexity 10435941737218905403210010410989267081890285748224.00 loss 112.87\n",
      "global step 32300 learning rate 0.0217 step-time 0.04 perplexity 218579318288879240366601686947559878193978909130752.00 loss 115.91\n",
      "global step 32400 learning rate 0.0215 step-time 0.04 perplexity 4923439482414354170328946840831510529414965902704640.00 loss 119.03\n",
      "global step 32500 learning rate 0.0213 step-time 0.04 perplexity 3259904693615914258346696659503564902030337583874048.00 loss 118.61\n",
      "global step 32600 learning rate 0.0213 step-time 0.03 perplexity 150178835374700820671556291490618005751984071639040.00 loss 115.54\n",
      "global step 32700 learning rate 0.0213 step-time 0.04 perplexity 7802854893696679721475556507339261019495080132608.00 loss 112.58\n",
      "global step 32800 learning rate 0.0213 step-time 0.03 perplexity 201989736229816884791887780486367112017060298752.00 loss 108.92\n",
      "global step 32900 learning rate 0.0213 step-time 0.03 perplexity 102870461792433229322875480819519317909901850705920.00 loss 115.16\n",
      "global step 33000 learning rate 0.0211 step-time 0.03 perplexity 78737461846720023682399766073021359439065393397760.00 loss 114.89\n",
      "global step 33100 learning rate 0.0211 step-time 0.03 perplexity 188493006651967294909060991121784573753682821120.00 loss 108.86\n",
      "global step 33200 learning rate 0.0211 step-time 0.03 perplexity 287256166347856375288123113893923075172185627688960.00 loss 116.18\n",
      "global step 33300 learning rate 0.0209 step-time 0.03 perplexity 24136057720576093342658155373557683069189837815808.00 loss 113.71\n",
      "global step 33400 learning rate 0.0209 step-time 0.03 perplexity 99703002894088756148216834040608341657194070016.00 loss 108.22\n",
      "global step 33500 learning rate 0.0209 step-time 0.03 perplexity 59718841227700109726149542081432167849077237612544.00 loss 114.61\n",
      "global step 33600 learning rate 0.0206 step-time 0.03 perplexity 2660660343907557953839276643298746504730116096.00 loss 104.59\n",
      "global step 33700 learning rate 0.0206 step-time 0.03 perplexity 46415543445695373350895543423910314778853357125632.00 loss 114.36\n",
      "global step 33800 learning rate 0.0206 step-time 0.04 perplexity 4273475839569807796867908264657155174969638912.00 loss 105.07\n",
      "global step 33900 learning rate 0.0206 step-time 0.03 perplexity 14004134621348430144483950138429546649389891584.00 loss 106.26\n",
      "global step 34000 learning rate 0.0206 step-time 0.03 perplexity 33072181247224071241871334769332484789603940696064.00 loss 114.02\n",
      "global step 34100 learning rate 0.0204 step-time 0.03 perplexity 5840587993654784997687634812804905591765202894848.00 loss 112.29\n",
      "global step 34200 learning rate 0.0204 step-time 0.03 perplexity 506215795733481391111666860351850341400618714791936.00 loss 116.75\n",
      "global step 34300 learning rate 0.0202 step-time 0.03 perplexity 1264298871525515246346419983112373910736065265664.00 loss 110.76\n",
      "global step 34400 learning rate 0.0202 step-time 0.04 perplexity 251037226985927149271374189650765142460521775104.00 loss 109.14\n",
      "global step 34500 learning rate 0.0202 step-time 0.04 perplexity 1201385763577093402031221833429212355864035328.00 loss 103.80\n",
      "global step 34600 learning rate 0.0202 step-time 0.03 perplexity 8933203239972870803993249886308008934244352.00 loss 98.90\n",
      "global step 34700 learning rate 0.0202 step-time 0.03 perplexity 13111155704634517040344953076539794121312698368.00 loss 106.19\n",
      "global step 34800 learning rate 0.0200 step-time 0.03 perplexity 354137851721072555910848095939974046066671616.00 loss 102.58\n",
      "global step 34900 learning rate 0.0200 step-time 0.03 perplexity 31070673164571382704394931287330751984435200.00 loss 100.14\n",
      "global step 35000 learning rate 0.0200 step-time 0.03 perplexity 532809676218841246820357205397842565544280064.00 loss 102.99\n",
      "global step 35100 learning rate 0.0198 step-time 0.03 perplexity 557203399998625274863570877595533317192548352.00 loss 103.03\n",
      "global step 35200 learning rate 0.0196 step-time 0.03 perplexity 4688323263074857552072528263535695422774312960.00 loss 105.16\n",
      "global step 35300 learning rate 0.0194 step-time 0.04 perplexity 8067308511704071050415254476430102684260892672.00 loss 105.70\n",
      "global step 35400 learning rate 0.0192 step-time 0.03 perplexity 968272413001869595593770641853571749518508032.00 loss 103.58\n",
      "global step 35500 learning rate 0.0192 step-time 0.03 perplexity 2283110476977885861573320964082378385391616.00 loss 97.53\n",
      "global step 35600 learning rate 0.0192 step-time 0.04 perplexity 4956982383217582249414044919042882672787456.00 loss 98.31\n",
      "global step 35700 learning rate 0.0192 step-time 0.04 perplexity 30060648031554077926069499916396732482584576.00 loss 100.11\n",
      "global step 35800 learning rate 0.0191 step-time 0.03 perplexity 2829810719107164302934575927015955029295104.00 loss 97.75\n",
      "global step 35900 learning rate 0.0191 step-time 0.03 perplexity 7412146333501218642413229139023034205601792.00 loss 98.71\n",
      "global step 36000 learning rate 0.0191 step-time 0.03 perplexity 1344868093879500440990844316528419331375104.00 loss 97.00\n",
      "global step 36100 learning rate 0.0191 step-time 0.03 perplexity 30251779335521056450188971679831589650432.00 loss 93.21\n",
      "global step 36200 learning rate 0.0191 step-time 0.04 perplexity 2104930487860682482403814860422292145438720.00 loss 97.45\n",
      "global step 36300 learning rate 0.0189 step-time 0.03 perplexity 128628184587752922519543572519524998578176.00 loss 94.66\n",
      "global step 36400 learning rate 0.0189 step-time 0.03 perplexity 91279331059128571282993957872366440677376.00 loss 94.31\n",
      "global step 36500 learning rate 0.0189 step-time 0.03 perplexity 46575673830912462677161205578117563809792.00 loss 93.64\n",
      "global step 36600 learning rate 0.0189 step-time 0.03 perplexity 87238638486924084964686026774943891456000.00 loss 94.27\n",
      "global step 36700 learning rate 0.0189 step-time 0.03 perplexity 62731347447095025457127806307692259049472.00 loss 93.94\n",
      "global step 36800 learning rate 0.0189 step-time 0.03 perplexity 1477530160942446695718274923683967427149824.00 loss 97.10\n",
      "global step 36900 learning rate 0.0187 step-time 0.03 perplexity 4202809974877295324793282011658270539776.00 loss 91.24\n",
      "global step 37000 learning rate 0.0187 step-time 0.04 perplexity 22156626643637455424638596441158885310464.00 loss 92.90\n",
      "global step 37100 learning rate 0.0187 step-time 0.03 perplexity 262977257104567454742762295388364865536.00 loss 88.47\n",
      "global step 37200 learning rate 0.0187 step-time 0.03 perplexity 1148091035261691452976799566283741679583232.00 loss 96.85\n",
      "global step 37300 learning rate 0.0185 step-time 0.04 perplexity 5848326798864194641612303394835762511872.00 loss 91.57\n",
      "global step 37400 learning rate 0.0185 step-time 0.04 perplexity 16524206746027205439566236408581724307456.00 loss 92.61\n",
      "global step 37500 learning rate 0.0185 step-time 0.03 perplexity 156925873574787009012695410124712312832.00 loss 87.95\n",
      "global step 37600 learning rate 0.0185 step-time 0.04 perplexity 59459913309873142259631841580366692352.00 loss 86.98\n",
      "global step 37700 learning rate 0.0185 step-time 0.04 perplexity 4641047331387022292529722496563281920.00 loss 84.43\n",
      "global step 37800 learning rate 0.0185 step-time 0.04 perplexity 793524609959169499974981297031712490913792.00 loss 96.48\n",
      "global step 37900 learning rate 0.0183 step-time 0.04 perplexity 84644198756893822808943722168559343763456.00 loss 94.24\n",
      "global step 38000 learning rate 0.0183 step-time 0.04 perplexity 387913598286823022985017203751475216384.00 loss 88.85\n",
      "global step 38100 learning rate 0.0183 step-time 0.04 perplexity 173735335369493065666298327087448064000.00 loss 88.05\n",
      "global step 38200 learning rate 0.0183 step-time 0.04 perplexity 1564201628368859063441574671482355712.00 loss 83.34\n",
      "global step 38300 learning rate 0.0183 step-time 0.04 perplexity 45046970456741060083994633464681857024.00 loss 86.70\n",
      "global step 38400 learning rate 0.0183 step-time 0.04 perplexity 13286881593014358554946264222400512.00 loss 78.57\n",
      "global step 38500 learning rate 0.0183 step-time 0.04 perplexity 44257988398237343350761490675549601792.00 loss 86.68\n",
      "global step 38600 learning rate 0.0183 step-time 0.04 perplexity 34593197620323084112210280591144347762688.00 loss 93.34\n",
      "global step 38700 learning rate 0.0181 step-time 0.04 perplexity 8870057520354548103567823207835631616.00 loss 85.08\n",
      "global step 38800 learning rate 0.0181 step-time 0.04 perplexity 10918254179074312041167004703500795904.00 loss 85.28\n",
      "global step 38900 learning rate 0.0181 step-time 0.04 perplexity 9361012487091895081278025156519788544.00 loss 85.13\n",
      "global step 39000 learning rate 0.0181 step-time 0.03 perplexity 12978840485454349704103407754850336768.00 loss 85.46\n",
      "global step 39100 learning rate 0.0179 step-time 0.03 perplexity 3872421314280623328295444024335410397184.00 loss 91.15\n",
      "global step 39200 learning rate 0.0178 step-time 0.03 perplexity 1723895754750622144392778926159560704.00 loss 83.44\n",
      "global step 39300 learning rate 0.0178 step-time 0.04 perplexity 46453442387704498405911430590959190016.00 loss 86.73\n",
      "global step 39400 learning rate 0.0178 step-time 0.04 perplexity 413159379442183353586357084525654825762816.00 loss 95.82\n",
      "global step 39500 learning rate 0.0176 step-time 0.03 perplexity 863766533138773962882154993254670532608.00 loss 89.65\n",
      "global step 39600 learning rate 0.0176 step-time 0.03 perplexity 12800935191265022752969351700757306408960.00 loss 92.35\n",
      "global step 39700 learning rate 0.0176 step-time 0.04 perplexity 568898598683358593676201335618633990144.00 loss 89.24\n",
      "global step 39800 learning rate 0.0176 step-time 0.04 perplexity 3185506931059403878568494650397355409408.00 loss 90.96\n",
      "global step 39900 learning rate 0.0176 step-time 0.04 perplexity 746615939533944217901483607041230503936.00 loss 89.51\n",
      "global step 40000 learning rate 0.0176 step-time 0.04 perplexity 217950096511758857433306085378760263073792.00 loss 95.19\n",
      "global step 40100 learning rate 0.0174 step-time 0.03 perplexity 5235842866550623400635818279154346885120.00 loss 91.46\n",
      "global step 40200 learning rate 0.0174 step-time 0.03 perplexity 7155976838726721471374751812391322779648.00 loss 91.77\n",
      "global step 40300 learning rate 0.0174 step-time 0.04 perplexity 2037669052920450789658955000370094407680.00 loss 90.51\n",
      "global step 40400 learning rate 0.0174 step-time 0.03 perplexity 2904443169173985721217031078147044510007296.00 loss 97.77\n",
      "global step 40500 learning rate 0.0172 step-time 0.03 perplexity 84258570595789767936947172604026814464.00 loss 87.33\n",
      "global step 40600 learning rate 0.0172 step-time 0.03 perplexity 2415891301308789908174568146516049920.00 loss 83.78\n",
      "global step 40700 learning rate 0.0172 step-time 0.03 perplexity 2405135346891916431888425933493764096.00 loss 83.77\n",
      "global step 40800 learning rate 0.0172 step-time 0.03 perplexity 20699387474632446225931021676707840.00 loss 79.02\n",
      "global step 40900 learning rate 0.0172 step-time 0.03 perplexity 9516820800463401860553238378530734080.00 loss 85.15\n",
      "global step 41000 learning rate 0.0171 step-time 0.03 perplexity 1432022716175697812287082015682985984.00 loss 83.25\n",
      "global step 41100 learning rate 0.0171 step-time 0.04 perplexity 2560064321980857994752301723683913728.00 loss 83.83\n",
      "global step 41200 learning rate 0.0171 step-time 0.04 perplexity 1987256859484806329283392121677545472.00 loss 83.58\n",
      "global step 41300 learning rate 0.0171 step-time 0.05 perplexity 5003588478511274194103138091857346560.00 loss 84.50\n",
      "global step 41400 learning rate 0.0169 step-time 0.05 perplexity 2097978298343893295301966185159983104.00 loss 83.63\n",
      "global step 41500 learning rate 0.0169 step-time 0.04 perplexity 3048549942974641712295558898264309760.00 loss 84.01\n",
      "global step 41600 learning rate 0.0169 step-time 0.04 perplexity 27072308484362470463695433735451181056.00 loss 86.19\n",
      "global step 41700 learning rate 0.0167 step-time 0.04 perplexity 9242422415674511013818695677902848.00 loss 78.21\n",
      "global step 41800 learning rate 0.0167 step-time 0.04 perplexity 20354542704944608121072220725313536.00 loss 79.00\n",
      "global step 41900 learning rate 0.0167 step-time 0.04 perplexity 158012250365223905694396672492175360.00 loss 81.05\n",
      "global step 42000 learning rate 0.0166 step-time 0.04 perplexity 104034823218050860016589975807066112.00 loss 80.63\n",
      "global step 42100 learning rate 0.0166 step-time 0.04 perplexity 152479062091791898071513291890360320.00 loss 81.01\n",
      "global step 42200 learning rate 0.0166 step-time 0.04 perplexity 26260625665626092104460952586420224.00 loss 79.25\n",
      "global step 42300 learning rate 0.0166 step-time 0.04 perplexity 2009536153278707144376489379350183936.00 loss 83.59\n",
      "global step 42400 learning rate 0.0164 step-time 0.04 perplexity 51585610453217661875318230140898508800.00 loss 86.84\n",
      "global step 42500 learning rate 0.0162 step-time 0.04 perplexity 170079736108600429906219571141935104.00 loss 81.12\n",
      "global step 42600 learning rate 0.0162 step-time 0.03 perplexity 74091282238941681396757226420436992.00 loss 80.29\n",
      "global step 42700 learning rate 0.0162 step-time 0.03 perplexity 7310198072973494561583142977985839104.00 loss 84.88\n",
      "global step 42800 learning rate 0.0161 step-time 0.04 perplexity 21858930938850265563317376334692352.00 loss 79.07\n",
      "global step 42900 learning rate 0.0161 step-time 0.03 perplexity 433220319811512067169502167690117120.00 loss 82.06\n",
      "global step 43000 learning rate 0.0161 step-time 0.03 perplexity 24244709753027829124918274965897216.00 loss 79.17\n",
      "global step 43100 learning rate 0.0161 step-time 0.04 perplexity 18466886694195201379521860651188224.00 loss 78.90\n",
      "global step 43200 learning rate 0.0161 step-time 0.04 perplexity 59083129730171430186882352292036608.00 loss 80.06\n",
      "global step 43300 learning rate 0.0159 step-time 0.03 perplexity 24143659488316807453560497846091776.00 loss 79.17\n",
      "global step 43400 learning rate 0.0159 step-time 0.03 perplexity 669186439066580413247426214227869696.00 loss 82.49\n",
      "global step 43500 learning rate 0.0157 step-time 0.03 perplexity 182153529073072520984506778020478976.00 loss 81.19\n",
      "global step 43600 learning rate 0.0157 step-time 0.03 perplexity 895016732623732180689926888620032.00 loss 75.87\n",
      "global step 43700 learning rate 0.0157 step-time 0.03 perplexity 6926026518295004952933099896832.00 loss 71.01\n",
      "global step 43800 learning rate 0.0157 step-time 0.03 perplexity 675749680137042832371625012232192.00 loss 75.59\n",
      "global step 43900 learning rate 0.0157 step-time 0.03 perplexity 1206575735082386120373590078521344.00 loss 76.17\n",
      "global step 44000 learning rate 0.0156 step-time 0.03 perplexity 1229838986376482599408578162851840.00 loss 76.19\n",
      "global step 44100 learning rate 0.0154 step-time 0.03 perplexity 59911653933120147232789457497227264.00 loss 80.08\n",
      "global step 44200 learning rate 0.0153 step-time 0.03 perplexity 223528114785037728665920174292992.00 loss 74.49\n",
      "global step 44300 learning rate 0.0153 step-time 0.03 perplexity 3073218190580873801090876850569216.00 loss 77.11\n",
      "global step 44400 learning rate 0.0153 step-time 0.03 perplexity 382215909329242972382149029658624.00 loss 75.02\n",
      "global step 44500 learning rate 0.0153 step-time 0.03 perplexity 114585505426882681767043547529216.00 loss 73.82\n",
      "global step 44600 learning rate 0.0153 step-time 0.03 perplexity 11569302796420142687183666413568.00 loss 71.53\n",
      "global step 44700 learning rate 0.0153 step-time 0.03 perplexity 4255467464213238616088299700224.00 loss 70.53\n",
      "global step 44800 learning rate 0.0153 step-time 0.03 perplexity 110440603309427236768222329962496.00 loss 73.78\n",
      "global step 44900 learning rate 0.0151 step-time 0.03 perplexity 87695018397989939079056497573888.00 loss 73.55\n",
      "global step 45000 learning rate 0.0151 step-time 0.04 perplexity 793171460051576658773557884485632.00 loss 75.75\n",
      "global step 45100 learning rate 0.0150 step-time 0.03 perplexity 10362726478079051048042159407104.00 loss 71.42\n",
      "global step 45200 learning rate 0.0150 step-time 0.03 perplexity 3396933884336685707051476713472.00 loss 70.30\n",
      "global step 45300 learning rate 0.0150 step-time 0.04 perplexity 59703931095173637713038783121719296.00 loss 80.07\n",
      "global step 45400 learning rate 0.0148 step-time 0.04 perplexity 4215951960915932272650805508571136.00 loss 77.42\n",
      "global step 45500 learning rate 0.0148 step-time 0.03 perplexity 1657411945889719255048329552199680.00 loss 76.49\n",
      "global step 45600 learning rate 0.0148 step-time 0.04 perplexity 19970718313561987369952727793664.00 loss 72.07\n",
      "global step 45700 learning rate 0.0148 step-time 0.04 perplexity 97110596537926072977815521722368.00 loss 73.65\n",
      "global step 45800 learning rate 0.0148 step-time 0.04 perplexity 5974624292143033693014883662036992.00 loss 77.77\n",
      "global step 45900 learning rate 0.0147 step-time 0.03 perplexity 4310861210176717087790726119424.00 loss 70.54\n",
      "global step 46000 learning rate 0.0147 step-time 0.04 perplexity 56164627189342270361822136631296.00 loss 73.11\n",
      "global step 46100 learning rate 0.0147 step-time 0.04 perplexity 42374623615776533291033262292992.00 loss 72.82\n",
      "global step 46200 learning rate 0.0147 step-time 0.04 perplexity 8844217678572612382888235630592.00 loss 71.26\n",
      "global step 46300 learning rate 0.0147 step-time 0.04 perplexity 4604972571533591036781043663241216.00 loss 77.51\n",
      "global step 46400 learning rate 0.0145 step-time 0.03 perplexity 19817755710212021239115629461504.00 loss 72.06\n",
      "global step 46500 learning rate 0.0145 step-time 0.04 perplexity 202668832943837718091797979201536.00 loss 74.39\n",
      "global step 46600 learning rate 0.0145 step-time 0.04 perplexity 258260902364832228295525034622976.00 loss 74.63\n",
      "global step 46700 learning rate 0.0144 step-time 0.04 perplexity 140303671048246376845759815352320.00 loss 74.02\n",
      "global step 46800 learning rate 0.0144 step-time 0.04 perplexity 202233190498230054963527074971648.00 loss 74.39\n",
      "global step 46900 learning rate 0.0144 step-time 0.04 perplexity 10459406553733593426165197963264.00 loss 71.43\n",
      "global step 47000 learning rate 0.0144 step-time 0.04 perplexity 2500428082383529471767662821376.00 loss 69.99\n",
      "global step 47100 learning rate 0.0144 step-time 0.03 perplexity 2193587509328376042624550699008.00 loss 69.86\n",
      "global step 47200 learning rate 0.0144 step-time 0.04 perplexity 464134875634400364055542890496.00 loss 68.31\n",
      "global step 47300 learning rate 0.0144 step-time 0.04 perplexity 35128747067865549880396263456768.00 loss 72.64\n",
      "global step 47400 learning rate 0.0142 step-time 0.04 perplexity 44053562897487418407472070656.00 loss 65.96\n",
      "global step 47500 learning rate 0.0142 step-time 0.03 perplexity 129892892901876901833280061440.00 loss 67.04\n",
      "global step 47600 learning rate 0.0142 step-time 0.03 perplexity 411290924326243293406575984640.00 loss 68.19\n",
      "global step 47700 learning rate 0.0141 step-time 0.04 perplexity 52783264325445525068023070720.00 loss 66.14\n",
      "global step 47800 learning rate 0.0141 step-time 0.03 perplexity 117970916413051896881406279680.00 loss 66.94\n",
      "global step 47900 learning rate 0.0141 step-time 0.04 perplexity 11001169348109080495633486315520.00 loss 71.48\n",
      "global step 48000 learning rate 0.0140 step-time 0.03 perplexity 101806146803682774114491170816.00 loss 66.79\n",
      "global step 48100 learning rate 0.0140 step-time 0.04 perplexity 1445242843394032894738250269196288.00 loss 76.35\n",
      "global step 48200 learning rate 0.0138 step-time 0.03 perplexity 158271955388611822610401984512.00 loss 67.23\n",
      "global step 48300 learning rate 0.0138 step-time 0.04 perplexity 2447868564446830420658487296000.00 loss 69.97\n",
      "global step 48400 learning rate 0.0138 step-time 0.04 perplexity 766743988238080641922498560.00 loss 61.90\n",
      "global step 48500 learning rate 0.0138 step-time 0.04 perplexity 231635849915801014311382417408.00 loss 67.61\n",
      "global step 48600 learning rate 0.0138 step-time 0.03 perplexity 39793850572131156592003383296.00 loss 65.85\n",
      "global step 48700 learning rate 0.0138 step-time 0.04 perplexity 2453283206248356306023052476416.00 loss 69.97\n",
      "global step 48800 learning rate 0.0137 step-time 0.03 perplexity 7978112248202614016228735320064.00 loss 71.15\n",
      "global step 48900 learning rate 0.0135 step-time 0.04 perplexity 1101841237231280495437102972928.00 loss 69.17\n",
      "global step 49000 learning rate 0.0135 step-time 0.04 perplexity 12634657573686978946906718208.00 loss 64.71\n",
      "global step 49100 learning rate 0.0135 step-time 0.04 perplexity 19993276597277322619077525504.00 loss 65.17\n",
      "global step 49200 learning rate 0.0135 step-time 0.03 perplexity 378100656205086624297860988928.00 loss 68.10\n",
      "global step 49300 learning rate 0.0134 step-time 0.04 perplexity 10171247839942574145548582912.00 loss 64.49\n",
      "global step 49400 learning rate 0.0134 step-time 0.04 perplexity 11810035856106068501466185728.00 loss 64.64\n",
      "global step 49500 learning rate 0.0134 step-time 0.03 perplexity 2100909203726128637603217408.00 loss 62.91\n",
      "global step 49600 learning rate 0.0134 step-time 0.03 perplexity 4563344082977907310472462336.00 loss 63.69\n",
      "global step 49700 learning rate 0.0134 step-time 0.03 perplexity 315219078364174573901971456.00 loss 61.02\n",
      "global step 49800 learning rate 0.0134 step-time 0.04 perplexity 84986937920694136982292398080.00 loss 66.61\n",
      "global step 49900 learning rate 0.0133 step-time 0.03 perplexity 11472437390201042175617138688.00 loss 64.61\n",
      "global step 50000 learning rate 0.0133 step-time 0.03 perplexity 87389561207331450349867237376.00 loss 66.64\n",
      "global step 50100 learning rate 0.0131 step-time 0.04 perplexity 64365754128129477776553017344.00 loss 66.33\n",
      "global step 50200 learning rate 0.0131 step-time 0.04 perplexity 452987840389298245873958912.00 loss 61.38\n",
      "global step 50300 learning rate 0.0131 step-time 0.04 perplexity 9170592588105543047291338752.00 loss 64.39\n",
      "global step 50400 learning rate 0.0131 step-time 0.04 perplexity 9472603071292801957333303296.00 loss 64.42\n",
      "global step 50500 learning rate 0.0130 step-time 0.04 perplexity 463424154513733607057522688.00 loss 61.40\n",
      "global step 50600 learning rate 0.0130 step-time 0.03 perplexity 32705153926405267758589673472.00 loss 65.66\n",
      "global step 50700 learning rate 0.0129 step-time 0.03 perplexity 56619267885468250838380052480.00 loss 66.21\n",
      "global step 50800 learning rate 0.0127 step-time 0.03 perplexity 22386241363492449174945792.00 loss 58.37\n",
      "global step 50900 learning rate 0.0127 step-time 0.03 perplexity 709657278466679021619904512.00 loss 61.83\n",
      "global step 51000 learning rate 0.0127 step-time 0.03 perplexity 159334920170669935727476736.00 loss 60.33\n",
      "global step 51100 learning rate 0.0127 step-time 0.03 perplexity 813748708367891875969892352.00 loss 61.96\n",
      "global step 51200 learning rate 0.0126 step-time 0.03 perplexity 10966159417096167669891072.00 loss 57.66\n",
      "global step 51300 learning rate 0.0126 step-time 0.03 perplexity 14359729185787152621895680.00 loss 57.93\n",
      "global step 51400 learning rate 0.0126 step-time 0.03 perplexity 10616557926453358668808192.00 loss 57.62\n",
      "global step 51500 learning rate 0.0126 step-time 0.03 perplexity 305661265117915179259002880.00 loss 60.98\n",
      "global step 51600 learning rate 0.0125 step-time 0.03 perplexity 21409568586742353037885440.00 loss 58.33\n",
      "global step 51700 learning rate 0.0125 step-time 0.03 perplexity 3498965156814699005739008.00 loss 56.51\n",
      "global step 51800 learning rate 0.0125 step-time 0.03 perplexity 2172812424832889730891776.00 loss 56.04\n",
      "global step 51900 learning rate 0.0125 step-time 0.03 perplexity 433017271693510308790272.00 loss 54.43\n",
      "global step 52000 learning rate 0.0125 step-time 0.03 perplexity 2835986140227274744528896.00 loss 56.30\n",
      "global step 52100 learning rate 0.0124 step-time 0.03 perplexity 26450843846094226456576.00 loss 51.63\n",
      "global step 52200 learning rate 0.0124 step-time 0.03 perplexity 301785812348086205284352.00 loss 54.06\n",
      "global step 52300 learning rate 0.0124 step-time 0.03 perplexity 62166598389177056755712.00 loss 52.48\n",
      "global step 52400 learning rate 0.0124 step-time 0.03 perplexity 806722469914244397662208.00 loss 55.05\n",
      "global step 52500 learning rate 0.0122 step-time 0.03 perplexity 49135142763575988191232.00 loss 52.25\n",
      "global step 52600 learning rate 0.0122 step-time 0.03 perplexity 4084841196739593368502272.00 loss 56.67\n",
      "global step 52700 learning rate 0.0121 step-time 0.04 perplexity 2005693467764643594240.00 loss 49.05\n",
      "global step 52800 learning rate 0.0121 step-time 0.03 perplexity 179213870847797952512.00 loss 46.64\n",
      "global step 52900 learning rate 0.0121 step-time 0.03 perplexity 14035156094525842128896.00 loss 51.00\n",
      "global step 53000 learning rate 0.0120 step-time 0.03 perplexity 1186419384354396346253312.00 loss 55.43\n",
      "global step 53100 learning rate 0.0119 step-time 0.03 perplexity 53485149893900460097536.00 loss 52.33\n",
      "global step 53200 learning rate 0.0119 step-time 0.04 perplexity 217818872754707638517760.00 loss 53.74\n",
      "global step 53300 learning rate 0.0119 step-time 0.04 perplexity 1326998013402281934848.00 loss 48.64\n",
      "global step 53400 learning rate 0.0119 step-time 0.04 perplexity 43816173429357180616704.00 loss 52.13\n",
      "global step 53500 learning rate 0.0119 step-time 0.03 perplexity 610642399900494194540544.00 loss 54.77\n",
      "global step 53600 learning rate 0.0118 step-time 0.03 perplexity 254965837048334909440.00 loss 46.99\n",
      "global step 53700 learning rate 0.0118 step-time 0.03 perplexity 60933185731542109913088.00 loss 52.46\n",
      "global step 53800 learning rate 0.0118 step-time 0.03 perplexity 7955815067570122260480.00 loss 50.43\n",
      "global step 53900 learning rate 0.0118 step-time 0.03 perplexity 194149132343689281536.00 loss 46.72\n",
      "global step 54000 learning rate 0.0118 step-time 0.03 perplexity 622403611367687979008.00 loss 47.88\n",
      "global step 54100 learning rate 0.0118 step-time 0.03 perplexity 4345908750077587881984.00 loss 49.82\n",
      "global step 54200 learning rate 0.0116 step-time 0.03 perplexity 937666659065323061248.00 loss 48.29\n",
      "global step 54300 learning rate 0.0116 step-time 0.03 perplexity 21771428636100616192.00 loss 44.53\n",
      "global step 54400 learning rate 0.0116 step-time 0.03 perplexity 700065924250861830144.00 loss 48.00\n",
      "global step 54500 learning rate 0.0116 step-time 0.03 perplexity 34974761702948828348416.00 loss 51.91\n",
      "global step 54600 learning rate 0.0115 step-time 0.03 perplexity 405434800620855296000.00 loss 47.45\n",
      "global step 54700 learning rate 0.0115 step-time 0.03 perplexity 6498100844611825041408.00 loss 50.23\n",
      "global step 54800 learning rate 0.0115 step-time 0.03 perplexity 601888519245587808256.00 loss 47.85\n",
      "global step 54900 learning rate 0.0115 step-time 0.03 perplexity 135537258368672694272.00 loss 46.36\n",
      "global step 55000 learning rate 0.0115 step-time 0.03 perplexity 502833100074469883904.00 loss 47.67\n",
      "global step 55100 learning rate 0.0115 step-time 0.03 perplexity 11576081554194349162496.00 loss 50.80\n",
      "global step 55200 learning rate 0.0114 step-time 0.03 perplexity 4734157207258714341376.00 loss 49.91\n",
      "global step 55300 learning rate 0.0114 step-time 0.03 perplexity 3094900563332522573824.00 loss 49.48\n",
      "global step 55400 learning rate 0.0114 step-time 0.03 perplexity 7484718304072634990592.00 loss 50.37\n",
      "global step 55500 learning rate 0.0113 step-time 0.03 perplexity 2583865772215648649216.00 loss 49.30\n",
      "global step 55600 learning rate 0.0113 step-time 0.03 perplexity 168626466544794468352.00 loss 46.57\n",
      "global step 55700 learning rate 0.0113 step-time 0.03 perplexity 106655174491312570368.00 loss 46.12\n",
      "global step 55800 learning rate 0.0113 step-time 0.03 perplexity 772800674617062981632.00 loss 48.10\n",
      "global step 55900 learning rate 0.0112 step-time 0.03 perplexity 160048691341955497984.00 loss 46.52\n",
      "global step 56000 learning rate 0.0112 step-time 0.03 perplexity 277654440790985408512.00 loss 47.07\n",
      "global step 56100 learning rate 0.0112 step-time 0.03 perplexity 283524678589883908096.00 loss 47.09\n",
      "global step 56200 learning rate 0.0111 step-time 0.03 perplexity 41689529423205752832.00 loss 45.18\n",
      "global step 56300 learning rate 0.0111 step-time 0.03 perplexity 174133172086718922752.00 loss 46.61\n",
      "global step 56400 learning rate 0.0111 step-time 0.03 perplexity 538160281318587695104.00 loss 47.73\n",
      "global step 56500 learning rate 0.0110 step-time 0.03 perplexity 37397294486852386816.00 loss 45.07\n",
      "global step 56600 learning rate 0.0110 step-time 0.03 perplexity 1560224253600866500608.00 loss 48.80\n",
      "global step 56700 learning rate 0.0109 step-time 0.03 perplexity 46281178137418883072.00 loss 45.28\n",
      "global step 56800 learning rate 0.0109 step-time 0.03 perplexity 18766611540125245440.00 loss 44.38\n",
      "global step 56900 learning rate 0.0109 step-time 0.03 perplexity 1004609374224217931776.00 loss 48.36\n",
      "global step 57000 learning rate 0.0107 step-time 0.03 perplexity 6440023197317558272.00 loss 43.31\n",
      "global step 57100 learning rate 0.0107 step-time 0.03 perplexity 555882247213398080.00 loss 40.86\n",
      "global step 57200 learning rate 0.0107 step-time 0.03 perplexity 17746575510415714304.00 loss 44.32\n",
      "global step 57300 learning rate 0.0106 step-time 0.03 perplexity 74344832567192895488.00 loss 45.76\n",
      "global step 57400 learning rate 0.0105 step-time 0.04 perplexity 108056148365764837376.00 loss 46.13\n",
      "global step 57500 learning rate 0.0104 step-time 0.03 perplexity 27023408082293514240.00 loss 44.74\n",
      "global step 57600 learning rate 0.0104 step-time 0.04 perplexity 394454526589908480.00 loss 40.52\n",
      "global step 57700 learning rate 0.0104 step-time 0.04 perplexity 268219033256805120.00 loss 40.13\n",
      "global step 57800 learning rate 0.0104 step-time 0.04 perplexity 444762740534154944.00 loss 40.64\n",
      "global step 57900 learning rate 0.0103 step-time 0.04 perplexity 3075475060797216256.00 loss 42.57\n",
      "global step 58000 learning rate 0.0102 step-time 0.03 perplexity 669551163704173440.00 loss 41.05\n",
      "global step 58100 learning rate 0.0102 step-time 0.03 perplexity 27365966159911342080.00 loss 44.76\n",
      "global step 58200 learning rate 0.0101 step-time 0.03 perplexity 322875501031998912.00 loss 40.32\n",
      "global step 58300 learning rate 0.0101 step-time 0.04 perplexity 146613576034570496.00 loss 39.53\n",
      "global step 58400 learning rate 0.0101 step-time 0.04 perplexity 679091738035724800.00 loss 41.06\n",
      "global step 58500 learning rate 0.0100 step-time 0.03 perplexity 22555440444924908.00 loss 37.65\n",
      "global step 58600 learning rate 0.0100 step-time 0.04 perplexity 117618646235162272.00 loss 39.31\n",
      "global step 58700 learning rate 0.0100 step-time 0.04 perplexity 2428310985873649152.00 loss 42.33\n",
      "global step 58800 learning rate 0.0099 step-time 0.04 perplexity 58387587216397136.00 loss 38.61\n",
      "global step 58900 learning rate 0.0099 step-time 0.04 perplexity 10110769990963322.00 loss 36.85\n",
      "global step 59000 learning rate 0.0099 step-time 0.03 perplexity 76915265147364272.00 loss 38.88\n",
      "global step 59100 learning rate 0.0098 step-time 0.03 perplexity 42239044577046000.00 loss 38.28\n",
      "global step 59200 learning rate 0.0098 step-time 0.03 perplexity 4495417058731243.00 loss 36.04\n",
      "global step 59300 learning rate 0.0098 step-time 0.03 perplexity 74690286904185888.00 loss 38.85\n",
      "global step 59400 learning rate 0.0097 step-time 0.04 perplexity 93794729736974448.00 loss 39.08\n",
      "global step 59500 learning rate 0.0096 step-time 0.03 perplexity 5768706258851261.00 loss 36.29\n",
      "global step 59600 learning rate 0.0096 step-time 0.03 perplexity 77180921010986080.00 loss 38.88\n",
      "global step 59700 learning rate 0.0096 step-time 0.03 perplexity 284585443691968800.00 loss 40.19\n",
      "global step 59800 learning rate 0.0095 step-time 0.03 perplexity 64158035368839232.00 loss 38.70\n",
      "global step 59900 learning rate 0.0095 step-time 0.03 perplexity 1289681425355028224.00 loss 41.70\n",
      "global step 60000 learning rate 0.0094 step-time 0.03 perplexity 1512240692747288.50 loss 34.95\n",
      "global step 60100 learning rate 0.0094 step-time 0.03 perplexity 330715136159385728.00 loss 40.34\n",
      "global step 60200 learning rate 0.0094 step-time 0.03 perplexity 3645769403445679.00 loss 35.83\n",
      "global step 60300 learning rate 0.0094 step-time 0.03 perplexity 37377019852325064.00 loss 38.16\n",
      "global step 60400 learning rate 0.0094 step-time 0.03 perplexity 334731083171316.88 loss 33.44\n",
      "global step 60500 learning rate 0.0094 step-time 0.03 perplexity 1088402209688933.88 loss 34.62\n",
      "global step 60600 learning rate 0.0094 step-time 0.03 perplexity 2437795793674642.50 loss 35.43\n",
      "global step 60700 learning rate 0.0093 step-time 0.03 perplexity 1538004144898889.00 loss 34.97\n",
      "global step 60800 learning rate 0.0093 step-time 0.03 perplexity 94863590780417.22 loss 32.18\n",
      "global step 60900 learning rate 0.0093 step-time 0.03 perplexity 1646342292843312.25 loss 35.04\n",
      "global step 61000 learning rate 0.0092 step-time 0.03 perplexity 22585113656703548.00 loss 37.66\n",
      "global step 61100 learning rate 0.0091 step-time 0.03 perplexity 1038854724800935.62 loss 34.58\n",
      "global step 61200 learning rate 0.0091 step-time 0.04 perplexity 1223795442019879.75 loss 34.74\n",
      "global step 61300 learning rate 0.0091 step-time 0.03 perplexity 1304591574078296.75 loss 34.80\n",
      "global step 61400 learning rate 0.0091 step-time 0.03 perplexity 3956462699741984.50 loss 35.91\n",
      "global step 61500 learning rate 0.0090 step-time 0.04 perplexity 63022463908098120.00 loss 38.68\n",
      "global step 61600 learning rate 0.0089 step-time 0.03 perplexity 932667435601715.75 loss 34.47\n",
      "global step 61700 learning rate 0.0089 step-time 0.03 perplexity 984664984752198.88 loss 34.52\n",
      "global step 61800 learning rate 0.0089 step-time 0.03 perplexity 75289674710311.69 loss 31.95\n",
      "global step 61900 learning rate 0.0089 step-time 0.03 perplexity 161894668123523.69 loss 32.72\n",
      "global step 62000 learning rate 0.0089 step-time 0.04 perplexity 4535828357485605.00 loss 36.05\n",
      "global step 62100 learning rate 0.0088 step-time 0.04 perplexity 2752584611218594.00 loss 35.55\n",
      "global step 62200 learning rate 0.0088 step-time 0.03 perplexity 63564727538984.59 loss 31.78\n",
      "global step 62300 learning rate 0.0088 step-time 0.03 perplexity 1911905745650720.50 loss 35.19\n",
      "global step 62400 learning rate 0.0088 step-time 0.03 perplexity 685259529769091.25 loss 34.16\n",
      "global step 62500 learning rate 0.0088 step-time 0.03 perplexity 213220185896492.97 loss 32.99\n",
      "global step 62600 learning rate 0.0088 step-time 0.03 perplexity 2013603344928798.75 loss 35.24\n",
      "global step 62700 learning rate 0.0087 step-time 0.03 perplexity 202125260742500.47 loss 32.94\n",
      "global step 62800 learning rate 0.0087 step-time 0.04 perplexity 79128968361326.41 loss 32.00\n",
      "global step 62900 learning rate 0.0087 step-time 0.03 perplexity 589208868814787.12 loss 34.01\n",
      "global step 63000 learning rate 0.0086 step-time 0.04 perplexity 108211848858795.59 loss 32.32\n",
      "global step 63100 learning rate 0.0086 step-time 0.04 perplexity 42858301307319.92 loss 31.39\n",
      "global step 63200 learning rate 0.0086 step-time 0.04 perplexity 141070718560757.06 loss 32.58\n",
      "global step 63300 learning rate 0.0085 step-time 0.03 perplexity 26804059704925.59 loss 30.92\n",
      "global step 63400 learning rate 0.0085 step-time 0.03 perplexity 48651282407361.30 loss 31.52\n",
      "global step 63500 learning rate 0.0085 step-time 0.03 perplexity 97385146938490.19 loss 32.21\n",
      "global step 63600 learning rate 0.0084 step-time 0.04 perplexity 44244163853821.47 loss 31.42\n",
      "global step 63700 learning rate 0.0084 step-time 0.04 perplexity 440052608936729.31 loss 33.72\n",
      "global step 63800 learning rate 0.0084 step-time 0.04 perplexity 77984455322430.70 loss 31.99\n",
      "global step 63900 learning rate 0.0084 step-time 0.03 perplexity 11732211940974.23 loss 30.09\n",
      "global step 64000 learning rate 0.0084 step-time 0.03 perplexity 107992099786721.08 loss 32.31\n",
      "global step 64100 learning rate 0.0083 step-time 0.04 perplexity 100182123308039.53 loss 32.24\n",
      "global step 64200 learning rate 0.0083 step-time 0.04 perplexity 18954541944238.52 loss 30.57\n",
      "global step 64300 learning rate 0.0083 step-time 0.04 perplexity 233156460028338.47 loss 33.08\n",
      "global step 64400 learning rate 0.0082 step-time 0.04 perplexity 10851984541774.69 loss 30.02\n",
      "global step 64500 learning rate 0.0082 step-time 0.04 perplexity 323699976230533.69 loss 33.41\n",
      "global step 64600 learning rate 0.0081 step-time 0.04 perplexity 44289854878589.45 loss 31.42\n",
      "global step 64700 learning rate 0.0081 step-time 0.04 perplexity 107642008995490.84 loss 32.31\n",
      "global step 64800 learning rate 0.0081 step-time 0.04 perplexity 44294663947011.38 loss 31.42\n",
      "global step 64900 learning rate 0.0081 step-time 0.03 perplexity 8151828782351.12 loss 29.73\n",
      "global step 65000 learning rate 0.0081 step-time 0.04 perplexity 43809312572907.74 loss 31.41\n",
      "global step 65100 learning rate 0.0081 step-time 0.03 perplexity 69402683116154.67 loss 31.87\n",
      "global step 65200 learning rate 0.0080 step-time 0.04 perplexity 12864910095369.76 loss 30.19\n",
      "global step 65300 learning rate 0.0080 step-time 0.04 perplexity 28798244998444.13 loss 30.99\n",
      "global step 65400 learning rate 0.0080 step-time 0.03 perplexity 2565306826533.21 loss 28.57\n",
      "global step 65500 learning rate 0.0080 step-time 0.04 perplexity 405410213694660.25 loss 33.64\n",
      "global step 65600 learning rate 0.0079 step-time 0.03 perplexity 2206951318935.89 loss 28.42\n",
      "global step 65700 learning rate 0.0079 step-time 0.03 perplexity 1729798891361.00 loss 28.18\n",
      "global step 65800 learning rate 0.0079 step-time 0.03 perplexity 122512420419344.39 loss 32.44\n",
      "global step 65900 learning rate 0.0079 step-time 0.04 perplexity 4233793143302.43 loss 29.07\n",
      "global step 66000 learning rate 0.0079 step-time 0.03 perplexity 10055531851655.41 loss 29.94\n",
      "global step 66100 learning rate 0.0079 step-time 0.03 perplexity 822843559607.13 loss 27.44\n",
      "global step 66200 learning rate 0.0079 step-time 0.03 perplexity 1905676760779.60 loss 28.28\n",
      "global step 66300 learning rate 0.0079 step-time 0.04 perplexity 1518291372088.49 loss 28.05\n",
      "global step 66400 learning rate 0.0079 step-time 0.03 perplexity 8195814991331.40 loss 29.73\n",
      "global step 66500 learning rate 0.0078 step-time 0.03 perplexity 304094060969.25 loss 26.44\n",
      "global step 66600 learning rate 0.0078 step-time 0.03 perplexity 1197449755565.06 loss 27.81\n",
      "global step 66700 learning rate 0.0078 step-time 0.03 perplexity 600755745460.49 loss 27.12\n",
      "global step 66800 learning rate 0.0078 step-time 0.03 perplexity 1732613866032.67 loss 28.18\n",
      "global step 66900 learning rate 0.0077 step-time 0.03 perplexity 6895677053888.85 loss 29.56\n",
      "global step 67000 learning rate 0.0076 step-time 0.03 perplexity 223729140608.47 loss 26.13\n",
      "global step 67100 learning rate 0.0076 step-time 0.03 perplexity 1394278260679.17 loss 27.96\n",
      "global step 67200 learning rate 0.0076 step-time 0.03 perplexity 1411415495041.64 loss 27.98\n",
      "global step 67300 learning rate 0.0076 step-time 0.03 perplexity 1014692162063.97 loss 27.65\n",
      "global step 67400 learning rate 0.0076 step-time 0.03 perplexity 1021007473710.28 loss 27.65\n",
      "global step 67500 learning rate 0.0076 step-time 0.03 perplexity 1549873387425.32 loss 28.07\n",
      "global step 67600 learning rate 0.0075 step-time 0.03 perplexity 1140722150756.03 loss 27.76\n",
      "global step 67700 learning rate 0.0075 step-time 0.03 perplexity 15879976675044.28 loss 30.40\n",
      "global step 67800 learning rate 0.0074 step-time 0.03 perplexity 824869244987.00 loss 27.44\n",
      "global step 67900 learning rate 0.0074 step-time 0.03 perplexity 1808868180063.21 loss 28.22\n",
      "global step 68000 learning rate 0.0074 step-time 0.03 perplexity 2123229669446.78 loss 28.38\n",
      "global step 68100 learning rate 0.0073 step-time 0.03 perplexity 2323591738849.28 loss 28.47\n",
      "global step 68200 learning rate 0.0073 step-time 0.03 perplexity 2941368036649.99 loss 28.71\n",
      "global step 68300 learning rate 0.0072 step-time 0.03 perplexity 4981750014944.42 loss 29.24\n",
      "global step 68400 learning rate 0.0071 step-time 0.03 perplexity 33828096399.27 loss 24.24\n",
      "global step 68500 learning rate 0.0071 step-time 0.03 perplexity 106597193858.16 loss 25.39\n",
      "global step 68600 learning rate 0.0071 step-time 0.03 perplexity 105865612520.84 loss 25.39\n",
      "global step 68700 learning rate 0.0071 step-time 0.03 perplexity 379177227972.03 loss 26.66\n",
      "global step 68800 learning rate 0.0070 step-time 0.03 perplexity 792221095588.13 loss 27.40\n",
      "global step 68900 learning rate 0.0070 step-time 0.03 perplexity 600766487500.50 loss 27.12\n",
      "global step 69000 learning rate 0.0070 step-time 0.03 perplexity 2143706662330.15 loss 28.39\n",
      "global step 69100 learning rate 0.0069 step-time 0.03 perplexity 92968553518.53 loss 25.26\n",
      "global step 69200 learning rate 0.0069 step-time 0.03 perplexity 71618285879.21 loss 24.99\n",
      "global step 69300 learning rate 0.0069 step-time 0.03 perplexity 50967055905.72 loss 24.65\n",
      "global step 69400 learning rate 0.0069 step-time 0.03 perplexity 87651330185.80 loss 25.20\n",
      "global step 69500 learning rate 0.0068 step-time 0.03 perplexity 223191355149.14 loss 26.13\n",
      "global step 69600 learning rate 0.0068 step-time 0.03 perplexity 112696831491.55 loss 25.45\n",
      "global step 69700 learning rate 0.0068 step-time 0.03 perplexity 243917301512.01 loss 26.22\n",
      "global step 69800 learning rate 0.0067 step-time 0.03 perplexity 37868208995.55 loss 24.36\n",
      "global step 69900 learning rate 0.0067 step-time 0.03 perplexity 101575959578.75 loss 25.34\n",
      "global step 70000 learning rate 0.0067 step-time 0.03 perplexity 13650003015.21 loss 23.34\n",
      "global step 70100 learning rate 0.0067 step-time 0.03 perplexity 441276416022.51 loss 26.81\n",
      "global step 70200 learning rate 0.0066 step-time 0.03 perplexity 7056196267.15 loss 22.68\n",
      "global step 70300 learning rate 0.0066 step-time 0.03 perplexity 339865547232.13 loss 26.55\n",
      "global step 70400 learning rate 0.0066 step-time 0.03 perplexity 122962364638.18 loss 25.54\n",
      "global step 70500 learning rate 0.0066 step-time 0.03 perplexity 100047457872.47 loss 25.33\n",
      "global step 70600 learning rate 0.0066 step-time 0.03 perplexity 8701245686.24 loss 22.89\n",
      "global step 70700 learning rate 0.0066 step-time 0.03 perplexity 2802715207.58 loss 21.75\n",
      "global step 70800 learning rate 0.0066 step-time 0.03 perplexity 59537421080.59 loss 24.81\n",
      "global step 70900 learning rate 0.0066 step-time 0.03 perplexity 15219645006.78 loss 23.45\n",
      "global step 71000 learning rate 0.0066 step-time 0.03 perplexity 1820191076.73 loss 21.32\n",
      "global step 71100 learning rate 0.0066 step-time 0.03 perplexity 123228539302.55 loss 25.54\n",
      "global step 71200 learning rate 0.0065 step-time 0.03 perplexity 7793095102.97 loss 22.78\n",
      "global step 71300 learning rate 0.0065 step-time 0.03 perplexity 12119480754.74 loss 23.22\n",
      "global step 71400 learning rate 0.0065 step-time 0.03 perplexity 6647394538.48 loss 22.62\n",
      "global step 71500 learning rate 0.0065 step-time 0.03 perplexity 8496983167.50 loss 22.86\n",
      "global step 71600 learning rate 0.0065 step-time 0.03 perplexity 9317310793.75 loss 22.96\n",
      "global step 71700 learning rate 0.0064 step-time 0.03 perplexity 7099042030.08 loss 22.68\n",
      "global step 71800 learning rate 0.0064 step-time 0.03 perplexity 26851320369.94 loss 24.01\n",
      "global step 71900 learning rate 0.0064 step-time 0.03 perplexity 7499192829.70 loss 22.74\n",
      "global step 72000 learning rate 0.0064 step-time 0.03 perplexity 6347802565.91 loss 22.57\n",
      "global step 72100 learning rate 0.0064 step-time 0.03 perplexity 37993372594.90 loss 24.36\n",
      "global step 72200 learning rate 0.0063 step-time 0.03 perplexity 6831569229.41 loss 22.64\n",
      "global step 72300 learning rate 0.0063 step-time 0.03 perplexity 6570294005.04 loss 22.61\n",
      "global step 72400 learning rate 0.0063 step-time 0.03 perplexity 1017924633.31 loss 20.74\n",
      "global step 72500 learning rate 0.0063 step-time 0.03 perplexity 431337710.86 loss 19.88\n",
      "global step 72600 learning rate 0.0063 step-time 0.03 perplexity 1799571608.77 loss 21.31\n",
      "global step 72700 learning rate 0.0062 step-time 0.03 perplexity 2570704865.07 loss 21.67\n",
      "global step 72800 learning rate 0.0062 step-time 0.03 perplexity 2658024899.69 loss 21.70\n",
      "global step 72900 learning rate 0.0061 step-time 0.03 perplexity 823671043.58 loss 20.53\n",
      "global step 73000 learning rate 0.0061 step-time 0.03 perplexity 2179210864.37 loss 21.50\n",
      "global step 73100 learning rate 0.0061 step-time 0.03 perplexity 4216303969.27 loss 22.16\n",
      "global step 73200 learning rate 0.0061 step-time 0.03 perplexity 186228161.26 loss 19.04\n",
      "global step 73300 learning rate 0.0061 step-time 0.03 perplexity 910951836.52 loss 20.63\n",
      "global step 73400 learning rate 0.0061 step-time 0.03 perplexity 1171537037.19 loss 20.88\n",
      "global step 73500 learning rate 0.0060 step-time 0.03 perplexity 497452103.55 loss 20.03\n",
      "global step 73600 learning rate 0.0060 step-time 0.03 perplexity 1384291276.75 loss 21.05\n",
      "global step 73700 learning rate 0.0059 step-time 0.03 perplexity 503002934.79 loss 20.04\n",
      "global step 73800 learning rate 0.0059 step-time 0.03 perplexity 604265616.53 loss 20.22\n",
      "global step 73900 learning rate 0.0059 step-time 0.03 perplexity 302321448.59 loss 19.53\n",
      "global step 74000 learning rate 0.0059 step-time 0.03 perplexity 552089638.26 loss 20.13\n",
      "global step 74100 learning rate 0.0059 step-time 0.03 perplexity 1650316687.28 loss 21.22\n",
      "global step 74200 learning rate 0.0059 step-time 0.03 perplexity 647284699.23 loss 20.29\n",
      "global step 74300 learning rate 0.0059 step-time 0.03 perplexity 798133980.91 loss 20.50\n",
      "global step 74400 learning rate 0.0059 step-time 0.03 perplexity 1257727102.13 loss 20.95\n",
      "global step 74500 learning rate 0.0058 step-time 0.03 perplexity 2133621020.40 loss 21.48\n",
      "global step 74600 learning rate 0.0058 step-time 0.03 perplexity 267579465.86 loss 19.40\n",
      "global step 74700 learning rate 0.0058 step-time 0.03 perplexity 1909141299.77 loss 21.37\n",
      "global step 74800 learning rate 0.0058 step-time 0.03 perplexity 521069883.88 loss 20.07\n",
      "global step 74900 learning rate 0.0058 step-time 0.03 perplexity 450581930.50 loss 19.93\n",
      "global step 75000 learning rate 0.0058 step-time 0.03 perplexity 35595125836.44 loss 24.30\n",
      "global step 75100 learning rate 0.0057 step-time 0.03 perplexity 850101972.05 loss 20.56\n",
      "global step 75200 learning rate 0.0057 step-time 0.03 perplexity 1213094030.38 loss 20.92\n",
      "global step 75300 learning rate 0.0057 step-time 0.03 perplexity 1728994626.64 loss 21.27\n",
      "global step 75400 learning rate 0.0056 step-time 0.03 perplexity 230528160.16 loss 19.26\n",
      "global step 75500 learning rate 0.0056 step-time 0.03 perplexity 788159407.99 loss 20.49\n",
      "global step 75600 learning rate 0.0056 step-time 0.03 perplexity 309167115.11 loss 19.55\n",
      "global step 75700 learning rate 0.0056 step-time 0.03 perplexity 99205572.31 loss 18.41\n",
      "global step 75800 learning rate 0.0056 step-time 0.03 perplexity 4242953519.36 loss 22.17\n",
      "global step 75900 learning rate 0.0056 step-time 0.03 perplexity 295420719.31 loss 19.50\n",
      "global step 76000 learning rate 0.0056 step-time 0.03 perplexity 443043561.06 loss 19.91\n",
      "global step 76100 learning rate 0.0056 step-time 0.03 perplexity 1625033506.57 loss 21.21\n",
      "global step 76200 learning rate 0.0055 step-time 0.03 perplexity 178258339.99 loss 19.00\n",
      "global step 76300 learning rate 0.0055 step-time 0.03 perplexity 1798174811.48 loss 21.31\n",
      "global step 76400 learning rate 0.0055 step-time 0.03 perplexity 135401424.30 loss 18.72\n",
      "global step 76500 learning rate 0.0055 step-time 0.03 perplexity 205060606.62 loss 19.14\n",
      "global step 76600 learning rate 0.0055 step-time 0.03 perplexity 1455122862.26 loss 21.10\n",
      "global step 76700 learning rate 0.0054 step-time 0.03 perplexity 1818645523.95 loss 21.32\n",
      "global step 76800 learning rate 0.0054 step-time 0.03 perplexity 293353857.87 loss 19.50\n",
      "global step 76900 learning rate 0.0054 step-time 0.03 perplexity 250874573.08 loss 19.34\n",
      "global step 77000 learning rate 0.0054 step-time 0.03 perplexity 48407550.95 loss 17.70\n",
      "global step 77100 learning rate 0.0054 step-time 0.03 perplexity 196782703.29 loss 19.10\n",
      "global step 77200 learning rate 0.0054 step-time 0.03 perplexity 503122281.76 loss 20.04\n",
      "global step 77300 learning rate 0.0053 step-time 0.03 perplexity 24546449.82 loss 17.02\n",
      "global step 77400 learning rate 0.0053 step-time 0.03 perplexity 76065529.23 loss 18.15\n",
      "global step 77500 learning rate 0.0053 step-time 0.03 perplexity 763310155.30 loss 20.45\n",
      "global step 77600 learning rate 0.0053 step-time 0.03 perplexity 255713222.44 loss 19.36\n",
      "global step 77700 learning rate 0.0053 step-time 0.03 perplexity 1143180324.14 loss 20.86\n",
      "global step 77800 learning rate 0.0052 step-time 0.03 perplexity 315005733.73 loss 19.57\n",
      "global step 77900 learning rate 0.0052 step-time 0.03 perplexity 153929122.21 loss 18.85\n",
      "global step 78000 learning rate 0.0052 step-time 0.03 perplexity 379958088.81 loss 19.76\n",
      "global step 78100 learning rate 0.0052 step-time 0.03 perplexity 374389127.36 loss 19.74\n",
      "global step 78200 learning rate 0.0052 step-time 0.03 perplexity 235585661.04 loss 19.28\n",
      "global step 78300 learning rate 0.0052 step-time 0.03 perplexity 681673688.57 loss 20.34\n",
      "global step 78400 learning rate 0.0051 step-time 0.03 perplexity 193645309.17 loss 19.08\n",
      "global step 78500 learning rate 0.0051 step-time 0.03 perplexity 231387919.65 loss 19.26\n",
      "global step 78600 learning rate 0.0051 step-time 0.03 perplexity 65550414.56 loss 18.00\n",
      "global step 78700 learning rate 0.0051 step-time 0.03 perplexity 36405853.95 loss 17.41\n",
      "global step 78800 learning rate 0.0051 step-time 0.03 perplexity 96768968.27 loss 18.39\n",
      "global step 78900 learning rate 0.0051 step-time 0.03 perplexity 21516789.91 loss 16.88\n",
      "global step 79000 learning rate 0.0051 step-time 0.03 perplexity 24865062.55 loss 17.03\n",
      "global step 79100 learning rate 0.0051 step-time 0.03 perplexity 47464689.88 loss 17.68\n",
      "global step 79200 learning rate 0.0050 step-time 0.03 perplexity 43270794.98 loss 17.58\n",
      "global step 79300 learning rate 0.0050 step-time 0.03 perplexity 24352491.31 loss 17.01\n",
      "global step 79400 learning rate 0.0050 step-time 0.03 perplexity 48652581.85 loss 17.70\n",
      "global step 79500 learning rate 0.0050 step-time 0.03 perplexity 21059047.08 loss 16.86\n",
      "global step 79600 learning rate 0.0050 step-time 0.03 perplexity 71353670.50 loss 18.08\n",
      "global step 79700 learning rate 0.0049 step-time 0.03 perplexity 10109891.83 loss 16.13\n",
      "global step 79800 learning rate 0.0049 step-time 0.03 perplexity 36511689.67 loss 17.41\n",
      "global step 79900 learning rate 0.0049 step-time 0.03 perplexity 31713804.82 loss 17.27\n",
      "global step 80000 learning rate 0.0049 step-time 0.03 perplexity 110986250.80 loss 18.52\n",
      "global step 80100 learning rate 0.0049 step-time 0.03 perplexity 41226997.06 loss 17.53\n",
      "global step 80200 learning rate 0.0049 step-time 0.03 perplexity 65107844.26 loss 17.99\n",
      "global step 80300 learning rate 0.0049 step-time 0.03 perplexity 16002066.19 loss 16.59\n",
      "global step 80400 learning rate 0.0049 step-time 0.03 perplexity 12542253.38 loss 16.34\n",
      "global step 80500 learning rate 0.0049 step-time 0.03 perplexity 64958589.33 loss 17.99\n",
      "global step 80600 learning rate 0.0048 step-time 0.03 perplexity 20615638.62 loss 16.84\n",
      "global step 80700 learning rate 0.0048 step-time 0.03 perplexity 9925953.34 loss 16.11\n",
      "global step 80800 learning rate 0.0048 step-time 0.03 perplexity 41020699.54 loss 17.53\n",
      "global step 80900 learning rate 0.0048 step-time 0.03 perplexity 56893943.37 loss 17.86\n",
      "global step 81000 learning rate 0.0047 step-time 0.03 perplexity 10036402.19 loss 16.12\n",
      "global step 81100 learning rate 0.0047 step-time 0.03 perplexity 36505346.46 loss 17.41\n",
      "global step 81200 learning rate 0.0047 step-time 0.03 perplexity 50059540.10 loss 17.73\n",
      "global step 81300 learning rate 0.0047 step-time 0.03 perplexity 16149782.19 loss 16.60\n",
      "global step 81400 learning rate 0.0047 step-time 0.03 perplexity 5512184.44 loss 15.52\n",
      "global step 81500 learning rate 0.0047 step-time 0.03 perplexity 13303422.17 loss 16.40\n",
      "global step 81600 learning rate 0.0047 step-time 0.03 perplexity 72085564.52 loss 18.09\n",
      "global step 81700 learning rate 0.0046 step-time 0.03 perplexity 53063221.13 loss 17.79\n",
      "global step 81800 learning rate 0.0046 step-time 0.03 perplexity 116061145.17 loss 18.57\n",
      "global step 81900 learning rate 0.0046 step-time 0.03 perplexity 15174805.36 loss 16.54\n",
      "global step 82000 learning rate 0.0046 step-time 0.03 perplexity 30586767.41 loss 17.24\n",
      "global step 82100 learning rate 0.0046 step-time 0.03 perplexity 103072851.11 loss 18.45\n",
      "global step 82200 learning rate 0.0045 step-time 0.03 perplexity 13094698.51 loss 16.39\n",
      "global step 82300 learning rate 0.0045 step-time 0.03 perplexity 10338185.63 loss 16.15\n",
      "global step 82400 learning rate 0.0045 step-time 0.03 perplexity 14616274.20 loss 16.50\n",
      "global step 82500 learning rate 0.0045 step-time 0.03 perplexity 4454513.91 loss 15.31\n",
      "global step 82600 learning rate 0.0045 step-time 0.03 perplexity 16439174.32 loss 16.62\n",
      "global step 82700 learning rate 0.0044 step-time 0.03 perplexity 1970670.15 loss 14.49\n",
      "global step 82800 learning rate 0.0044 step-time 0.03 perplexity 32046302.65 loss 17.28\n",
      "global step 82900 learning rate 0.0044 step-time 0.03 perplexity 12645533.49 loss 16.35\n",
      "global step 83000 learning rate 0.0044 step-time 0.03 perplexity 18913854.47 loss 16.76\n",
      "global step 83100 learning rate 0.0044 step-time 0.03 perplexity 9236605.68 loss 16.04\n",
      "global step 83200 learning rate 0.0044 step-time 0.03 perplexity 16535739.80 loss 16.62\n",
      "global step 83300 learning rate 0.0044 step-time 0.03 perplexity 23457604.44 loss 16.97\n",
      "global step 83400 learning rate 0.0043 step-time 0.03 perplexity 6856490.15 loss 15.74\n",
      "global step 83500 learning rate 0.0043 step-time 0.03 perplexity 7141616.67 loss 15.78\n",
      "global step 83600 learning rate 0.0043 step-time 0.03 perplexity 25062707.97 loss 17.04\n",
      "global step 83700 learning rate 0.0043 step-time 0.03 perplexity 8403216.52 loss 15.94\n",
      "global step 83800 learning rate 0.0043 step-time 0.03 perplexity 21725310.38 loss 16.89\n",
      "global step 83900 learning rate 0.0043 step-time 0.03 perplexity 2890978.40 loss 14.88\n",
      "global step 84000 learning rate 0.0043 step-time 0.03 perplexity 2866551.55 loss 14.87\n",
      "global step 84100 learning rate 0.0043 step-time 0.03 perplexity 26464557.35 loss 17.09\n",
      "global step 84200 learning rate 0.0043 step-time 0.03 perplexity 8255417.03 loss 15.93\n",
      "global step 84300 learning rate 0.0043 step-time 0.03 perplexity 12802677.06 loss 16.37\n",
      "global step 84400 learning rate 0.0043 step-time 0.03 perplexity 4150131.78 loss 15.24\n",
      "global step 84500 learning rate 0.0043 step-time 0.03 perplexity 1977579.03 loss 14.50\n",
      "global step 84600 learning rate 0.0043 step-time 0.03 perplexity 6278370.80 loss 15.65\n",
      "global step 84700 learning rate 0.0042 step-time 0.03 perplexity 2016050.65 loss 14.52\n",
      "global step 84800 learning rate 0.0042 step-time 0.03 perplexity 15240818.37 loss 16.54\n",
      "global step 84900 learning rate 0.0042 step-time 0.03 perplexity 12327776.83 loss 16.33\n",
      "global step 85000 learning rate 0.0042 step-time 0.03 perplexity 2972322.92 loss 14.90\n",
      "global step 85100 learning rate 0.0042 step-time 0.03 perplexity 7891219.93 loss 15.88\n",
      "global step 85200 learning rate 0.0042 step-time 0.03 perplexity 1399860.23 loss 14.15\n",
      "global step 85300 learning rate 0.0042 step-time 0.03 perplexity 17800910.48 loss 16.69\n",
      "global step 85400 learning rate 0.0041 step-time 0.03 perplexity 8783709.17 loss 15.99\n",
      "global step 85500 learning rate 0.0041 step-time 0.03 perplexity 1678019.78 loss 14.33\n",
      "global step 85600 learning rate 0.0041 step-time 0.03 perplexity 1208151.73 loss 14.00\n",
      "global step 85700 learning rate 0.0041 step-time 0.03 perplexity 10760775.01 loss 16.19\n",
      "global step 85800 learning rate 0.0041 step-time 0.03 perplexity 2392038.19 loss 14.69\n",
      "global step 85900 learning rate 0.0041 step-time 0.03 perplexity 13698513.29 loss 16.43\n",
      "global step 86000 learning rate 0.0041 step-time 0.03 perplexity 1235654.27 loss 14.03\n",
      "global step 86100 learning rate 0.0041 step-time 0.03 perplexity 3860757.91 loss 15.17\n",
      "global step 86200 learning rate 0.0041 step-time 0.03 perplexity 3745156.60 loss 15.14\n",
      "global step 86300 learning rate 0.0041 step-time 0.03 perplexity 4525987.47 loss 15.33\n",
      "global step 86400 learning rate 0.0040 step-time 0.03 perplexity 460792.77 loss 13.04\n",
      "global step 86500 learning rate 0.0040 step-time 0.03 perplexity 539957.33 loss 13.20\n",
      "global step 86600 learning rate 0.0040 step-time 0.03 perplexity 774371.30 loss 13.56\n",
      "global step 86700 learning rate 0.0040 step-time 0.03 perplexity 1778560.75 loss 14.39\n",
      "global step 86800 learning rate 0.0039 step-time 0.04 perplexity 2938009.22 loss 14.89\n",
      "global step 86900 learning rate 0.0039 step-time 0.03 perplexity 771345.88 loss 13.56\n",
      "global step 87000 learning rate 0.0039 step-time 0.03 perplexity 784317.69 loss 13.57\n",
      "global step 87100 learning rate 0.0039 step-time 0.03 perplexity 2148463.59 loss 14.58\n",
      "global step 87200 learning rate 0.0039 step-time 0.03 perplexity 2316230.31 loss 14.66\n",
      "global step 87300 learning rate 0.0038 step-time 0.03 perplexity 1436130.36 loss 14.18\n",
      "global step 87400 learning rate 0.0038 step-time 0.03 perplexity 405854.47 loss 12.91\n",
      "global step 87500 learning rate 0.0038 step-time 0.03 perplexity 73310.02 loss 11.20\n",
      "global step 87600 learning rate 0.0038 step-time 0.03 perplexity 832623.56 loss 13.63\n",
      "global step 87700 learning rate 0.0038 step-time 0.04 perplexity 699402.04 loss 13.46\n",
      "global step 87800 learning rate 0.0038 step-time 0.04 perplexity 497789.79 loss 13.12\n",
      "global step 87900 learning rate 0.0038 step-time 0.04 perplexity 2750085.80 loss 14.83\n",
      "global step 88000 learning rate 0.0037 step-time 0.04 perplexity 804362.85 loss 13.60\n",
      "global step 88100 learning rate 0.0037 step-time 0.04 perplexity 320069.00 loss 12.68\n",
      "global step 88200 learning rate 0.0037 step-time 0.03 perplexity 639744.87 loss 13.37\n",
      "global step 88300 learning rate 0.0037 step-time 0.03 perplexity 353092.94 loss 12.77\n",
      "global step 88400 learning rate 0.0037 step-time 0.04 perplexity 234884.65 loss 12.37\n",
      "global step 88500 learning rate 0.0037 step-time 0.04 perplexity 413780.89 loss 12.93\n",
      "global step 88600 learning rate 0.0037 step-time 0.03 perplexity 1451980.32 loss 14.19\n",
      "global step 88700 learning rate 0.0037 step-time 0.03 perplexity 553324.11 loss 13.22\n",
      "global step 88800 learning rate 0.0037 step-time 0.03 perplexity 942926.76 loss 13.76\n",
      "global step 88900 learning rate 0.0037 step-time 0.03 perplexity 430346.58 loss 12.97\n",
      "global step 89000 learning rate 0.0037 step-time 0.04 perplexity 679837.25 loss 13.43\n",
      "global step 89100 learning rate 0.0037 step-time 0.04 perplexity 441437.13 loss 13.00\n",
      "global step 89200 learning rate 0.0037 step-time 0.03 perplexity 271241.27 loss 12.51\n",
      "global step 89300 learning rate 0.0037 step-time 0.03 perplexity 268705.59 loss 12.50\n",
      "global step 89400 learning rate 0.0037 step-time 0.03 perplexity 800459.64 loss 13.59\n",
      "global step 89500 learning rate 0.0036 step-time 0.03 perplexity 244038.35 loss 12.41\n",
      "global step 89600 learning rate 0.0036 step-time 0.03 perplexity 440363.13 loss 13.00\n",
      "global step 89700 learning rate 0.0036 step-time 0.03 perplexity 518349.35 loss 13.16\n",
      "global step 89800 learning rate 0.0036 step-time 0.03 perplexity 795477.43 loss 13.59\n",
      "global step 89900 learning rate 0.0036 step-time 0.03 perplexity 212998.09 loss 12.27\n",
      "global step 90000 learning rate 0.0036 step-time 0.03 perplexity 269153.22 loss 12.50\n",
      "global step 90100 learning rate 0.0036 step-time 0.03 perplexity 296540.05 loss 12.60\n",
      "global step 90200 learning rate 0.0035 step-time 0.03 perplexity 752663.60 loss 13.53\n",
      "global step 90300 learning rate 0.0035 step-time 0.03 perplexity 217821.59 loss 12.29\n",
      "global step 90400 learning rate 0.0035 step-time 0.03 perplexity 177347.41 loss 12.09\n",
      "global step 90500 learning rate 0.0035 step-time 0.03 perplexity 422829.32 loss 12.95\n",
      "global step 90600 learning rate 0.0035 step-time 0.03 perplexity 200956.63 loss 12.21\n",
      "global step 90700 learning rate 0.0035 step-time 0.03 perplexity 650023.87 loss 13.38\n",
      "global step 90800 learning rate 0.0034 step-time 0.03 perplexity 1025787.63 loss 13.84\n",
      "global step 90900 learning rate 0.0034 step-time 0.04 perplexity 141094.49 loss 11.86\n",
      "global step 91000 learning rate 0.0034 step-time 0.03 perplexity 229974.51 loss 12.35\n",
      "global step 91100 learning rate 0.0034 step-time 0.03 perplexity 425077.90 loss 12.96\n",
      "global step 91200 learning rate 0.0033 step-time 0.03 perplexity 178497.17 loss 12.09\n",
      "global step 91300 learning rate 0.0033 step-time 0.03 perplexity 583303.42 loss 13.28\n",
      "global step 91400 learning rate 0.0033 step-time 0.03 perplexity 188606.99 loss 12.15\n",
      "global step 91500 learning rate 0.0033 step-time 0.03 perplexity 501957.15 loss 13.13\n",
      "global step 91600 learning rate 0.0033 step-time 0.03 perplexity 258099.37 loss 12.46\n",
      "global step 91700 learning rate 0.0033 step-time 0.03 perplexity 332208.88 loss 12.71\n",
      "global step 91800 learning rate 0.0033 step-time 0.03 perplexity 222281.73 loss 12.31\n",
      "global step 91900 learning rate 0.0033 step-time 0.03 perplexity 97781.72 loss 11.49\n",
      "global step 92000 learning rate 0.0033 step-time 0.03 perplexity 107701.26 loss 11.59\n",
      "global step 92100 learning rate 0.0033 step-time 0.03 perplexity 149174.58 loss 11.91\n",
      "global step 92200 learning rate 0.0033 step-time 0.03 perplexity 236401.30 loss 12.37\n",
      "global step 92300 learning rate 0.0032 step-time 0.03 perplexity 567463.41 loss 13.25\n",
      "global step 92400 learning rate 0.0032 step-time 0.03 perplexity 63586.63 loss 11.06\n",
      "global step 92500 learning rate 0.0032 step-time 0.03 perplexity 104285.32 loss 11.55\n",
      "global step 92600 learning rate 0.0032 step-time 0.03 perplexity 152362.80 loss 11.93\n",
      "global step 92700 learning rate 0.0032 step-time 0.03 perplexity 169335.99 loss 12.04\n",
      "global step 92800 learning rate 0.0032 step-time 0.03 perplexity 233433.70 loss 12.36\n",
      "global step 92900 learning rate 0.0031 step-time 0.03 perplexity 41918.51 loss 10.64\n",
      "global step 93000 learning rate 0.0031 step-time 0.03 perplexity 54628.63 loss 10.91\n",
      "global step 93100 learning rate 0.0031 step-time 0.03 perplexity 53453.12 loss 10.89\n",
      "global step 93200 learning rate 0.0031 step-time 0.03 perplexity 72086.63 loss 11.19\n",
      "global step 93300 learning rate 0.0031 step-time 0.03 perplexity 232159.51 loss 12.36\n",
      "global step 93400 learning rate 0.0031 step-time 0.03 perplexity 55827.83 loss 10.93\n",
      "global step 93500 learning rate 0.0031 step-time 0.03 perplexity 83772.63 loss 11.34\n",
      "global step 93600 learning rate 0.0031 step-time 0.03 perplexity 137637.74 loss 11.83\n",
      "global step 93700 learning rate 0.0030 step-time 0.03 perplexity 241292.34 loss 12.39\n",
      "global step 93800 learning rate 0.0030 step-time 0.03 perplexity 183051.53 loss 12.12\n",
      "global step 93900 learning rate 0.0030 step-time 0.03 perplexity 66658.34 loss 11.11\n",
      "global step 94000 learning rate 0.0030 step-time 0.03 perplexity 48561.73 loss 10.79\n",
      "global step 94100 learning rate 0.0030 step-time 0.03 perplexity 116347.61 loss 11.66\n",
      "global step 94200 learning rate 0.0030 step-time 0.03 perplexity 46775.73 loss 10.75\n",
      "global step 94300 learning rate 0.0030 step-time 0.03 perplexity 79115.53 loss 11.28\n",
      "global step 94400 learning rate 0.0030 step-time 0.03 perplexity 56387.94 loss 10.94\n",
      "global step 94500 learning rate 0.0030 step-time 0.03 perplexity 138854.91 loss 11.84\n",
      "global step 94600 learning rate 0.0029 step-time 0.03 perplexity 83718.68 loss 11.34\n",
      "global step 94700 learning rate 0.0029 step-time 0.03 perplexity 195232.27 loss 12.18\n",
      "global step 94800 learning rate 0.0029 step-time 0.03 perplexity 42219.14 loss 10.65\n",
      "global step 94900 learning rate 0.0029 step-time 0.03 perplexity 40939.14 loss 10.62\n",
      "global step 95000 learning rate 0.0029 step-time 0.03 perplexity 69055.19 loss 11.14\n",
      "global step 95100 learning rate 0.0029 step-time 0.03 perplexity 23573.79 loss 10.07\n",
      "global step 95200 learning rate 0.0029 step-time 0.03 perplexity 120233.09 loss 11.70\n",
      "global step 95300 learning rate 0.0029 step-time 0.03 perplexity 69768.58 loss 11.15\n",
      "global step 95400 learning rate 0.0029 step-time 0.03 perplexity 82926.66 loss 11.33\n",
      "global step 95500 learning rate 0.0029 step-time 0.03 perplexity 21484.37 loss 9.98\n",
      "global step 95600 learning rate 0.0029 step-time 0.03 perplexity 29443.00 loss 10.29\n",
      "global step 95700 learning rate 0.0029 step-time 0.03 perplexity 23969.44 loss 10.08\n",
      "global step 95800 learning rate 0.0029 step-time 0.03 perplexity 27683.34 loss 10.23\n",
      "global step 95900 learning rate 0.0029 step-time 0.03 perplexity 48958.75 loss 10.80\n",
      "global step 96000 learning rate 0.0028 step-time 0.04 perplexity 58732.24 loss 10.98\n",
      "global step 96100 learning rate 0.0028 step-time 0.04 perplexity 20267.20 loss 9.92\n",
      "global step 96200 learning rate 0.0028 step-time 0.03 perplexity 94396.66 loss 11.46\n",
      "global step 96300 learning rate 0.0028 step-time 0.03 perplexity 48483.57 loss 10.79\n",
      "global step 96400 learning rate 0.0028 step-time 0.03 perplexity 82878.16 loss 11.33\n",
      "global step 96500 learning rate 0.0028 step-time 0.03 perplexity 25476.86 loss 10.15\n",
      "global step 96600 learning rate 0.0028 step-time 0.03 perplexity 88407.55 loss 11.39\n",
      "global step 96700 learning rate 0.0027 step-time 0.03 perplexity 32214.81 loss 10.38\n",
      "global step 96800 learning rate 0.0027 step-time 0.03 perplexity 51713.33 loss 10.85\n",
      "global step 96900 learning rate 0.0027 step-time 0.03 perplexity 47344.54 loss 10.77\n",
      "global step 97000 learning rate 0.0027 step-time 0.03 perplexity 35061.45 loss 10.46\n",
      "global step 97100 learning rate 0.0027 step-time 0.03 perplexity 25248.25 loss 10.14\n",
      "global step 97200 learning rate 0.0027 step-time 0.03 perplexity 27447.64 loss 10.22\n",
      "global step 97300 learning rate 0.0027 step-time 0.03 perplexity 16310.34 loss 9.70\n",
      "global step 97400 learning rate 0.0027 step-time 0.03 perplexity 47767.06 loss 10.77\n",
      "global step 97500 learning rate 0.0027 step-time 0.03 perplexity 72876.81 loss 11.20\n",
      "global step 97600 learning rate 0.0027 step-time 0.03 perplexity 11505.82 loss 9.35\n",
      "global step 97700 learning rate 0.0027 step-time 0.03 perplexity 39903.86 loss 10.59\n",
      "global step 97800 learning rate 0.0027 step-time 0.03 perplexity 49477.89 loss 10.81\n",
      "global step 97900 learning rate 0.0027 step-time 0.03 perplexity 19164.98 loss 9.86\n",
      "global step 98000 learning rate 0.0027 step-time 0.03 perplexity 30273.15 loss 10.32\n",
      "global step 98100 learning rate 0.0027 step-time 0.03 perplexity 19476.58 loss 9.88\n",
      "global step 98200 learning rate 0.0027 step-time 0.03 perplexity 28805.20 loss 10.27\n",
      "global step 98300 learning rate 0.0027 step-time 0.03 perplexity 11655.49 loss 9.36\n",
      "global step 98400 learning rate 0.0027 step-time 0.03 perplexity 17937.88 loss 9.79\n",
      "global step 98500 learning rate 0.0027 step-time 0.03 perplexity 9575.60 loss 9.17\n",
      "global step 98600 learning rate 0.0027 step-time 0.03 perplexity 21139.56 loss 9.96\n",
      "global step 98700 learning rate 0.0026 step-time 0.04 perplexity 48040.42 loss 10.78\n",
      "global step 98800 learning rate 0.0026 step-time 0.03 perplexity 15771.61 loss 9.67\n",
      "global step 98900 learning rate 0.0026 step-time 0.03 perplexity 46668.93 loss 10.75\n",
      "global step 99000 learning rate 0.0026 step-time 0.03 perplexity 28112.65 loss 10.24\n",
      "global step 99100 learning rate 0.0026 step-time 0.03 perplexity 25741.34 loss 10.16\n",
      "global step 99200 learning rate 0.0026 step-time 0.03 perplexity 7319.15 loss 8.90\n",
      "global step 99300 learning rate 0.0026 step-time 0.03 perplexity 15319.86 loss 9.64\n",
      "global step 99400 learning rate 0.0026 step-time 0.03 perplexity 42014.90 loss 10.65\n",
      "global step 99500 learning rate 0.0026 step-time 0.03 perplexity 20819.11 loss 9.94\n",
      "global step 99600 learning rate 0.0026 step-time 0.03 perplexity 17636.22 loss 9.78\n",
      "global step 99700 learning rate 0.0026 step-time 0.03 perplexity 62896.51 loss 11.05\n",
      "global step 99800 learning rate 0.0026 step-time 0.03 perplexity 28715.44 loss 10.27\n",
      "global step 99900 learning rate 0.0026 step-time 0.03 perplexity 32042.22 loss 10.37\n",
      "global step 100000 learning rate 0.0026 step-time 0.03 perplexity 9326.39 loss 9.14\n",
      "global step 100100 learning rate 0.0026 step-time 0.03 perplexity 15686.56 loss 9.66\n",
      "global step 100200 learning rate 0.0026 step-time 0.03 perplexity 32603.29 loss 10.39\n",
      "global step 100300 learning rate 0.0025 step-time 0.03 perplexity 17536.80 loss 9.77\n",
      "global step 100400 learning rate 0.0025 step-time 0.03 perplexity 31461.05 loss 10.36\n",
      "global step 100500 learning rate 0.0025 step-time 0.03 perplexity 16568.37 loss 9.72\n",
      "global step 100600 learning rate 0.0025 step-time 0.03 perplexity 67919.44 loss 11.13\n",
      "global step 100700 learning rate 0.0025 step-time 0.04 perplexity 10546.52 loss 9.26\n",
      "global step 100800 learning rate 0.0025 step-time 0.04 perplexity 17443.65 loss 9.77\n",
      "global step 100900 learning rate 0.0025 step-time 0.03 perplexity 19967.47 loss 9.90\n",
      "global step 101000 learning rate 0.0025 step-time 0.03 perplexity 27387.21 loss 10.22\n",
      "global step 101100 learning rate 0.0025 step-time 0.03 perplexity 17109.60 loss 9.75\n",
      "global step 101200 learning rate 0.0025 step-time 0.03 perplexity 44188.95 loss 10.70\n",
      "global step 101300 learning rate 0.0024 step-time 0.03 perplexity 11612.23 loss 9.36\n",
      "global step 101400 learning rate 0.0024 step-time 0.03 perplexity 11109.77 loss 9.32\n",
      "global step 101500 learning rate 0.0024 step-time 0.04 perplexity 7592.66 loss 8.93\n",
      "global step 101600 learning rate 0.0024 step-time 0.04 perplexity 10503.22 loss 9.26\n",
      "global step 101700 learning rate 0.0024 step-time 0.04 perplexity 28190.97 loss 10.25\n",
      "global step 101800 learning rate 0.0024 step-time 0.04 perplexity 37089.24 loss 10.52\n",
      "global step 101900 learning rate 0.0024 step-time 0.03 perplexity 31304.64 loss 10.35\n",
      "global step 102000 learning rate 0.0024 step-time 0.03 perplexity 11687.93 loss 9.37\n",
      "global step 102100 learning rate 0.0024 step-time 0.04 perplexity 10985.93 loss 9.30\n",
      "global step 102200 learning rate 0.0024 step-time 0.03 perplexity 7202.84 loss 8.88\n",
      "global step 102300 learning rate 0.0024 step-time 0.03 perplexity 6555.00 loss 8.79\n",
      "global step 102400 learning rate 0.0024 step-time 0.03 perplexity 15602.43 loss 9.66\n",
      "global step 102500 learning rate 0.0024 step-time 0.03 perplexity 10933.09 loss 9.30\n",
      "global step 102600 learning rate 0.0024 step-time 0.03 perplexity 4517.72 loss 8.42\n",
      "global step 102700 learning rate 0.0024 step-time 0.03 perplexity 7818.69 loss 8.96\n",
      "global step 102800 learning rate 0.0024 step-time 0.03 perplexity 16113.11 loss 9.69\n",
      "global step 102900 learning rate 0.0023 step-time 0.03 perplexity 10029.62 loss 9.21\n",
      "global step 103000 learning rate 0.0023 step-time 0.03 perplexity 8178.61 loss 9.01\n",
      "global step 103100 learning rate 0.0023 step-time 0.03 perplexity 10549.13 loss 9.26\n",
      "global step 103200 learning rate 0.0023 step-time 0.03 perplexity 7263.12 loss 8.89\n",
      "global step 103300 learning rate 0.0023 step-time 0.03 perplexity 8142.06 loss 9.00\n",
      "global step 103400 learning rate 0.0023 step-time 0.03 perplexity 6546.32 loss 8.79\n",
      "global step 103500 learning rate 0.0023 step-time 0.03 perplexity 3165.17 loss 8.06\n",
      "global step 103600 learning rate 0.0023 step-time 0.03 perplexity 14296.53 loss 9.57\n",
      "global step 103700 learning rate 0.0023 step-time 0.03 perplexity 14865.65 loss 9.61\n",
      "global step 103800 learning rate 0.0023 step-time 0.03 perplexity 6312.93 loss 8.75\n",
      "global step 103900 learning rate 0.0023 step-time 0.03 perplexity 4401.95 loss 8.39\n",
      "global step 104000 learning rate 0.0023 step-time 0.03 perplexity 8559.18 loss 9.05\n",
      "global step 104100 learning rate 0.0022 step-time 0.03 perplexity 5074.11 loss 8.53\n",
      "global step 104200 learning rate 0.0022 step-time 0.03 perplexity 7310.62 loss 8.90\n",
      "global step 104300 learning rate 0.0022 step-time 0.04 perplexity 3579.82 loss 8.18\n",
      "global step 104400 learning rate 0.0022 step-time 0.04 perplexity 9378.46 loss 9.15\n",
      "global step 104500 learning rate 0.0022 step-time 0.03 perplexity 12462.48 loss 9.43\n",
      "global step 104600 learning rate 0.0022 step-time 0.03 perplexity 3603.55 loss 8.19\n",
      "global step 104700 learning rate 0.0022 step-time 0.04 perplexity 9227.54 loss 9.13\n",
      "global step 104800 learning rate 0.0022 step-time 0.03 perplexity 2986.97 loss 8.00\n",
      "global step 104900 learning rate 0.0022 step-time 0.03 perplexity 4123.02 loss 8.32\n",
      "global step 105000 learning rate 0.0022 step-time 0.04 perplexity 4059.76 loss 8.31\n",
      "global step 105100 learning rate 0.0022 step-time 0.03 perplexity 2336.86 loss 7.76\n",
      "global step 105200 learning rate 0.0022 step-time 0.03 perplexity 4909.10 loss 8.50\n",
      "global step 105300 learning rate 0.0022 step-time 0.03 perplexity 6181.88 loss 8.73\n",
      "global step 105400 learning rate 0.0022 step-time 0.03 perplexity 4533.47 loss 8.42\n",
      "global step 105500 learning rate 0.0022 step-time 0.03 perplexity 2022.26 loss 7.61\n",
      "global step 105600 learning rate 0.0022 step-time 0.04 perplexity 7407.38 loss 8.91\n",
      "global step 105700 learning rate 0.0021 step-time 0.03 perplexity 3399.46 loss 8.13\n",
      "global step 105800 learning rate 0.0021 step-time 0.03 perplexity 2298.93 loss 7.74\n",
      "global step 105900 learning rate 0.0021 step-time 0.03 perplexity 6769.19 loss 8.82\n",
      "global step 106000 learning rate 0.0021 step-time 0.03 perplexity 2739.19 loss 7.92\n",
      "global step 106100 learning rate 0.0021 step-time 0.03 perplexity 6164.19 loss 8.73\n",
      "global step 106200 learning rate 0.0021 step-time 0.03 perplexity 4397.34 loss 8.39\n",
      "global step 106300 learning rate 0.0021 step-time 0.03 perplexity 6156.81 loss 8.73\n",
      "global step 106400 learning rate 0.0021 step-time 0.04 perplexity 4636.53 loss 8.44\n",
      "global step 106500 learning rate 0.0021 step-time 0.03 perplexity 3257.50 loss 8.09\n",
      "global step 106600 learning rate 0.0021 step-time 0.04 perplexity 6293.91 loss 8.75\n",
      "global step 106700 learning rate 0.0021 step-time 0.03 perplexity 10027.69 loss 9.21\n",
      "global step 106800 learning rate 0.0021 step-time 0.04 perplexity 8230.43 loss 9.02\n",
      "global step 106900 learning rate 0.0021 step-time 0.03 perplexity 9520.29 loss 9.16\n",
      "global step 107000 learning rate 0.0021 step-time 0.04 perplexity 4091.94 loss 8.32\n",
      "global step 107100 learning rate 0.0021 step-time 0.03 perplexity 2384.57 loss 7.78\n",
      "global step 107200 learning rate 0.0021 step-time 0.04 perplexity 10427.41 loss 9.25\n",
      "global step 107300 learning rate 0.0020 step-time 0.04 perplexity 12287.11 loss 9.42\n",
      "global step 107400 learning rate 0.0020 step-time 0.04 perplexity 2070.10 loss 7.64\n",
      "global step 107500 learning rate 0.0020 step-time 0.03 perplexity 2221.24 loss 7.71\n",
      "global step 107600 learning rate 0.0020 step-time 0.03 perplexity 1499.85 loss 7.31\n",
      "global step 107700 learning rate 0.0020 step-time 0.03 perplexity 3244.76 loss 8.08\n",
      "global step 107800 learning rate 0.0020 step-time 0.03 perplexity 3488.44 loss 8.16\n",
      "global step 107900 learning rate 0.0020 step-time 0.03 perplexity 4435.83 loss 8.40\n",
      "global step 108000 learning rate 0.0020 step-time 0.03 perplexity 2121.63 loss 7.66\n",
      "global step 108100 learning rate 0.0020 step-time 0.04 perplexity 1793.46 loss 7.49\n",
      "global step 108200 learning rate 0.0020 step-time 0.04 perplexity 4178.54 loss 8.34\n",
      "global step 108300 learning rate 0.0019 step-time 0.04 perplexity 3815.85 loss 8.25\n",
      "global step 108400 learning rate 0.0019 step-time 0.03 perplexity 3036.00 loss 8.02\n",
      "global step 108500 learning rate 0.0019 step-time 0.04 perplexity 3053.59 loss 8.02\n",
      "global step 108600 learning rate 0.0019 step-time 0.03 perplexity 1352.40 loss 7.21\n",
      "global step 108700 learning rate 0.0019 step-time 0.04 perplexity 2521.48 loss 7.83\n",
      "global step 108800 learning rate 0.0019 step-time 0.03 perplexity 3543.78 loss 8.17\n",
      "global step 108900 learning rate 0.0019 step-time 0.03 perplexity 3946.21 loss 8.28\n",
      "global step 109000 learning rate 0.0019 step-time 0.04 perplexity 3566.02 loss 8.18\n",
      "global step 109100 learning rate 0.0019 step-time 0.03 perplexity 3939.77 loss 8.28\n",
      "global step 109200 learning rate 0.0019 step-time 0.03 perplexity 1387.39 loss 7.24\n",
      "global step 109300 learning rate 0.0019 step-time 0.03 perplexity 6947.00 loss 8.85\n",
      "global step 109400 learning rate 0.0019 step-time 0.03 perplexity 3670.66 loss 8.21\n",
      "global step 109500 learning rate 0.0019 step-time 0.04 perplexity 1387.15 loss 7.24\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
